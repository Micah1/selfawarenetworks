This is the transcript of the original Neural Lace Podcast Monologue

The original audio was published to youtube in April 2017 and contains the root concepts from both the Self Aware Networks Theory of Mind and the Super Information Theory that unites physics. The articles on this page are also evidence that my work on Self Aware Networks and Super Information Theory traces back to 2017 at least. https://medium.com/silicon-valley-global-news/the-neural-lace-journal-talk-show-a-compilation-of-articles-and-links-ae3fbb0d5d5a
These articles were again published on Substack.

Here is the youtube link.
https://youtu.be/YrX_68oKuVs

OpenAI Whisper Transcript:

So, welcome to the Neuralace podcast.

The Neuralace podcast is all about neurophysics, which is the science and technology behind

Neuralace. Spatial computing, which is the technology behind virtual reality,

augmented reality, and mixed reality. It's also an umbrella topic that includes

deep learning artificial intelligence. Some of the topics that come up in this podcast

will be about computer vision specifically, about applying deep learning to medical imaging.

It's the first podcast that will attempt to unite the topics of web VR with Neuralace.

It's the podcast that dares ask how information in the networks of the brain are organized and

how we might query that information in any region of the brain with a brain port that understands

the transmission protocol of the brain. I'm going to propose that transmission protocol

on this podcast. And we also talk about hacking into the brain's VR system to add things to your

reality that are not really there. And I actually attempt to explain how we could do that in this

first episode. And then we also chat about, in the course of this podcast, talking about

the kinds of things we can download, information from the brain, like holograms of your experiences,

as if your eyes became cameras. So we can capture what you saw and what you heard. And you can share

those experiences with others. You could share those experiences in court. We also talk about

defending our brains against remote hacks. And we boldly, we dare to boldly go where no one has gone

before. So welcome very much to the Neuralace podcast. Your host is Micah Blumberg. The reality

is that Neuralace is a brain-computer interface. It's not the same as the EG because it's bi-directional

because we're reading and writing. So the first step is we need a supercomputer, like a self-driving

car, or maybe it could be, you know, the NVIDIA, DJI One. Google has this new TPU. And Intel,

just reading, they recently bought this, not recently, but they bought this company that

makes their own sort of AI processor that just focuses on multiplying matrices really fast

in a very dedicated way. But it's, you know, it deviates from the GPU and that they're getting

away from doing all this extra work to process graphics. I mean, they just want to, we just want

to be able, our minds are, you know, definitely adapted to, that's why, you know, virtual reality

and our spatial computing, the technology behind virtual reality is, is important to the topic of

Neuralace because we have to, once we have that bridge between the brain and the computer, we have

to think in terms of, you know, relating what's in the brain to relating what's in the computer in

terms of, in terms of spatial computing. Because that's, it's kind of what we're doing is we're

creating a spatial computation of say, well, all of these different lines belong together.

And we're going to call this the concept of a car. And not only do we have this concept of a car,

which is all these lines, but we can also predict which way the car is moving. And the self-driving

car is doing the same thing. It's predicting there is a concept, which is a unification of a bunch of

lines and edges and spatial computing and cars doing it and humans are doing it. But if we bring

spatial computing into neuroscience, we can revolutionize medical neuroscience. And that

means that we have the same computer that's figuring out if this is a cat or a dog or a car door and how

this car door is moving, if it's opening or if it's closing, so that I know if the car that I'm

driving is going to hit it or not. If any, I just need to know if the path in front of me is clear,

right? But if the same sort of like spatial computing of a self-driving car can just figure,

you can put that around the body of a person, right? While you get, you know, a really, really

great scan of their brain. And I'm not, and my research tells me there are certain parts of the

brain that are more valuable than others. And so that's why, you know, you see that, that, you know,

the stories of why Elon Musk with Neuralink, he's talking about wetware and installing chips into the

core of the brain. And, you know, and I think they're targeting the old brain, you know,

because all your incoming senses go through the old brain first before they go out to the rest of

the neocortex. So there in the midbrain is the chance to see what the integration of your different

senses looks like and before it goes out to the rest of the neocortex. And what's interesting about

that is that most of the information that's arriving at that point is not coming in from

your senses. It's coming back from the cortex. So it's like one fifth of the information you're

seeing is, um, from your eyes and the other four fifths or eight ninths is from the back of your head

that, that would make sense in the sense that there's nothing, you can't see what's behind you,

but yet you're not afraid that there's nothing behind you because you kind of like,

even though you're like in one, and let's say you take 10 seconds of time. Okay. And you're just

sitting for during that 10 seconds and that 10 seconds of time, you don't know that there's

anything behind you, but the other eight ninths of activity coming back towards the midbrain might

be telling you, Oh, you don't have to freak out though, because you're, we understand that you're

inside the metaphor of a room. There's a room, there's a room behind you. You're not going to,

there's a floor underneath that you're not going to fall through it. These are all, um, supporting

concepts that are taking a little bit of information coming from your senses and bolstering that into

your complete experience. And so if we can, if we can, and so my thinking is what Elon Musk and

Neuralink are thinking is that if we can somehow, um, put a microphone, uh, close enough to the midbrain

and, you know, a really good microphone, I'm talking about a microphone that has

a billion times improvement in the sensitivity of its sensing capability. So you could configure

it for EEG, or you could configure it to measure a single electron passing over a cell membrane. So

if you put a really big microphone up there, then, and you get those signals out and you're able to

like, you know, uh, triangulate, maybe get a few of them up there and you're able to triangulate the

origin point of a brainwave, for example. And you're, but you're able to, um, to track that

brainwave relative to other brainwaves with, with diffusion tensor imaging. Um, or you're able to,

you know, you're able to, um, there's all sorts of interesting things you can do. If you,

the idea here is that if we can, you know, at the same time, have this self-driving car type of

computer figuring out the environment and it's watching you, it's watching you eat that steak.

And we're seeing all this activity light up, right? With this microphone, right in your thalamus,

where your senses are coming together, where the sight of the steak is coming together with the

smell of the steak and the taste of the steak. And they're all hitting the center of your brain

at the same exact time. That's when we want to see it. That's where the converging point is,

right? That's where we want to get the sensor because then we can figure out what your

representation of steak is. And in terms of, and that means in terms of, of, of, uh, brainwaves,

in terms of electromagnetic frequencies, in terms of the actual dimension, the spatial dimensions,

like imagine, imagine a voxel, uh, of at the center of your brain of, it's a large voxel.

And in this voxel is a, um, is a, is a, is a, is a three-dimensional, um,

I'll say a three-dimensional matrix, like a three-dimensional matrix, right? X, Y, and Z.

And each point in this three-dimensional matrix has it, has a, um, a temple, a time and space

pattern going through it. And that pattern is the ionic charges at each point in that matrix.

Okay. And so they're changing over time. And so we want to recognize what the pattern of stake is

with computer vision combined with the, uh, the microphone, the, the sensor that's in, in,

in the fusion tensor, um, imaging, uh, microphone of your, um, of, of this pattern. If we can capture

this pattern with our microphone and compare it to the pattern of you eating steak, and then,

and then we'll put this in the, in the neural hierarchy, the deep learning hierarchy, the computer

figures out what that third pattern is. That's step number one to achieving neural needs. That's

when we have the, the, uh, the basic idea of what brain patterns are now, part two. Okay. Part two,

the second step is we need to figure out what the communications protocol of the brain is,

because some people have speculated and that the, the, um, the brain is organized like the internet.

Um, but really, I mean, even, you know, the internet structure itself has changed. Um, so that's not a,

you know, in, in the structure of the brain will change over the development of a human being's

lifetime. Um, and people, and you'll have different, you have structural differences between

people who have very different personalities. Some people will, um, process, um, mathematics with,

with their, with their audio side of their brain. And some people will, um, process it with the visual

side of the visual part of their brain. Anyway. So the, the point being that there are differences,

there's some differences, um, and how the brain is, is, is, is the structure of the brain. And then

there's a lot of different, um, there's different tools being used to sort of analyze what that

structure is. And it's, it's, it, but my thinking is once we are able to, we, you know, people are

trying to figure out what the structure of their brain is in terms of like, you know, analyzing MRI

images while someone watches a movie, right? This is a classic example. Um, there was a guy,

Jack Gallant, who in Berkeley was, um, was having someone watch a movie and then he would record the

MRI of their brain activity while they watch the movie. And then he would correlate the patterns

in the MRI with, uh, it, it's complicated, but, but eventually he was able to take how he designed

his system and then apply it to new images. And he was able to, um, uh, sort of recreate the movie

from MRI images, which, um, which is just, it's, it's, it's really interesting because from this

basic, um, achievement, he was able to sort of create a map of, of how different associations

that we make are located sort of regionally across the brain. So in one part of the brain,

you'll see like a dog and the next dog will be cat. But then in a totally different region of the

brain, you'll see like house. And next to that, you'll see like boat. And so he's got all these

great maps and they're, they're really cool. And you should definitely check them out. Check out the

work of Jack Gallant and, and all the people working with him, which I couldn't, couldn't name

right now. But so, um, so, but we need to figure out what is the, what the thing is, if the, if the

brain is, uh, like a network, what's the communication protocol between neuron to neuron or

between bunches of neurons and bunches of neurons, is it, and, and so this goes into, you know, web

VR. So I wanted to make a web VR app for my group, um, San Francisco virtual reality, which I am one of,

which I'm one of the organizers and I run the media group and I wanted to make an event app. And I was

talking to a web VR developer, a friend of mine. She said, there's no turnkey solution for

multiplayer. So there are three sort of like web VR companies I know of, which is one of which is,

uh, Janus VR. One is, um, high fidelity. One is all space and that's web VR, but it's all based on

sort of like, um, it's not like traditional web VR, like traditional web VR is Mozilla and Chrome

and something and something like a frame or, you know, you basically means that you can use

JavaScript or, or three JS. And, um, but the point is that you can use, um, web VR allows you to use

the language of H of HTML and JavaScript ideally to, uh, create virtual reality. But if you look at all

the examples of stuff that's been created on Mozilla and Chrome, it either, there is no multiplayer

element to it or like multiplayer is like four people max because it's just a, it's a problem

in the, and it's because the web VR and someone was telling me that I was at the offices that Google

for San Francisco HTML five, um, listening to Tony Parisi speak and, and about how attitudes have

really changed in one year. Now, now we're web VR is getting massive respect and, and it should be,

um, and, but the question is like, well, why is it, there's no turnkey solution for a frame for massive

multiplayer. And the main, I think the main reason is rooted in the fact that it's all based on TCP

at a high, at the highest level. If you want something that's massive multiplayer, you have to sort of

like use an internet protocol called UDP. Um, and UDP without going into a ton of specifics,

is faster, um, because it doesn't require a connection. You're just throwing out packets of

data. And, but TCP does require a connection, which is interesting because the connection

means there's, there's actually a feedback loop of information. So the two main, you know, there's a,

and there's, there's, there's other hierarchies beneath that of, of, you know, like, like hypertext,

uh, uh, transmission protocol is the, you know, HT, HTTP. And I mean, but, but those are the,

the, at the highest level in this hierarchy of communication protocols, it's just TCP or UDP,

or, you know, if you, you can set your, your router to both, which means the, which means, uh,

um, that your application can decide. Um, but what, what that means is that, um, it's,

it's really kind of a big conceptual difference because if you have a feedback loop or if you

don't, you're able to deliver things in order. If you have this feedback loop of checks. And so

you're able to send a whole and complete message and you're able to verify that it was sent, but

otherwise you're just sending stuff in the dark. And if so, UDP is really great for, um, for it's not

just the speed of it, but it's also the fact that it's sort of designed in a way that, um,

if you miss part of it, it's like what you're missing is like an update on where the other

characters located in a, in virtual reality. So it's like, it's like it, that is okay because it's

like missing, um, frames in a movie, but the movie continues on at the same pace. So what that would

look like if you lost a few packets would be like, you're in virtual reality and you see the person

in front of you moving and they vanish for a second and then they reappear where they would

have been if you'd seen them move in a consistent way. So that's like, that's like what the kind of

packet data loss. And that makes sense in the kind of like, when you're talking about virtual reality,

because we're now we're, we're dealing with sort of like a spatial relationship between, uh,

people over a network and you need, it's better to, it's better for it to be fast than it is for

it to be accurate in terms of, you know, trying to have the kind of speed that's necessary for

massive multiplayer. So that's why like massive multiplayer online is not even really on the

radar of web VR, like Chrome or Mozilla, but it is, um, definitely on the radar for, I mean,

it's definitely a core component of, you know, Janus VR and, and, uh, all space, et cetera.

Is a connection between ourselves and what we're observing, um, is like a conscious,

if I see something and I'm conscious of it, and does that mean that I'm in a feedback loop with it?

Am I establishing a connection with it? Um, or it seems like, you know, the way it was thought was,

you know, there's just a photon bouncing off that thing and the photons bouncing off my eye

and causing an electron to travel into the ganglia neuron or into the retina and then the ganglia.

But I mean, the point is that even if I sort of, is that there, it's like, what if, and, but then,

so what then is, is the obvious question, which is the brain more likely to have a, um, a UDP like

protocol from between neuron to neuron or a transmission, uh, control protocol from neuron to

neuron, um, or from brainwave, uh, to, um, I mean, what does the brainwave communicate with?

I mean, a brainwave can only communicate with the electromagnetic field that's already in the

brain. So we're talking about two points in the electromagnetic field communicating.

And that's kind of interesting too. Um, so the, the idea is what is a, what is a brainwave

communication protocol? Like when two brain, two points of, of, of the brain are communicating

in terms of electromagnetic frequencies. Um, and you know, and then it's like, well, wait a second,

what is the communication protocol for when the mind is conscious? And what is it? When is the

communication protocol for when something is unconscious? Cause we know that there's an

unconscious element to our mind as well. And what if, what if when something is conscious,

that's more like TCP cause there's a connection as a feedback loop, right? What if when something

is unconscious, that's more like UDP, it means there is no, there's no feedback loop. Okay. So,

um, so that's, so that's the idea here of, of once we figure out and, and, and my, my proposal,

of course, that I came up with, which is like, was, Oh, you got to do both. Right. It's the obvious

you got to do both. You got to call it like, cause the brain, or I could, I mean,

maybe it's something we haven't thought of as well, but, um, the brain may be alternating between,

um, information that's sent in, in, you know, cause information can drop out if you're,

if you're, uh, if your cell dies, you could, you could lose information, especially if someone's

killing their brain cells by, by, uh, drinking alcohol or, or smoking weed or, or just, you know,

they have a lot, they have an inflammation problem. They have a disease. They're losing

brain cells. They're losing memories, right? Information can drop out on you and, and the

dropout can be good. You know, an AI dropped this technique called dropout means you throw

away half of your, your, uh, network, your brain cells, throw away half of your network data,

um, at random and dropout actually can improve, um, memory for, uh, AI. It can make the AI program

more robust. So it can handle, uh, dropouts of a different nature. For example, in reality,

when you're trying to observe the, the pattern of a cat and part of the cat is behind the pole,

you know, and, and, and that means that you see the head of the cat and then you see the pole

and then you see the tail of the cat. Well, we drop out the kind of, you know, AI system is

more robust. So it's more likely to recognize that the two different pieces of a cat are actually

one cat that's behind something. And the same thing could be true for humans. If someone drinks

a lot of alcohol, they could be, um, more, or could, or they smoke some trees. They could

come to some creative conclusion that they may not have otherwise had because

essentially their brain is, um, connecting dots is that it to make up for the fact that dots are

missing, the connecting dots that may not have normally connected. So it's like you're connecting

ideas. You're becoming creative because you're, because some of your brain cells died and we can call

it creativity or we can call that, you know, um, dropout. And, um, so the, so that, so, okay.

So this is good because if we have, you know, something like TCP with electromagnetism that way,

and if we have TCP is conscious information going from neuron turnaround or from two points in the

electromagnetic field, um, if we have that, then, um, potentially, um, you know, in terms of what

those frequencies are, if we can create, if we can put a chip somewhere in the brain, it could be

anywhere. It could even be outside the body that can send and receive electromagnetic frequencies

according to the communication protocol of the brain, which, which I am, I just like to call some,

I mean, I should have a really cool name, like, like ATP, like, because ATP alternate, it would be

alternating transmission protocol. Okay. But ATP is also on a biological level, the energy of your cells.

So when your cells metabolize your food and, um, that the results of the metabolic activity is the

production of ATP, which is, um, your, uh, your currency, your, your cells energy currency. And so

that's your money and ATP. And once you have ATP, um, in terms of, you know, network, or you can maybe

call it like ETC, like, like, uh, or EMTC or electromagnetic transmission protocol. That's

another cool name, but the idea either way is that it doesn't have to be, you know, it's basically the,

it's like, basically it's like saying, okay, we need to alternate between, um, the ability to send and

receive, you know, UDP information on one level, uh, and TCP information on another level. And then you

have to like further subcategorize below that in this hierarchy of transmission protocols,

because you gotta have protocols that are really kind of like great at processing audio information

and protocols that are great at processing, um, uh, visual information. So I imagine like visual

information would, the protocol would probably be like something in the gamma frequency is there

are a lot of gamma waves that light up when people are doing very visual things. Um, someone puts

on a blindfold, you're going to see a huge drop in their gamma waves. So, and I imagine that the,

the frequency for audio might be like, uh, it could be alpha waves, uh, maybe that alpha waves is the

frequency range for, um, the audio cortex or for audio concepts, which an audio concept. Now, if you go

back to Jack Gallant and the idea that you have the concept of a dog is near a concept of a cat, but both of

those concepts far away from the concept of a house, which is closer to the concept of

a, um, motorbike. So if concepts are distributed in that way, that means the, the, in a concept then

for the brain has to consist of like, you know, an audio concept and a visual concept. Like imagine

when you're eating the tape, the, in your eating steak, you've got that audio portion of it, right?

This is, this is from this, this, this idea is from a book called, um, criteria causation. Actually

it's called the neural basis of free will criteria causation by Peter C. And the whole, the neural

basis of free will part, you know, take it or leave it. The part that you want to read about

is called criteria causation. Um, that is the idea that, that are the concepts of our, that are,

is the idea that our, our cells are coincidence detectors. And then from the coincidence is the

basis of information. It's the basis of a bit. So the cell firing twice within a few seconds

is the basis of a bit in the brain. It's like an on versus an off. And so the idea is that

if we can trans, if we can not only notice the, um, the, um, the pattern of steak, but we can translate

it into, um, a matrix of, of ons and offs in terms of, um, ions. And then we can put that through a,

um, uh, the electromagnetic or alternating transmission protocol. Then we can communicate

with the brain and then, but we, you don't, you don't want it, but you don't want to send the

exact pattern of steak to the brain while it's having some other pattern. Cause now you're adding

two patterns together that do not necessarily go together. But what you would do then is figure out

the difference between the pattern that's there and the pattern that should be there if steak was

there. And then you send the difference between those two patterns so that the pattern that received

just allows you to have steak into your reality where there was none before. And it just, it's just, um,

it's like flipping dominoes in a three dimensional, a four dimensional electromagnetic matrix of

particles. Yeah. So once we can do that, that means that you're going to have things in your

reality that are not actually there, or your reality itself can be altered to where the, let's say

you're, you're sitting in a room and all of a sudden the room goes away and you're on some other

planet because we can, because your whole reality can change once, um, once we have narrow lace and

after the reason I'm, I'm really interested in giving me a neural lace is because I want it to

happen. I want it to happen faster. I want everyone to, I want to democratize. I want everyone to have

it. And, and beyond neural lace, the, what I'm, what after I'm building neural lace first and happy to

work with anyone who wants to work, tell me, build it. But after we build neural lace, the idea is to,

um, uh, build artificial neocortex and then artificial brains. We have, there's a lot of great

ideas on how to achieve neural lace and a lot of people want to keep their ideas a secret. And so

we were talking about that in terms of if we give away how to make neural lace on this podcast,

um, what is our objective then? And should neural lace be democratized so every company can know

how to do it. And my thought there is that we should democratize neural lace because we really want

this technology to exist in the world. And we do not necessarily know, um, because it's a very

complex thing that, uh, to try to solve. And so we, we want to encourage as many people

to get on board with, um, helping us to create neural lace as possible so that we can have neural

lace in our lifetimes. So someone remarked to me recently that in order to achieve, um, the kind

of neural lace we're talking about or brand computer interface we're talking about, you'd have to have

a number of like Nobel prize winning discoveries that would happen. It's pretty much simultaneously.

Um, and the, and so one of the positive outcomes that, um, of neural lace is, is we'll be able

to do, um, it's on the way to discovering it, we're going to have to revolutionize medical,

um, neuroscience. And that, that means, for example, um, we're going to have to figure out

the, um, transmission. Let's, let's say that the, the brain is, is organized like, um, the brain

and the nervous system are organized like the internet, for example, as many neuroscientists

believe. Um, we're going to have to figure out what the, um, transmission protocol of the brain

is. And if we do that, um, you know, from neuron to neuron, if we do that, that's going to enable

the kinds of products like, um, like artificial limbs that can connect to your nervous system or

reconnecting the spine. Um, it's not necessarily, um, as glamorous as the science fiction concept

of neural lace, but there's a lot of practical medical applications to being able to connect a,

a limb or reconnect the spine, um, an artificial limb to your nervous system. So you can, um,

really, uh, even, even create new kinds of limbs. Um, so these are, um, really positive outcomes that

can come from pursuing a direction of research towards neural lace about, uh, new, uh, ways to

send data into the brain. Uh, for example, David Eagleman, uh, talks about, um, there are strips

that you can put like on your tongue or on your back that, um, the strip is a grid of electrodes

and it's connected to a camera and the camera is watching, uh, the world and takes that image and

it converts it into, uh, the electrical grid of, of, of, um, of signals on your back or on your tongue

and eventually your brain figures out, um, how to see an image from that. So that is, is a way of,

of inputting data back into the brain. In fact, just using your regular eyes is actually putting data

back into your brain. So that's, that's another input channel to, to think about. There are people

who are, um, studying medical imaging, um, you know, with, with MRI machines, uh, besides people

who, um, are putting chips inside, uh, they're cutting into the brain and putting chips inside

the, the, the patients of, who have, um, epilepsy who need their brains opened up anyways. Um, and

then, so this has been sort of like a long dream of, of, of, of scientists who study the brain of

what kinds of sensors can we attach? And once we have the sensor data, how can we analyze that data?

Um, with, uh, with, with new, uh, techniques like, um, deep learning with computer vision,

we have, we can create, you know, different kinds of biosensors. We can apply deep learning to these,

um, uh, sets of data in new ways and potentially revolutionize medical neuroscience by combining,

uh, state of the art AI with, uh, medical imaging of all kinds, not just EEG, but, uh, so

that's my thought there. Um, we can totally, uh, start with some EEG products and, and come

up with, and the results of that action will, will certainly yield some really awesome new

neural lace products. But I just wanted to say that, um, I, I'm really interested in creating

a, a, a, a sort of, um, installed, um, uh, wetware, um, sensor for the purpose of just research.

But I, I, I was at CES 2017 and I saw this, um, really awesome new, uh, wireless, uh, EEG system.

So that's one of the things that, um, companies are talking about right now is, you know, we

actually could do sort of a noninvasive, um, neural lace that gets at the core of our brain

by using a variety of, of, um, tools such as, um, you know, like, like they use, um, ultrasonic,

um, sound for surgery now. So there are ways to stimulate the brain and read the brain,

like really deep, uh, without actually cutting into the brain. And so that's definitely,

uh, definitely, um, you know, if you look at the, uh, supercomputer that Nvidia is doing

for self-driving cars, that's a really mean and serious machine and they're still figuring

out how to do it. So basically their computer has all these sensors, LiDAR, like it was

just mentioned, uh, eight cameras besides that and a whole bunch of other things. And it's

figuring out not only the spaces around the car, but also it's, it's, it's figuring out

what the objects are. Like there is a, it's saying, well, this is, this is a cat and this

is a dog and this is a car door and this, this car door is going to open or this car

door is going to close. And there's a car across the street that's, that's moving at

such and such speed and it's going to intersect the path in front of you. These are things

that they're having, uh, that require a supercomputer that's going to be installed in the trunk of

your car for, for, for driving. But if we could take for women, if we could take those self-driving

cars and apply them to the task of, of neuroscience, we could probably create a revolution in neuroscience.







A version of the same transcript with time codes is found below



Detecting language using up to the first 30 seconds. Use `--language` to specify the language

Detected language: English

[00:00.000 --> 00:04.680]  So, welcome to the Neuralace podcast.

[00:06.220 --> 00:11.220]  The Neuralace podcast is all about neurophysics, which is the science and technology behind

[00:11.220 --> 00:17.880]  Neuralace. Spatial computing, which is the technology behind virtual reality,

[00:18.040 --> 00:24.920]  augmented reality, and mixed reality. It's also an umbrella topic that includes

[00:24.920 --> 00:29.940]  deep learning artificial intelligence. Some of the topics that come up in this podcast

[00:29.940 --> 00:35.760]  will be about computer vision specifically, about applying deep learning to medical imaging.

[00:36.700 --> 00:42.120]  It's the first podcast that will attempt to unite the topics of web VR with Neuralace.

[00:42.140 --> 00:48.000]  It's the podcast that dares ask how information in the networks of the brain are organized and

[00:48.000 --> 00:55.100]  how we might query that information in any region of the brain with a brain port that understands

[00:55.100 --> 01:00.620]  the transmission protocol of the brain. I'm going to propose that transmission protocol

[01:00.620 --> 01:06.080]  on this podcast. And we also talk about hacking into the brain's VR system to add things to your

[01:06.080 --> 01:13.040]  reality that are not really there. And I actually attempt to explain how we could do that in this

[01:13.040 --> 01:18.720]  first episode. And then we also chat about, in the course of this podcast, talking about

[01:18.720 --> 01:23.920]  the kinds of things we can download, information from the brain, like holograms of your experiences,

[01:24.840 --> 01:30.240]  as if your eyes became cameras. So we can capture what you saw and what you heard. And you can share

[01:30.240 --> 01:36.120]  those experiences with others. You could share those experiences in court. We also talk about

[01:36.120 --> 01:43.200]  defending our brains against remote hacks. And we boldly, we dare to boldly go where no one has gone

[01:43.200 --> 01:49.700]  before. So welcome very much to the Neuralace podcast. Your host is Micah Blumberg. The reality

[01:49.700 --> 01:55.920]  is that Neuralace is a brain-computer interface. It's not the same as the EG because it's bi-directional

[01:55.920 --> 02:03.240]  because we're reading and writing. So the first step is we need a supercomputer, like a self-driving

[02:03.240 --> 02:12.220]  car, or maybe it could be, you know, the NVIDIA, DJI One. Google has this new TPU. And Intel,

[02:12.220 --> 02:18.340]  just reading, they recently bought this, not recently, but they bought this company that

[02:18.340 --> 02:22.660]  makes their own sort of AI processor that just focuses on multiplying matrices really fast

[02:22.660 --> 02:29.560]  in a very dedicated way. But it's, you know, it deviates from the GPU and that they're getting

[02:29.560 --> 02:34.800]  away from doing all this extra work to process graphics. I mean, they just want to, we just want

[02:34.800 --> 02:40.380]  to be able, our minds are, you know, definitely adapted to, that's why, you know, virtual reality

[02:40.380 --> 02:46.200]  and our spatial computing, the technology behind virtual reality is, is important to the topic of

[02:46.200 --> 02:52.560]  Neuralace because we have to, once we have that bridge between the brain and the computer, we have

[02:52.560 --> 02:58.940]  to think in terms of, you know, relating what's in the brain to relating what's in the computer in

[02:58.940 --> 03:04.720]  terms of, in terms of spatial computing. Because that's, it's kind of what we're doing is we're

[03:04.720 --> 03:09.940]  creating a spatial computation of say, well, all of these different lines belong together.

[03:10.540 --> 03:17.460]  And we're going to call this the concept of a car. And not only do we have this concept of a car,

[03:17.520 --> 03:24.000]  which is all these lines, but we can also predict which way the car is moving. And the self-driving

[03:24.000 --> 03:30.160]  car is doing the same thing. It's predicting there is a concept, which is a unification of a bunch of

[03:30.160 --> 03:37.660]  lines and edges and spatial computing and cars doing it and humans are doing it. But if we bring

[03:37.660 --> 03:44.220]  spatial computing into neuroscience, we can revolutionize medical neuroscience. And that

[03:44.220 --> 03:49.240]  means that we have the same computer that's figuring out that if this is a cat or a dog or a car door and

[03:49.240 --> 03:53.740]  how this car door is moving, if it's opening or if it's closing, so that I know if the car that I'm

[03:53.740 --> 03:58.740]  driving is going to hit it or not. If any, I just need to know if the path in front of me is clear,

[03:58.740 --> 04:04.400]  right? But if the same sort of like spatial computing of a self-driving car can just figure,

[04:04.480 --> 04:11.780]  you can put that around the body of a person, right? While you get, you know, a really, really

[04:11.780 --> 04:18.300]  great scan of their brain. And my research tells me there are certain parts of the brain that are

[04:18.300 --> 04:23.560]  more valuable than others. And so that's why, you know, you see that, you know, the stories of why

[04:23.560 --> 04:28.700]  Elon Musk with Neuralink, he's talking about wetware and installing chips into the core of

[04:28.700 --> 04:32.440]  the brain. And, you know, and I think they're targeting the old brain, you know, because all

[04:32.440 --> 04:36.840]  your incoming senses go through the old brain first before they go out to the rest of the

[04:36.840 --> 04:47.300]  neocortex. So there in the midbrain is the chance to see what the integration of your different

[04:47.300 --> 04:53.680]  senses looks like and before it goes out to the rest of the neocortex. And what's interesting about

[04:53.680 --> 04:59.580]  that is that most of the information that's arriving at that point is not coming in from

[04:59.580 --> 05:05.000]  your senses, it's coming back from the cortex. So it's like one fifth of the information you're

[05:05.000 --> 05:12.160]  seeing is from your eyes. And the other four fifths or eight ninths is from the back of your head.

[05:12.700 --> 05:18.400]  That would make sense in the sense that there's nothing, you can't see what's behind you, but yet

[05:18.400 --> 05:23.720]  you're not afraid that there's nothing behind you. Because you kind of like, even though you're like

[05:23.720 --> 05:29.140]  in one, let's say you take 10 seconds of time, okay, and you're just sitting forward during that

[05:29.140 --> 05:34.760]  10 seconds. And that 10 seconds of time, you don't know that there's anything behind you. But the other

[05:34.760 --> 05:40.320]  eight ninths of activity coming back towards the midbrain might be telling you, oh, you don't have

[05:40.320 --> 05:46.200]  to freak out though, because you're, we understand that you're inside the metaphor of a room.

[05:46.200 --> 05:49.980]  There's a room, there's a room behind you, you're not gonna, there's a floor underneath

[05:49.980 --> 05:55.920]  that you're not going to fall through it. These are all supporting concepts that are taking a

[05:55.920 --> 06:00.680]  little bit of information coming from your senses, and bolstering that into your complete experience.

[06:01.580 --> 06:08.200]  And so if we can, if we can, and so my thinking is what Elon Musk and Neuralink are thinking is that

[06:08.200 --> 06:18.600]  if we can somehow put a microphone close enough to the midbrain, and, you know, a really good

[06:18.600 --> 06:23.900]  microphone, I'm talking about a microphone that has a billion times improvement in the sensitivity

[06:23.900 --> 06:30.900]  of its sensing capability. So you could configure it for EEG, or you could configure it to measure a

[06:30.900 --> 06:34.660]  single electron passing over a cell membrane. So if you put a really big microphone up there,

[06:34.660 --> 06:42.580]  then you get those signals out, and you're able to like, you know, triangulate, maybe get a few of

[06:42.580 --> 06:47.600]  them up there, and you're able to triangulate the origin point of a brainwave, for example. And

[06:47.600 --> 06:53.520]  you're, but you're able to, to track that brainwave relative to other brainwaves with, with diffusion

[06:53.520 --> 07:01.920]  tensor imaging. Or you're able to, you know, you're able to, there's all sorts of interesting things

[07:01.920 --> 07:08.360]  you can do if you, if you, the idea here is that if we can, you know, at the same time have this

[07:08.360 --> 07:12.280]  self-driving car type of computer figuring out the environment, and it's watching you, it's watching

[07:12.280 --> 07:17.140]  you eat that steak. And we're seeing all this activity light up, right, with this microphone, right

[07:17.140 --> 07:22.260]  in your thalamus, where your senses are coming together, where the sight of the steak is coming

[07:22.260 --> 07:26.420]  together with the smell of the steak and the taste of the steak, and they're all hitting the center of

[07:26.420 --> 07:31.260]  your brain at the same exact time. That's when we want to see it. That's where the converging point

[07:31.260 --> 07:35.540]  is, right? That's where we want to get the sensor, because then we can figure out what your

[07:35.540 --> 07:44.020]  representation of steak is. And in terms of, and that means in terms of, of, of brainwaves, in terms

[07:44.020 --> 07:50.200]  of electromagnetic frequencies, in terms of the actual dimension, the spatial dimensions, like imagine,

[07:50.200 --> 08:00.500]  imagine a voxel of, at the center of your brain, of, it's a large voxel, and in this voxel is a, is a, is a,

[08:00.500 --> 08:08.420]  is a, is a three-dimensional, I'll say a three-dimensional matrix, like a three-dimensional matrix, right?

[08:08.520 --> 08:17.140]  X, Y, and Z. And each point in this three-dimensional matrix has it, has a, a temple, a time and space

[08:17.140 --> 08:24.380]  pattern going through it, and that pattern is the ionic charges at each point in that matrix, okay?

[08:24.760 --> 08:31.880]  And so they're changing over time, and so we want to recognize what the pattern of steak is with

[08:31.880 --> 08:42.740]  computer vision, combined with the, the microphone, the, the sensor that's in, in, in the fusion tensor

[08:42.740 --> 08:52.140]  imaging microphone of your, of, of this pattern, if we can capture this pattern with our microphone and

[08:52.140 --> 08:57.380]  compare it to the pattern of you eating steak. And then, and then we'll put this in the, in the neural

[08:57.380 --> 09:01.880]  hierarchy, the deep learning hierarchy, the computer figures out what that third pattern is. That's step

[09:01.880 --> 09:06.760]  number one to achieving neural needs. That's when we have the, the, the basic idea of what brain

[09:06.760 --> 09:13.080]  patterns are. Now, part two, okay, part two, the second step is we need to figure out what the

[09:13.080 --> 09:18.380]  communications protocol of the brain is, because some people have speculated and that the, the,

[09:18.440 --> 09:24.740]  the brain is organized like the internet. But really, I mean, even, you know, the internet structure

[09:24.740 --> 09:31.260]  itself has changed. So that's not a, you know, in, in the structure of the brain will change over

[09:31.260 --> 09:37.660]  the development of a human being's lifetime. Um, and people, and you'll have different,

[09:37.740 --> 09:41.840]  you have structural differences between people who have very different personalities. Some people

[09:41.840 --> 09:50.960]  will, um, process, um, mathematics with, with their, with their audio side of their brain. And some

[09:50.960 --> 09:57.140]  people will, um, process it with the visual side of the visual part of their brain. Anyway, so the,

[09:57.140 --> 10:02.560]  the point being that there are differences, there's some differences, um, and how the brain is,

[10:02.640 --> 10:07.080]  is, is, is, but the, the structure of the brain, and then there's a lot of different, um, there's

[10:07.080 --> 10:11.460]  different tools being used to sort of analyze what that structure is. And it's, it's, it, but my

[10:11.460 --> 10:16.100]  thinking is once we are able to, we, you know, people are trying to figure out what the structure of

[10:16.100 --> 10:21.740]  the brain is in terms of like, you know, analyzing MRI images while someone watches a movie, right?

[10:21.740 --> 10:31.040]  This is a classic example. Um, there was a guy, Jack Gallant, who in Berkeley was, um, was having

[10:31.040 --> 10:37.660]  someone watch a movie and then he would record the MRI of their brain activity while they watched the

[10:37.660 --> 10:47.820]  movie. And then he would correlate the patterns in the MRI with, um, it, it's complicated, but,

[10:47.820 --> 10:55.880]  but eventually he was able to take how he designed his system and then apply it to new images. And he

[10:55.880 --> 11:04.400]  was able to, um, uh, sort of recreate the movie from MRI images, which, um, which is just, it's,

[11:04.400 --> 11:13.920]  it's, it's really interesting because from this basic, um, achievement, he was able to sort of create a

[11:13.920 --> 11:21.200]  map of, uh, how different associations that we make are located sort of regionally across the

[11:21.200 --> 11:25.880]  brain. So in one part of the brain, you'll see like a dog and the next to dog will be cat, but

[11:25.880 --> 11:30.160]  then in a totally different region of the brain, you'll see like house and next to that, you'll see

[11:30.160 --> 11:36.720]  like boat. And so he's got all these great maps and they're, they're really cool. And you should

[11:36.720 --> 11:41.380]  definitely check them out. Check out the work of Jack Gallant and, and all the people working with

[11:41.380 --> 11:47.460]  him, which I couldn't, couldn't name right now. But so, um, so, but we need to figure out what is

[11:47.460 --> 11:55.360]  the, what the thing is if the, if the brain is, uh, like a network, what's the communication protocol

[11:55.360 --> 12:01.200]  between neuron to neuron or between bunches of neurons and bunches of neurons? Is it, and, and

[12:01.200 --> 12:08.840]  so this goes into, you know, web VR. So I wanted to make a web VR app for my group, um, San Francisco

[12:08.840 --> 12:13.920]  virtual reality, which I am one of which I'm one of the organizers and I run the media group

[12:13.920 --> 12:17.940]  and I wanted to make an event app. And I was talking to a web VR developer, a friend of

[12:17.940 --> 12:22.980]  mine. She said, there's no turnkey solution for multiplayer. So there are three sort of

[12:22.980 --> 12:29.660]  like web VR companies I know of, which is one of which is, uh, Janus VR. One is, um, high

[12:29.660 --> 12:36.520]  fidelity. One is all space and that's web VR, but it's all based on sort of like, um, it's

[12:36.520 --> 12:42.940]  not like traditional web VR, like traditional web VR is Mozilla and Chrome and something

[12:42.940 --> 12:48.080]  like a frame or, you know, you basically means that you can use JavaScript or, or three

[12:48.080 --> 12:57.000]  JS. And, um, but the point is that you can use, um, web VR allows you to use the language

[12:57.000 --> 13:06.460]  of H of HTML and JavaScript ideally to, uh, create virtual reality. But if you look at all

[13:06.460 --> 13:12.300]  the examples of stuff that's been created on Mozilla and Chrome, either there is no multiplayer

[13:12.300 --> 13:19.040]  element to it or like multiplayer is like four people max, because it's just a, it's a problem

[13:19.040 --> 13:23.240]  in the, and it's because the web VR and someone was telling me that I was at the offices that

[13:23.240 --> 13:29.940]  Google for San Francisco HTML five, um, listening to Tony Parisi speak and, and about how attitudes

[13:29.940 --> 13:34.600]  have really changed in one year. Now we're web VR is getting massive respect and, and it

[13:34.600 --> 13:40.940]  should be. Um, and, but the question is like, well, why is it, there's no turnkey solution

[13:40.940 --> 13:47.020]  for a frame for massive multiplayer. And the main, I think the main reason is rooted in

[13:47.020 --> 13:53.000]  the fact that it's all based on TCP at a high, at the highest level. If you want something

[13:53.000 --> 13:58.500]  that's massive multiplayer, you have to sort of like use an internet protocol called UDP.

[13:58.500 --> 14:08.920]  Um, and UDP without going into a ton of specifics is faster, um, because it doesn't require a

[14:08.920 --> 14:14.700]  connection. You're just throwing out packets of data. And, but TCP does require a connection,

[14:14.700 --> 14:18.760]  which is interesting because the connection means there's, there's actually a feedback loop

[14:18.760 --> 14:23.980]  of information. So the two main, you know, there's a high, and there's, there's, there's

[14:23.980 --> 14:32.060]  other hierarchies beneath that of, of, you know, like, like hypertext, uh, transmission protocol

[14:32.060 --> 14:38.120]  is the, you know, HT, HTTP. And I mean, but, but those are the, at the highest level in this

[14:38.120 --> 14:45.180]  hierarchy of communication protocols, it's just TCP or UDP, or, you know, if you, you can set

[14:45.180 --> 14:51.460]  your, your router to both, which means the, which means, uh, um, that your application

[14:51.460 --> 14:58.280]  can decide. Um, but what, what that means is that, um, it's, it's really kind of a big

[14:58.280 --> 15:04.040]  conceptual difference because if you have a feedback loop or if you don't, you're able

[15:04.040 --> 15:09.940]  to deliver things in order. If you have this feedback loop of checks and so you're able

[15:09.940 --> 15:14.760]  to send a whole and complete message and you're able to verify that it was sent.

[15:14.760 --> 15:19.940]  But otherwise you're just sending stuff in the dark. And if so, UDP is really great

[15:19.940 --> 15:26.200]  for, um, for, it's not just the speed of it, but it's also the fact that it's sort

[15:26.200 --> 15:32.200]  of designed in a way that, um, if you miss part of it, it's like what you're missing

[15:32.200 --> 15:41.120]  is like an update on where the other characters located in a, in virtual reality. So it's

[15:41.120 --> 15:47.260]  like, it's like, it, that is okay because it's like missing, um, frames in a movie, but

[15:47.260 --> 15:52.300]  the movie continues on at the same pace. So what that would look like if you lost a few

[15:52.300 --> 15:57.640]  packets would be like, you are in virtual reality and you see the person in front of

[15:57.640 --> 16:03.080]  you moving and they vanish for a second. And then they reappear where they would have

[16:03.080 --> 16:08.980]  been if you'd seen them move in a consistent way. So that's like, that's like what the kind

[16:08.980 --> 16:12.980]  of packet data loss. And that makes sense in the kind of like, when you're talking about

[16:12.980 --> 16:17.280]  virtual reality, because we're now we're, we're dealing with sort of like a spatial

[16:17.280 --> 16:23.020]  relationship between, uh, people over a network and you need, it's better to, it's

[16:23.020 --> 16:28.380]  better for it to be fast than it is for it to be accurate in terms of, you know, trying

[16:28.380 --> 16:32.380]  to have the kind of speed that's necessary for massive multiplayer. So that's why like

[16:32.380 --> 16:37.780]  massive multiplayer online is not even really on the radar of web VR, like Chrome

[16:37.780 --> 16:45.500]  or Mozilla, but it is, um, definitely on the radar for, I mean, it's definitely a core

[16:45.500 --> 16:51.420]  component of, you know, Janus VR and, and, uh, all space, et cetera, is a connection

[16:51.420 --> 17:00.280]  between ourselves and what we're observing. Um, is like a conscious, uh, if I see something

[17:00.280 --> 17:05.200]  and I'm conscious of it. And does that mean that I'm in a feedback loop with it? Am I

[17:05.200 --> 17:11.300]  establishing a connection with it? Um, or it seems like, you know, the way it was thought

[17:11.300 --> 17:16.540]  was, you know, there's just a photon bouncing off that thing and the photons bouncing off

[17:16.540 --> 17:22.180]  my eye and causing an electron to travel into the ganglia neuron or into the retina and then

[17:22.180 --> 17:33.300]  the ganglia. But I mean, the point is that even if I sort of, is that there it's like,

[17:33.460 --> 17:40.000]  what if, but then, so what then is, is the obvious question, which is the brain more likely

[17:40.000 --> 17:49.720]  to have a, um, a UDP like protocol from between neuron to neuron or a transmission, uh, control

[17:49.720 --> 17:57.260]  protocol from neuron to neuron, um, or from brainwave, uh, to, um, I mean, what does the

[17:57.260 --> 18:02.080]  brainwave communicate with? I mean, a brainwave can only communicate with the electromagnetic

[18:02.080 --> 18:06.940]  field that's already in the brain. So we're talking about two points in the electromagnetic

[18:06.940 --> 18:16.140]  field communicating. And that's kind of interesting too. Um, so the, the idea is what is a, what

[18:16.140 --> 18:21.660]  is a brainwave communication protocol like when two brain, two points of, of, of the

[18:21.660 --> 18:28.100]  brain are communicating in terms of electromagnetic frequencies? Um, and, you know, and then it's

[18:28.100 --> 18:32.560]  like, well, wait a second, what is the communication protocol for when the mind is conscious? And

[18:32.560 --> 18:36.260]  what is it, what is the communication protocol for when something is unconscious? Because we

[18:36.260 --> 18:42.400]  know that there's an unconscious element to our mind as well. And what if, what if when

[18:42.400 --> 18:46.000]  something is conscious, that's more like TCP, because there's a connection, there's a feedback

[18:46.000 --> 18:51.800]  loop, right? What if when something is unconscious, that's more like UDP, that means there is no,

[18:52.000 --> 19:02.100]  there's no feedback loop. Okay. So, um, so that's, so that's the idea here of, of once we figure out

[19:02.100 --> 19:06.820]  and, and, and my, my proposal, of course, that I came up with, which is like, was, oh, you got

[19:06.820 --> 19:11.740]  to do both, right? It's the obvious, you got to do both. You got to call it like, because

[19:11.740 --> 19:16.800]  the brain or it could, I mean, maybe it's something we haven't thought of as well, but, um, the

[19:16.800 --> 19:25.940]  brain may be alternating between, um, information that's sent in, in, you know, because information

[19:25.940 --> 19:31.200]  can drop out if you're, if you're, uh, if your cell dies, you could, you could lose information,

[19:31.320 --> 19:37.040]  especially if someone's killing their brain cells by, by, uh, drinking alcohol or smoking

[19:37.040 --> 19:40.800]  weed or, or just, you know, they have a lot, they have an inflammation problem, they have a

[19:40.800 --> 19:45.260]  disease, they're losing brain cells, they're losing memories, right? Information can drop

[19:45.260 --> 19:50.080]  out on you and, and the dropout can be good. You know, an AI drop, this technique called

[19:50.080 --> 19:55.420]  dropout means you throw away half of your, your, uh, network, your brain cells, throw away

[19:55.420 --> 20:03.080]  half of your network data, um, at random and dropout actually can improve, um, memory for,

[20:03.080 --> 20:10.280]  uh, AI. It can make the AI program more robust. So it can handle, uh, dropouts of a different

[20:10.280 --> 20:16.220]  nature. For example, in reality, when you're trying to observe the, the pattern of a cat

[20:16.220 --> 20:22.000]  and part of the cat is behind the pole, you know, and, and, and that means that you see

[20:22.000 --> 20:25.940]  the head of the cat and then you see the pole and then you see the tail of the cat. Well,

[20:26.200 --> 20:32.240]  we drop out the kind of, you know, AI system is more robust. So it's more likely to recognize

[20:32.240 --> 20:36.920]  that the two different pieces of a cat are actually one cat that's behind something.

[20:36.920 --> 20:41.860]  And the same thing could be true for humans. If someone drinks a lot of alcohol, they could

[20:41.860 --> 20:50.140]  be, um, more or could, or they smoke some trees, they could come to some creative conclusion

[20:50.140 --> 20:57.260]  that they may not have otherwise had because essentially their brain is, um, connecting

[20:57.260 --> 21:03.340]  dots is that it to make up for the fact that dots are missing the connecting dots that may

[21:03.340 --> 21:09.800]  not have normally connected. So it's like you're connecting ideas. You're becoming creative

[21:09.800 --> 21:14.980]  because you're, because some of your brain cells died and we can call that creativity or

[21:14.980 --> 21:25.260]  we can call that, you know, um, dropout. And, um, so the, so that, so, okay. So this is good

[21:25.260 --> 21:30.940]  because if we have, you know, something like TCP with electromagnetism that way, and if we have

[21:30.940 --> 21:35.880]  TCP is conscious information going from neuron to neuron or from two points in the electromagnetic

[21:35.880 --> 21:44.960]  field, um, if we have that, then, um, potentially, um, you know, in terms of what those frequencies

[21:44.960 --> 21:50.760]  are, if we can create, if we can put a chip somewhere in the brain, it could be anywhere.

[21:50.900 --> 21:56.200]  It could even be outside the body that can send and receive electromagnetic frequencies.

[21:56.200 --> 22:03.400]  According to the communication protocol of the brain, which, which I am, I just like to call

[22:03.400 --> 22:08.820]  some, I mean, I should have a really cool name, like, like ATP, like, because ATP alternate,

[22:08.820 --> 22:15.700]  it would be alternating transmission protocol. Okay. But ATP is also on a biological level,

[22:15.880 --> 22:24.840]  the energy of your cells. So when your cells metabolize your food and, um, that the results of

[22:24.840 --> 22:32.640]  the metabolic activity is the production of ATP, which is, um, your, uh, your currency, your,

[22:32.640 --> 22:44.200]  your cells energy currency. And so that's your money and ATP. And once you have ATP, um, in terms of,

[22:44.200 --> 22:50.900]  you know, network, or you can maybe call it like ETC, like, like, uh, or EMTC or electromagnetic

[22:50.900 --> 22:57.280]  magnetic transmission protocol, that's another cool name, but the idea either way is that it doesn't

[22:57.280 --> 23:01.780]  have to be, you know, it's basically the, it's like, basically it's like saying, okay, we need

[23:01.780 --> 23:09.840]  to alternate between, um, the ability to send and receive, you know, UDP information on one level,

[23:09.840 --> 23:16.460]  uh, and TCP information on another level. And then you have to like further subcategorize below that

[23:16.460 --> 23:20.560]  and this hierarchy of transmission protocols, because you got to have protocols that are really

[23:20.560 --> 23:28.120]  kind of like great at processing audio information and protocols that are great at processing, um,

[23:29.280 --> 23:34.220]  visual information. So I imagine like visual information would, the protocol would probably

[23:34.220 --> 23:39.660]  be like something in the gamma frequency is there are a lot of gamma waves that light up when people

[23:39.660 --> 23:44.880]  are doing very visual things. Um, someone puts on a blindfold, you're going to see a huge drop in

[23:44.880 --> 23:50.620]  their gamma waves. So, and I imagine that the frequency for audio might be like, it could be

[23:50.620 --> 23:57.160]  alpha waves, uh, maybe that alpha waves is the frequency range for, um, the audio cortex or for

[23:57.160 --> 24:03.600]  audio concepts, which an audio concept. Now, if you go back to Jack Gallant and the idea that you have

[24:03.600 --> 24:09.500]  the concept of a dog is near a concept of a cat, but both of those concepts far away from the concept of a

[24:09.500 --> 24:18.240]  house, which is closer to the concept of a, um, motorbike. So if concepts are distributed in that

[24:18.240 --> 24:24.180]  way, that means the, the, and a concept then for the brain has to consist of like, you know, an audio

[24:24.180 --> 24:29.340]  concept and a visual concept. Like imagine when you're eating the tape, the, and you're eating steak,

[24:29.600 --> 24:34.780]  you've got that audio portion of it, right? This is, this is from this, this, this idea is from a book

[24:34.780 --> 24:39.640]  called, um, criteria causation. Actually, it's called the neural basis of free will, criteria

[24:39.640 --> 24:46.240]  causation by Peter C. And the whole, the neural basis of free will part, you know, take it or

[24:46.240 --> 24:52.240]  leave it. The part that you want to read about is called criteria causation. Um, that is the idea

[24:52.240 --> 25:00.140]  that, that are the concepts of our, that are, it's the idea that our, our cells are coincidence

[25:00.140 --> 25:06.940]  detectors. And then from the coincidence is the basis of information is the basis of a bit. So

[25:06.940 --> 25:13.300]  the cell firing twice within a few seconds is the basis of a bit in the brain. It's like an on versus

[25:13.300 --> 25:25.380]  an off. And so the idea is that we can trans, if we can not only notice the, um, the, um, the pattern

[25:25.380 --> 25:36.240]  of steak, but we can translate it into, um, a matrix of, of ons and offs in terms of, um, ions.

[25:36.480 --> 25:48.960]  And then we can put that through a, um, uh, the electromagnetic or alternating transmission protocol.

[25:48.960 --> 25:54.180]  Then we can communicate with the brain and then, but we, you don't, you don't want it,

[25:54.220 --> 25:59.180]  but you don't want to send the exact pattern of steak to the brain while it's having some other

[25:59.180 --> 26:04.160]  pattern. Cause now you're adding two patterns together that do not necessarily go together.

[26:04.260 --> 26:08.880]  But what you would do then is figure out the difference between the pattern that's there

[26:08.880 --> 26:12.860]  and the pattern that should be there. If steak was there,

[26:12.860 --> 26:18.660]  and then you send the difference between those two patterns so that the pattern that received

[26:18.660 --> 26:22.760]  just allows you to have stake into your reality where there was none before.

[26:23.420 --> 26:30.440]  And it just, it's just, um, it's like flipping dominoes in a three dimensional,

[26:30.940 --> 26:37.780]  a four dimensional electromagnetic matrix of particles. Yeah. So once we can do that,

[26:37.780 --> 26:41.240]  that means that you're going to have things in your reality that are not

[26:41.240 --> 26:46.540]  actually there, or your reality itself can be altered to where the, let's say you're,

[26:46.640 --> 26:52.400]  you're sitting in a room and all of a sudden the room goes away and you're on some other planet

[26:52.400 --> 27:00.980]  because we can, because your whole reality can change once, um, once we have neural lace.

[27:01.300 --> 27:05.160]  And after the reason I'm, I'm really interested in giving me neural laces,

[27:05.160 --> 27:08.440]  because I want it to happen. I want it to happen faster. I want everyone to,

[27:08.500 --> 27:10.380]  I want to democratize. I want everyone to have it.

[27:10.380 --> 27:15.700]  Um, and, and beyond neural lace, the, what I'm, what, after I'm building neural lace first

[27:15.700 --> 27:20.400]  and happy to work with anyone who wants to work, tell me, build it. But after we build neural lace,

[27:20.540 --> 27:27.900]  the idea is to, um, uh, build artificial neocortex and then artificial brains.

[27:27.900 --> 27:32.580]  We have, there's a lot of great ideas on how to achieve neural lace and a lot of people want

[27:32.580 --> 27:37.060]  to keep their ideas a secret. And so we were talking about that in terms of if we give away

[27:37.060 --> 27:42.400]  how to make neural lace on this podcast, um, what is our objective then? And should neural lace

[27:42.400 --> 27:48.880]  be democratized? So every company can know how to do it. And my thought there is that we should

[27:48.880 --> 27:52.980]  democratize neural lace because we really want this technology to exist in the world. And we do not

[27:52.980 --> 27:59.040]  necessarily know. Um, cause it's, it's a very complex thing that, uh, to try to solve. And so

[27:59.040 --> 28:04.380]  we, we want to encourage as many people to get on board with, um, helping us to create neural lace

[28:04.380 --> 28:09.060]  as possible so that we can have neural lace in our lifetimes. So someone remarked to me recently

[28:09.060 --> 28:12.860]  that in order to achieve, um, the kind of neural lace we're talking about or brand computer interface

[28:12.860 --> 28:16.480]  we're talking about, you'd have to have a number of like Nobel prize winning discoveries that

[28:16.480 --> 28:22.960]  would happen. It's pretty much simultaneously. Um, and the, and so one of the positive outcomes

[28:22.960 --> 28:28.960]  that, um, of neural lace is, is we'll be able to do, um, it's on the way to discovering it,

[28:28.960 --> 28:33.880]  we're going to have to revolutionize medical, um, neuroscience. And that, that means, for example,

[28:34.480 --> 28:39.580]  um, we're going to have to figure out the, um, transmission. Let's, let's say that the,

[28:39.620 --> 28:43.300]  the brain is, is organized like, um, the brain and the nervous system are organized like the internet,

[28:43.300 --> 28:49.120]  for example, as many neuroscientists believe. Um, we're going to have to figure out what the,

[28:49.120 --> 28:56.360]  um, transmission protocol of the brain is. And if we do that, um, you know, from neuron to neuron,

[28:56.420 --> 29:01.060]  if we do that, that's going to enable the kinds of products like, um, like artificial limbs that can

[29:01.060 --> 29:06.580]  connect to your nervous system or reconnecting the spine. Um, it's not necessarily, um, as glamorous

[29:06.580 --> 29:11.860]  as the science fiction concept of neural lace, but there's a lot of practical medical applications

[29:11.860 --> 29:17.920]  to being able to connect, uh, a limb or reconnect the spine, um, an artificial limb to your nervous

[29:17.920 --> 29:25.820]  system. So you can, um, really, uh, even, even create new kinds of limbs. Um, so these are, um,

[29:26.000 --> 29:32.020]  really positive outcomes that can come from pursuing a direction of research towards neural lace

[29:32.020 --> 29:38.440]  about, uh, new, uh, ways to send data into the brain. Uh, for example, David Eagleman, uh,

[29:38.440 --> 29:42.260]  talks about, um, there are strips that you can put like on your tongue or on your back

[29:42.260 --> 29:47.300]  that, um, the strip is a grid of electrodes and it's connected to a camera and the camera

[29:47.300 --> 29:53.580]  is watching, uh, the world and takes that image and it converts it into, uh, the electrical grid

[29:53.580 --> 29:58.800]  of, of, of, um, of signals on your back or on your tongue. And eventually your brain figures

[29:58.800 --> 30:04.660]  out, um, how to see an image from that. So that is, is a way of, of inputting data back

[30:04.660 --> 30:08.040]  into the brain. In fact, just using your regular eyes is actually putting data back into your

[30:08.040 --> 30:13.060]  brain. So that's, that's another input channel to think about. There are people who are, um,

[30:13.060 --> 30:19.040]  studying medical imaging, um, you know, with, with MRI machines, uh, besides who, um, are

[30:19.040 --> 30:22.860]  putting chips inside, uh, the cutting into the brain and putting chips inside the, the,

[30:22.860 --> 30:28.540]  the patients of who have, um, epilepsy who need their brains opened up anyways. Um, and

[30:28.540 --> 30:32.980]  then, so this has been sort of like a long dream of, of, of, of scientists who study the

[30:32.980 --> 30:38.180]  brain of what kinds of sensors can we attach? And once we have the sensor data, how can

[30:38.180 --> 30:44.400]  we analyze that data? Um, with, uh, with, with new, uh, techniques like, um, deep learning

[30:44.400 --> 30:49.500]  with computer vision, we have, we can create, you know, different kinds of biosensors. We

[30:49.500 --> 30:55.660]  can apply deep learning to these, um, uh, sets of data in new ways and potentially revolutionize

[30:55.660 --> 31:01.100]  medical neuroscience by combining, uh, state of the art AI with, uh, medical imaging of all

[31:01.100 --> 31:05.840]  kinds, not just EEG. But, uh, so that's my thought there. Um, we can totally, uh, start

[31:05.840 --> 31:11.920]  with some EEG products and, and come up with, and the results of that action will, will certainly

[31:11.920 --> 31:18.160]  yield some really awesome new neural lace products. But I just wanted to say that, um, I, I'm really

[31:18.160 --> 31:28.340]  interested in creating, uh, uh, uh, sort of, um, installed, um, uh, wetware, um, sensor for the purpose

[31:28.340 --> 31:36.920]  of just research. But I, I, I was at CES 2017 and I saw this, um, really awesome new, uh, wireless, uh, EEG

[31:36.920 --> 31:42.220]  system. So that's one of the things that, um, companies are talking about right now is, you know, we

[31:42.220 --> 31:47.480]  actually could do sort of a non-invasive neural lace. It gets at the core of our brain by using a variety

[31:47.480 --> 31:55.200]  of, of, um, tools such as, um, you know, like, like they use, um, ultrasonic, um, sound for surgery

[31:55.200 --> 32:01.300]  now. So there are ways to stimulate the brain and read the brain like really deep, uh, without

[32:01.300 --> 32:07.220]  actually cutting into the brain. And so that's definitely, definitely, um, you know, if you look

[32:07.220 --> 32:12.700]  at the, uh, supercomputer that NVIDIA is doing for self-driving cars, that's a really mean and serious

[32:12.700 --> 32:16.760]  machine and they're still figuring out how to do it. So basically their computer has all these

[32:16.760 --> 32:21.960]  sensors, LIDAR, like it was just mentioned, uh, eight cameras besides that and a whole bunch of

[32:21.960 --> 32:26.820]  other things. And it's figuring out not only the spaces around the car, but also it's, it's, it's

[32:26.820 --> 32:32.120]  figuring out what the objects are. Like there is a, it's saying, well, this is, this is a cat and this

[32:32.120 --> 32:37.660]  is a dog and this is a car door and this star is, this car door is going to open or this car door is

[32:37.660 --> 32:43.120]  going to close. And there's a car across the street that's, that's moving at such and such speed

[32:43.120 --> 32:48.340]  and it's going to intersect the path in front of you. These are things that they're having, uh,

[32:48.640 --> 32:52.700]  that require a supercomputer that's going to be installed in the trunk of your car for, for,

[32:52.800 --> 32:58.460]  for driving. But if we could take for women, if we could take those self-driving cars and apply

[32:58.460 --> 33:03.940]  them to the task of, of neuroscience, we could probably create a revolution in neuroscience.

