Neural Lace Podcast 3 (Auto transcript needs fixing)

Apr 22, 2017

Original Audio here https://youtu.be/_yKjtTVoVlU

Audio Transcription by OpenAI's Whisper

So, welcome to the NeuroLaze podcast, episode 3, with Eric Matzner

Matzner

Matzner

Happy to be here.

Happy to have you on the podcast, Eric.

So Eric recently gave a talk on neurogenesis, which was fascinating, in eye-opening.

And while he was giving the talk, we had a peek at his computer, and he had countless

documents that, countless, would you say, he studies the brain.

It's pretty obvious.

Countless research documents into how neurogenesis works, I mean, just the unbelievable amount

of files.

I've never seen that many files.

That was just my desktop, I think you guys saw.

So this man studies the brain, and when you study the brain, you're a neuroscientist.

So when I found out that I would have the chance to talk to him for the podcast, I jumped

at it, because that's what he does.

To be clear, I'm not a classically trained neuroscientist, but I definitely think it's

been more time researching the brain than most neuroscientists, like scientists and

doctors.

I pretty much, through my business of running Neutropics Company Neutro, I am basically

a paid researcher all day long on enhancing the human brain and memory and learning and

the growth of neurons, which is what neurogenesis is for those who don't know.

And that's something that I try to find a practical application for, instead of just

doing research, I live the research.

Eric Massner is the CEO of a Neutropics Company.

So we're finding out what is Neutropics and how do Neutropics interface with neural

arrays?

So that's, if we're using neuroplasticity changes and neurogenesis changes, and since

Neutropics facilitate neuroplasticity, the change between connections and neurons, it

might facilitate the integration of these devices.

I actually was in discussion with a couple doctors, one who's doing some biofeedback

things and otherwise, and mentioned to him how these things, you know, Neutropics might

offer a way for the body to more easily integrate with these devices that are people are trying

to learn and use stimulation and muscle stimulation through their devices.

We've got like Adam Gazzelli has some biofeedback devices and a couple other people I've been

interested in getting into with, but there's honestly not a lot of research in combining

Neutropics with those activities.

There is definitely some research on Neutropics plus brain training, has a better effect than

either alone.

Now, so as a researcher, how far have you dived into like the metabolism of neurons

and glia and how that functions?

So I mean, I'm interested in, you know, the mitochondria a lot, which are making the

energy in the brain, ATP, correct.

So increased brain activity does use increased mitochondrial activity.

Absolutely.

So that when you talk about brain nutrition is kind of a thing that might be required

is to increase your brain supply of the precursors that are allowing for energy building.

You also want to have the phospholipids out there, which are going to help build new cells,

including DHA, like some fish oil, because the brain might hit a physical limitation

on trying to like interact with these machinery and it may be augmented by these substances.

And I've been looking into, I haven't done enough research on them, but I'm starting

to look at cochlear implants and other type of implants and see what you can do to like

more rapidly integrate those into the brain, because I think there'll be something analogous

for when we're looking at neural laces.

So I spoke to David Eagleman on November 19th, and I was, it was at an event and I just,

he was talking, I don't actually remember if he was talking there, but he, I asked him

about my hypothesis about how we can send VR directly to the brain.

But I was careful about how I phrased that I said, you know, if we stick all these new

sensors in the brain, won't they inhibit the other senses similar to how a, well, so my

idea was it won't a new sensor input regulate the intention gain of the brain and inhibit

other senses.

And I don't think at that time, he was on the same page with me about what would happen

because he talks a lot about, you know, adding new sensors to the brain.

So a kind of reverse example that I could show him is that if you put on a blindfold

for 90 minutes, in total 100% darkness, your visual cortex seems you no longer have eyes

and you may start to have like audio hallucinations if you're listening to music or a podcast

and your visual cortex may begin to process what your ears are hearing.

So what I'm talking about then with neural laces is that when you reverse that process,

if you take the, or even whether it's neural laces or whether it's just, you know, adding

in 10 extra eyeballs, that was the correct question I approached.

If you add in 10 extra eyeballs, you're basically doing the reverse of wearing a blindfold.

And so when you take the blindfold off to your brain, you're now adding a new sensor

inputs and these new sensory inputs inhibit the auditory processing back to the same levels

they were at before you put the blindfold on.

So if you gave yourself 10 pairs of new eyes, it's going to inhibit some other part of your

brain function that operates for your other senses.

That's my hypothesis.

At the time, I didn't get the chance to get all of that out because, you know, we were

at an event, but I hope to convince him, hope to talk to him again about that topic.

And yeah, so that's, that's what I was talking about with David Eagleman.

Yeah, I've talked to Eagleman as well.

He was one of the people that I actually spoke with him after he was interviewed by Adam

Giseli.

And, you know, he didn't actually, I wrote him an email and I sent him all these papers

and, you know, about mossy fiber reorganization, like, and things like that in increases in

like, you know, the waves in the brain that are caused by new tropos, I didn't get a response

for him, but I think that he probably doesn't necessarily know the answer.

But I think that any extra usage of the brain, including 10, you know, eight more eyes or

10 more eyes would cause an increase in energy in the brain.

Fortunately, the brain is like really efficient, but, you know, weighing only 2% of our body

weight, it uses 20% of energy.

And there is actually limitations evolutionarily on that energy usage, because if you use too

much energy before you could get your next meal, you would die.

And so I think part of what new tropics do is they allow us to, for example, like use

more of the neurotransmitter acetylcholine beyond what would be a normally okay level.

And so some people, if you don't supplement with acetylcholine or another precursor for

acetylcholine, you can get a headache or you can have other issues.

So in my builds, we like put that in there, but I definitely think that shows the demand

is increased.

And, you know, also I know about neurotransmitter precursor signaling, where if you have those

precursors in there, your body will naturally amp up the production of those downstream

neurotransmitters.

So, so you were just before the talk today, you were showing me this, this was one researcher's

he has this injectable, was it injectable probe, I guess it's sort of a probe that

you inject.

And it's like a, it's like a, it just, it's a sensor that you inject and then it sends

a signal outside the brain or it looks like they're going through the brain through blood

vessels through a vein, through a vein to basically try to get in the brain without causing

physical damage.

And these are nanomachines and they're planning to inject rats with them.

I think they got them into some rats, I think we're looking at two different things.

One was a guy, I believe he's an Australian researcher who has that probe and then we

were looking at a paper called synthetic injectable synthetics or something like that.

Which looks to be pretty amazing if we can get that technology into rats, to human brains

and you know, in the rat brain, I think, you know, the brain is difficult in general to

go into because once you're in there, there's a lot of trouble you can cause very easily.

And I think that that was part of the issue with that guy with the probe is that they're

going through the blood, the vessel, so they're not going directly into the brain, they want

to like pop out inside the brain, kind of like use it as a source system and a back level

in there.

I've seen some other promising technologies.

One of them was a friend who says this company was working on, I don't know if it's indiate

or not, but basically they inject something in your shoulder and your collarbone basically

near your neck and then they grow neurons up, they're trying to build a link from that

device that sits outside the brain and build a neural pathway up your neck and back into

your brain.

So that one's kind of like an interesting one to me because they're recognizing that

they can't go directly in the brain as easily and they found a way to do it outside and

then hook into the brain's network.

Question I have is what happens once you program into the brain, what kind of information are

you going to be getting, what kind of signal are you going to transduce, are you going

to be able to just come up with a yes or no the device has and maybe that's enough of

a signal, are you going to be able to like wire into some other processing that?

There's a lot of different sensors and new ones that I'm learning about every day.

There's a lot of companies that have been stopped that are just announcing new sensors.

A couple of years ago I saw a sensor that's basically, it's not only a sensor, it's also

a transmitter and so it senses electromagnetic waves and transmit them.

There's a number of companies that are building artificial neurons.

One in terms of like new tropics and energy demands are putting on the brain and ultimately

an artificial neuron to really behave like a real neuron would be able to supply the

brain with energy in terms of being a proper interface between the brain and not just like

feed off the brain or interface like a normal neuron.

So that means it would need to have the similar structure and it would need to release neurotransmitter

chemicals.

So if you're making a hard machine, you're really going to have to keep resupplying those

neurotransmitters or have some way that it can what's called endogenously produced.

So it either needed, what it could try to do is potentially tap into the body system

for restoring these neurotransmitters already and put those in.

The question is, could it generate its own new neurotransmitters as things called neuropeptides

for short chains of amino acids, they can signal as if a neurotransmitter.

In my gold pills, there's something called NUPEP, which is a Russian dipeptide form

of paracetam, paracetam being the original and the tropic.

A dipeptide form being the short chain of amino acids has a much easier way of crossing

the blood-brain barrier in the stomach and other ways to the body, but it's considered

a neurotransmitter in that way.

And there are also hundreds of neurotransmitters, we don't even know all of them, and we don't

even know even the limit on how many there are, but realistically we talk about less

than 10.

People know about serotonin, dopamine, acetylcholine, epinephrine, and I don't know what other

people know about.

That's what I mentioned that biohackers working on creating artificial cells from DNA.

So hopefully that means eventually, and it's still like this hasn't happened yet.

It's sort of like one of those holy grail moments, like can we create an artificial

cell?

Actually, I don't know.

There has been some recent announcements.

I mean, there is somebody who has made, they've made an artificial life form that has its

own type of programming.

Whether that is relevant in the brain, I don't know.

I think we'd be much more able to like, I think, like looking at like TCDS and like sending

electrical waves to the brain.

Transcranial, direct current stimulation.

Yes.

Looking stuff like that where you're like physically stimulating the pathways or they

have, you know, where they optogenetics, where they put in a light sensitive gene from a

jellyfish that when you put light on it directly, actually on the brain, it turns on the brain

engineering human brain cells and then injects them into the brain.

Yes.

It would involve modifying the brain's DNA.

And those, I thought, well, transcranial magnetic stimulation seems like it reaches

deep into the brain from remote.

The big problem with that, I heard, is that it's actually like trying to interface with

the brain with a shotgun.

It's like too big, but they're currently working on reducing it to, so it's less like

a shotgun and more like a surgical knife.

Yes.

They have some that are implant in the brain for people with strokes and other types of

ailments that will, you know, try to stabilize the brain or fire.

You know, we're starting to get pacemakers built into the body that automatically like

gives your heart a shock if it's needed.

So I think that there's things that are making progress, but again, yeah, they are definitely

shotgun type approaches.

The brain is just difficult to experiment in human brains.

Like even cracking open a human brain is not something you do lightly.

So I think we really need to figure out methods to do this in animals.

And that means from whatever's forgotten animals to humans is 10 to 20 years at the current

cycle.

Hopefully things speed up.

Okay, so what I'm advocating is that we use some new wireless technologies that I know

about are coming to the market.

I mean, there's a lot of different ways we can go about solving our list, but we should

use wireless.

For example, let's say let's take the topic of tractography, which is usually associated

with MRI and diffusion tensor imaging.

The traditional course of tractography is that you do basically an MRI scan for multiple

angles, at least three different angles, and then you combine the different, you like

you have an X capture and Y capture and a Z capture, and you combine the different angles.

The computer figures out the geometry of all the, it's receiving the image capture from

different angles and it's creating an image in three dimensions.

That's great if you want to control your computer while sitting inside this giant MRI machine

in a hospital.

So little known secret is that tractography is not, it's tensor calculus, it's not limited

to the domain of giant magnetic machines or MRI.

So if we can stick multiple sensors in or near, let's say that we stuck, we can do tractography

with other kinds of sensors, which is the point I'm making.

And that means that you're taking multiple snapshots from different angles of the brain

and then you're using a computer program to stitch together those images and to create

a 3D model of that space.

Even with, you know, the limits of what you can do with MRI though are not really reached.

Like currently MRI, you know, you're getting like the box of size could contain, you know,

like a thousand neurons, for example, or many more than that.

And that's not the definition we're going to need for neural lace.

But if you're taking 3000 images from multiple angles, right, and you're doing tractography

with them with a massive amount of images, and then you're thinking about something from

virtual reality, which is called, which people have tried, there's, it's called photogrammetry

where you can, there's an app on the buy called realities.io, where you can walk inside a virtual

room that looks it's photorealistic and you can lean around when I was at CES 2017, I

tried this new technology by a company called HypeVR.

HypeVR introduced the ability to walk around inside a video.

So you're moving inside a video and at the XTechExpo recently.

So the PlayStation VR chief, Dr. Richard Marx, showed me a demo of the new Sony technology

being developed in the UK, which allows six degrees of freedom, position tracking at the,

from a regular 3D 360 video.

And actually, Facebook just unveiled some new cameras that at F8, that actually make

a lot, basically, they're not going to sell the cameras directly, but it's a hardware

spec that allows you to have, you know, either an X24 or 24 cameras or six cameras in one

device allows people, less professionals to capture video with six degrees of freedom.

With both techniques, though, you obviously would have some occlusion, depending on how

far away you move, attempt to move from the origin point of the camera.

So these are primarily aimed at professionals to record like concerts and movies, plays,

live shows, and then the concert itself, the concert hall or the producer of the film or

the of the show would then do a lot of post production to fix it.

So that is the technology coming down the pipeline that allows us to do basically video

grammatry to move around inside the videos in real time.

So the next step then, of course, is to take the basic, the core of that video grammatry

process, which is photogrammetry and applied to medical imaging, like a massive, like

your camera looks amazing.

It's massive and it's super expensive and it creates like, you know, that's light field

is amazing, but it's it's also a firehose of data.

So Hyde VR has got this advantage where they're using actually just normal compression, but

they're bringing down the size of that data to something you can you can download on and

watch on your Oculus, which is a huge step forward.

You got to you got to compress it.

You got to make it shippable.

But now Sony in the UK has come up with a technology where you can take a standard 3D 360 rig,

which is so much less expensive, and you can make a you can make a movie that you have

position tracking and I was in this was in this concert hall.

And I noticed that the reflections just like looked a little bit too good.

I could lean around corners, the parallax is too good.

And this is because, you know, when you do photogrammetry from a fixed point of view,

you can't see what's around the chair.

So so the solution, this is going to be an enterprise solution.

It's not immediately like available to, you know, this is for people who make it's going

to be for people who make concerts to actually, I need to get through this little bit faster.

Yes, sorry.

Yeah.

So but what are they going to do with the technology related to neural aces?

Okay, so so the point being is that this technology is coming down the pipeline so fast.

What you can do with it, though, is you're turning four dimensional video and the computer's

figuring out the geometry from four dimensional video from all these different angles.

And it's creating like, so this is the technology that we can now apply to medical imaging.

So instead of like focusing on the outside world, we turn this around and focus it on

the inside world with, so we take 3000, you know, images of the brain with tractography

from different and we're combining it with not photogrammetry, videogrammetry, right?

And which is a massive, you know, computational problem, but we do that and now we can track

like, you know, basically like, you know, everything going through a specific region

of the brain, or at some point, we're going to be able to do the entire brain.

Is it still concerned neural aces if it's purely external on the brain?

Well, so the information, so yeah, because if it's pure, so we need to, we need to do

two things with neural aces.

One is sense and one is transmit.

So we need a two way directional information.

So for the research purposes, that will get us halfway there.

Then we need to transmit the stuff back into the brain.

That's the other half of it.

What I'm saying is that there's all these, so we have, so what I'm working on is new

designs for combining new forms of tractography with technologies like videogrammetry to do

the, to figure out, basically we're getting a lot of new brain data.

Then what I want to do is take 12 DGX-1 supercomputers and video supercomputers, each costs like

$129,000.00

And we're going to take that data and see, there's a lot of brain wave data that, there's

a lot you can do with a supercomputer right now that's not being done.

Current EEG has not even reached its potential.

Current MRI or DT has not reached its potential, not yet.

But I want to take this, and I want to take, I want to do sort of like a self-traffin car

project where you are analyzing the biomarkers of the person, their heart rate, their pupil

dilation, their eye movement, their body movement, their, all their medical, all their medical

imaging, you know, their eye and everything.

No biometrics are good.

And then, and then we will need to do the environment too.

So the environment is like, you know, Facebook just announced at the F8 today that they have

AR product that they're shipping and AR is computer vision of everything on your table.

Your cup, your glass, the table itself, it says this is a bag and now it's made a virtual

object of it so that your apps can, with your phone, interact with your table.

It recognizes everything around you.

So we need this kind of technology, which they're taking from like, you know, in this

super, the, the supercomputers that goes in the back of your self-driving car and they're

making it work on a little phone, which is just incredible.

But we need to categorize the objects in, in the environment.

And then, so this is what the 12, this is what the 12 supercomputers are going to, they're

all going to be categorizing what's in the environment with what's going on in the brain

and they're making the links between the neural correlates between the object on the table

when you're looking at it with the eye tracking and the pupil dilation and how you respond

and then the, the, the, the tractography of your, of your brain and how you're, what's

happening in there in terms of electromagnetic waves or neurotransmitters and so we're going

to figure out what those signals are exactly.

And I think we start to translate what we're seeing and doing and feeling and the space

so that we can let this device, it's on us or in us, yeah, comprehended.

I think that's a starting point.

I think as much as you could do outside of the brain to start processing for the neural

lace is, is important.

I also think it's possible, like for example, I have something called a North Paw and it's

a magnetic, it's a band that goes around your ankle, it has like eight pager motors,

one in each direction and it has a digital compass and it constantly vibrates the direction

that North is in.

Yep.

And if you wear this for a while, you start to just know, like it's literally vibrating

all the time you have it on and lasts for like eight or nine hours.

And so all the time it's on, you know which way North is.

So your brain starts to feel this and you eventually stop feeling the vibration, you

just know which way North is.

So it becomes pretty easy to like add in information into the brain, which is something

I really believe the brain will function like a USB port.

I mean, I met Neil Harbison at a body hacking conference in Austin a year or so ago.

And he, you know, Neil Harbison is a first official cyborg.

He has a picture with his passport, went with the device in his head.

He's colorblind.

And so he actually has a camera in his head that detects color and transmits it into vibration

and vibrational signals that correspond to different color, to different sounds.

And so he actually, he's a real synesthetic where he hears color.

And so he could look at you, you have a black shirt on, he would know the tone, even without

being able to detect it, he could look behind him and he's actually able to basically feel

color in a way and feel a new sensation of how someone lights up basically how they sound

from their color.

So he's really doing something interesting and he could tell you what color it is based

on the sound.

And he has a true innate feeling that is like an add on into his brain.

So I think that our brain has the potential to pretty easily add these things in if we

feed it the data.

So I think, you know, playing off what you're saying, if you, if we figure out ways to feed

it this data, the device can like maybe understand more without having to do any processing in

our brain and it can then interpret it and signal to us what's going on.

Yeah.

And I also want to, you know, so I also want to talk about the feedback that the neuroscientists

use.

We were talking about before the podcast neuroscientists are beginning to speak up about provide feedback

about what they think is possible in terms of neural lace, you know, especially with

narrowly being a big topic.

And I've been getting feedback personally like people are like, well, you know, why would

you like give away, you know, the secret, don't give the secret sauce away on your podcast

and like, are you devaluing yourself?

Like what's your value proposition?

If you want to work with a company that's doing neural lace tonight.

And so I wanted to maybe bring that into the conversation a little bit because, you know,

we're talking about, you know, here's the thing is that the thing I want to say is there's

no secret sauce for neural lace, you know, I could talk all day about all the things

we can do in terms of computer imaging in terms of, you know, but that doesn't see,

you know, Google can give away TensorFlow and they did.

TensorFlow is, you know, the deep learning library.

But the thing is, it doesn't replace the fact that, you know, Google has its own chips,

they have their own servers, the servers are massive, they have their own engineers, the

engineer themselves are not giving away their value.

The value of creating the neural lace podcast and talking about all this stuff is that I

can reach more people with this podcast who will say, Hey, man, that's a great podcast.

Here's what I've been working on.

And they contact me.

And for example, like I reached out to Elon Musk and I started, I raised a 10 million,

you know, go fund me.

And I'm not relying on that at all.

I was like, why not, why not do it?

Some people are like laughing at that.

And I'm like, why not?

You know, the thing is what happened was someone, someone called me the next called me up the

next day.

And actually we're going to talk tomorrow.

And the thing is, they basically they're saying, okay, what would you say to Elon Musk and

all this stuff?

And I said, do you, and then I'm answering, or so do you know what Elon Musk, he says,

no, but I know this other guy, and this other guy wants to do, my impression is this other

guy wants to do neural lace.

So it turns out it's not just Elon Musk, it's not just Carl, it's not just me.

There are other companies in stealth mode that want to do neural lace and that's the value

of this podcast.

I'm not giving away my value, because there is no single secret.

I mean, I know my value is not just the libraries in my head.

It's also my network.

I mean, I know companies that have products that like new sensors that have, you know,

a massive capabilities that I know Elon Musk doesn't know about.

So if companies want to work with me and we can, we can beat Elon Musk to the punch.

That's what I'm saying.

Personally, I would never bet against Elon Musk.

I know George Hots is a friend of mine and I've seen him bet against Elon.

It generally doesn't seem to work out well.

I also, you know, I'm on a hyper loop.

I'm on a team building the hyper loop in the Elon Musk competition.

No, no, I'm a fan of Elon Musk.

I'm for encouraging the diffusion of these type of technologies because they offer us

a way to upgrade ourselves in a serious way.

I constantly wish I was a cyborg or I could clear the internet and get an answer back.

That would be probably, you know, a whole sea change in terms of learning because at

the moment I struggle with the physical limitations on integrating information into my mind.

I read at, you know, 1200 words a minute or so, which is basically at the beyond, well

beyond the normal.

Nice.

Is that photo reading?

I use a form of what's called RSVP rapid serial visual presentation where it puts one

word at a time.

Oh yeah.

And I've tried that.

That's an algorithm.

The company named Spritz kind of redid this algorithm and then got ripped off by everybody.

Nice.

And so you can find generic open source versions of it where other ones are just.

Yeah.

If you really want to, like sometimes when I really want to digest a book, I'll read

it the normal way and then I'll do it also with that app.

I just speed read first.

If I really felt like, you know, people say that to me all the time, but if you're reading

a book and it takes you like a month because I'm reading a chapter by chapter, I read

Ready Player One in like three hours.

So, you know, it's a few hundred thousand words.

I was able to take it in.

That sounds really reasonable because it's actually the audio book version of Ready

Player One is six hours.

Okay.

Yeah.

So the speed reading of three hours is totally reasonable.

It's your brain can handle it for sure.

Yeah.

So I mean, there is a limitation on like how fast audio processing.

I think there is not really a limitation on how fast your eyes can process if you're

able to like form the syntax around it.

I kind of use my own tortoise synesthesia to do that, but I would love for those words

of those thoughts and those ideas to pop into my head.

So one of the ideas, David Higelman talks about, he says, if you put a blindfold over

your eyes for 90 minutes straight, what happens is your visual cortex, which takes up two

thirds of your brain activity, which may be part of why your eyes process stuff faster,

two thirds of your brain is doing your visual stuff.

If you cut that off for 90 minutes, like total blackness, not a single speck of light, your

visual cortex will then start to process other stimuli that's coming into your brain, like

from your ears, for example, or from vibrations.

The light is the fastest traveling thing we know.

And so the neurons in the brain actually fire much slower than the speed of light.

But the idea, though, is that eyes evolve in nature because it's the fastest signaling

and whoever's the fastest is going to survive that interaction.

So you have multiple development of eyes throughout.

But what's interesting, though, if you're listening to an audio book after you turned

off your visual cortex, then you start to visualize the audio book.

And so now you have your whole visual cortex integrating that audio information and working

on the audio information.

And I think that's exactly what I'm saying, though, as well with the Northpaw and these

other things is that your body quickly adapts and the brain can reroute around.

When people do go blind, their sense of touch, like the sense of touch of Braille readers

is amazing compared to like you or I, like we would not, you could touch us with something.

We wouldn't know it, but a Braille reader would know a lot more.

So one more thing I wanted to bring up in my podcast is that, you know, when someone

does, one result that surprised scientists recently when they do MRI scans was that the

brain wave pattern that when someone gets when they're looking at a real apple is different

from what they get when they're looking at a virtual apple.

And they couldn't figure out why this is.

And I think there is an obvious answer to that.

So I read this book and scientists should read this book, it's called Criteria Causation.

Actually, it's called The Neural Basis of Free Will, Criteria Causation.

I think a lot of people did not read this book because it has the word free will in

it, but you should read this book.

You especially should read this book because you're a hard, because you love neuroscience

and this is a hard neuroscience book, The Neural Basis of Free Will.

Peter TSE, I don't want to mispronounce his name.

He's a brilliant guy.

You can watch his, well, some of his talks on YouTube.

So Criteria Causation is this, it is the answer to why you look at the way your brain is going

to be different when you look at an apple versus when you see a virtual apple.

And this is important to neural lace because if we send an apple through the computer and

through the neural lace, you know, okay, so we have the brain imaging, okay, if you know

you're getting an image through your neural lace, then the brain wave pattern of an apple

you're getting through neural lace should be different from a VR apple and it should

be different from a real apple.

And the reason why is because you know it.

So it's like, okay, so, and this also, I don't know if this is going to be too much for one

podcast.

So, so the idea is from Criteria Causation is that your neurons are coincidence detectors

and they're only detecting, you know, like he'd be in learning, they're only detecting

when a neuron receives a signal between a certain span of time, which is it's modular

that neuron can adjust.

So that could be like three milliseconds, if it gets two signals within three milliseconds,

then it activates, right.

And so you have this coincidence pattern and you have a network of coincidence patterns.

So let's say that in the brain, a quint when your neuron activates, well, this is this is

a computational biology idea from, you know, the idea that when you're when you're in your

action potential fires it's a one and when it doesn't, it's a zero.

But let's say that, I mean, it's, and now we know it's actually more, you can't summarize

it so simply.

Now we know it's a little bit more complex than that.

Yeah, it is.

But you mean, because because a neuron has this, the dendrite is itself a computer and

you can't just really like reduce all information to a one, besides the action potential has

amplitude as a wave, and plus you're really talking about whole networks, you know, not

the signal.

Yeah, I think I think part of the problem is, is beyond the such simple signaling.

Yeah.

If you look into something like quantum nervous, like quantum biology, sure, yeah, it throws

all this stuff to the wayside in some way, not all of it to the wayside, but there are

some complex things like quantum tunneling of the mitochondria, the observer, the observer

effect.

How do you square the observer effect with neural A's?

And the, you know, I do have a proposal for that.

Yeah.

Well, I'd like to see it.

I mean, I think there's some interesting, you know, ways we might go about, you know,

figuring this out.

If you, you know, when you look at like the small brain mammal preservation prize, they're

like turning their brain to glass, right?

They got all the position of the neurons, but they lost all the metadata.

So my thinking is that, you know, that a particle is like, it's like a polarized wave.

And a wave is like a, the decoherence pattern that you get through the double slit experiment

when you observe it is like a depolarized particle.

I mean, usually when you talk about polarization at that level, you're talking about, you know,

as a particle travels through space, which direction is it, is it pointed at, you know,

the electromagnetic wave, but a particle itself could be also considered a kind of polarization

to different, different kind of polarization.

So my thinking is like, what if a particle is like a one and the decoherence pattern

is like a zero?

Okay.

And then now, so when I observe a particle and there's a feedback loop, so this is what

I said is, is when we communicate, when you have neural layers is if the brain is like

a network like the internet, is it more like TCP transmission control protocol or more

like UDP user datagram protocol.

And so with TCP, you actually have to have a connection of feedback loop to between the

two points.

If we have a connection to what we're observing, then observation and the observer effect is

more like TCP.

Right.

It's not, we're just, it's not what we intuitively think it's not just a packet of data thrown

at us.

We're making connections to what we're observing.

It's a packet signal and something coming back.

Yeah.

And so that's why, so when we observe something, if it turns from a particle into a wave decoherence

pattern, it's like when we're observing something, we're getting the one, the one's coming to

our brain and it's turning into a zero.

It's like the one is traveling from the universe into our brain.

See?

Yeah.

It's interesting.

I mean, there's some weird things with the feedback in the brain.

I mean, like I wanted to, I didn't want to interrupt you before, but when you're, when

you're talking about the VR world being different than that, I gave a talk called, I have a

talk called VR in the mind.

Yeah.

That basically looks at the neuroscience between virtual world and this world and what makes

you basically embedded into that.

What makes, what are the properties that make you actually feel like you're in there?

By the reason people fall over when they're like, when they're in there and why, you know,

part of it has to do with the speed, the size, the color, and if you get these like four

or five different things right, the brain does believe you're in there.

But then there's a number of experiments they do where they take, for example, they'll put

your arm, they'll put a heating band on your arm in the real world and then they'll project

a fake arm in the world and then in the same position though with another band on it and

they'll change the color of the band, depending on, and they'll ask you at what temperature

does it seem too hot?

And so if they have the color as blue, which is traditionally in the brain, a signal for

cold, the temperature which they feel it's hot is lower.

If it's red, sorry, the temperature could go higher when it's red, they feel it's arms

burning before it's reached that same temperature.

So how do I know something is, is hot or cold or how do I have a concept of hot or cold

or how do I have the concept of a cup?

Well, in deep learning a concept, which deep learning is sort of based on neuroscience,

a concept of a cup basically is sort of a map of all the links between all the different

lines and edges that the computer is seeing that make up the cup, that it's associating

with the cup.

So it's really a map of associations, any concept in any domain, whether it's audio

concept, concept, whether it's audio visual concept where it's like, you know, I have,

I see a microphone and I hear, I mean, I see a tripod and I hear a tripod when I knock

on it and I'm not going to knock on it because I don't want to mess with the microphone.

If you've had any kind of concept, all it is is a map of the associations between all

the different lines and edges and colors or sounds.

It's just a map of bits, yeah, into a pattern, it's like a bucket of ideas, but there's a

coffee cup and there's a water cup and yet one has a handle and one doesn't, but our

brain knows they're both a cup, right?

So the idea is that what I'm seeing on the table though is a concept, I'm not seeing

anything other than a concept.

That's why like, let's say the corner, for a human, in the corner of your eye, you don't

have the same sort of color vision in the corner and you only focus on what you're directly

looking at.

So what's to the side of your eye is kind of blurred out, but yet you experience a table

as, you don't experience a table as something that loses color in the sides of your eye

or something.

What I'm saying, the point is that what we're seeing is a concept of the table.

There's all sorts of optical illusions you can see to sort of like, you know, there's

actually a hole in your eye and you can use a pencil at about the length of six inches

from your eyes to figure out where the hole in your eye is.

You're not seeing anything at all with that hole, but you don't notice there's a hole

in your eye and your everyday observer thing.

So we're not seeing anything except for a concept and the concept is actually consisting

of, so let's say that we have, you know, in the brain with criteria causation, we have

networks of coincidence detectors or networks of, so coincidence is a bit and we have all

these bits that are being, the associations are being mapped until I have the concept

of a cup.

This is in my mind and we're also mapping this with imaging.

So this is what we're doing when we see a cup as we have a map of associations of coincidences.

And then what I'm doing is taking this map and, you know, capturing it with all these,

you know, sensors and all this artificial intelligence technology.

And then once I have it, once the computer knows that, oh, this is the, okay, so back

to the apples, back to the apples is because we're running out of time.

So the idea is why should the brain image of when you see a real apple be different

from when you see a virtual apple and when you see an apple with neural lace.

And the reason is because an apple is in some sense the criteria summation of all the different

pieces of criteria that you've put together.

So because you're in an MRI machine and you know you're in an MRI machine and you know

you're seeing a virtual reality, the sum total of your experience that you're observing

is based on all this criteria that's based on all the knowledge of everything that's

true and that unique brain wave pattern must be different.

In fact, because the, between the two, between whether it's a real apple or virtual reality,

because the information that's summing up to that unique brain wave pattern, that neural

correlate is has to match the, the, that's some of that information has to match the

sum of the, of what's true for that situation.

So if you're seeing a virtual apple, a virtual apple, it should have a different brain wave

pattern from seeing a real apple because it is different summary of information in your

real life, different brain wave signature.

What about every, isn't every apple you're going to see throw a little different brain

wave, like a red apple versus a granny Smith versus a, yes, yes, a granny Smith, a green

apple and a red apple should have a different neural signature.

Yeah, that's, that's what I'm, I wonder about honestly, my biggest question is how are we

going to get to like a drilled down level of the information going across into the brain

and out of the brain?

Yeah.

Like I use some of these biofeedback, neurofeedback headsets, for example, I can use where I meditate

and I can control the weather pattern and I have a game where we can measure the brain

intensity and I can hover a ball at different levels and I can move through an obstacle

course and I, there's all these things, but the, and you know, I'm ready to, I make a

joke that I'm ready to start shaving my head just so I can wear the EG headset, you know,

like, yeah, but I, you know, I'm much more at this point, I think there's a lot of stuff

that could be done from the outside of the brain.

And I think that, you know, then we, we would probably move into the neural ways or something

inside the brain that can, I think we should really be really good at reading before we

start trying to write and like to create, but obviously that's where the big benefit

is, is like, is being, but I think you can also write by, you know, there's these other

trainings of brain waves where they say like, imagine you're closing a box or you're like

opening a box and then that could correlate to something completely different and you

just trigger that visual fingerprint of electrical activity in your brain and it can like open

and close the door, you know, and so nice, I think a lot of it could be basic like that.

I mean, I have, I was thinking that the, you could also use some audio input into the neural

way so you could say the thing or you could at least think the pattern it might go into

your brain.

How do we get things into the brain?

So generally there's something called the blood brain barrier, which is as it sounds

the barrier that stops the particles of a certain size or certain shapes or certain

class from entering the brain.

However, I have seen some recent studies that might allow us to temporarily put down this

barrier.

There's an ultrasound technology that's using little tiny micro bubbles to basically fire

if you fired at this barrier, it puts the barrier down.

And so there's going to be a lot of other molecules that maybe had been too big or had

otherwise been ruled out.

So I think also that part of the brain is actually below the blood brain barrier.

So we could target, if you like the old brain, if they've talked to a lower part of the limbic

system, especially the part, you know, coming into the spinal cord area, that's another

way to read.

It's just another option.

If you're trying to hit that like quote unquote crock brain, you have like some of your,

that's more where your reactions are and things.

Well, I think a lot of the neural lay stuff, but it's also, so it's also actually all your

incoming senses hit the crock brain first.

So that's like the convergence point of all your incoming senses.

So it's a key interest for now.

Yes, but your thought and your conception of things that are not immediately based in

this moment where you're like, that's your prefrontal cortex.

It's kind of the later parts of your brain to evolve.

And where I think a lot of neural lace activity would go, so if you're trying to shock me

to get up, you want to maybe use something in the cerebral cortex.

To debate you on the topic, my feeling, and which could be wrong, is that if we target

the old brain, we can, we can, I want to do VR for the brain, which means VR and AR with

our glasses.

So I really am focused on the old brain because I just want to be able to send an image to

you that you interpret as VR or AR.

So that's why like, you know, I'm into like the sensory areas, the primitive primal areas.

You have basically a whole other area for visual processing.

I'm not interested necessarily in messing with people's high level thoughts, not yet.

The first product of neural lace that I'm interested in building, we'll just mess with

the, not mess with, we'll be targeting the lower level primal sensory inputs.

That's interesting because I would probably, as a matter of debate, think that the best

spot would be, would be to go to the higher levels.

Because that might offer you more flexibility in generating images or things to force feedback

into the brain.

One other thought on that, on the note of that debate real quick is that the, you could

sort of imagine, I imagine the brain as a feedback loop as a whole, as basically a fractal

of feedback loops, if you will.

So there are, you know, grand feedback loops, a small feedback loop.

So everything comes in through like, through our senses, to the thalamus, to the old brain,

it goes through the neocortex, which is the new brain, the new rind, but then at the peak

of the neocortex, it actually comes back to the old brain, back to the thalamus.

As if the thalamus is both the entry point of the senses and the top of the pyramid of

the neocortex at the same time, because it's a big loop.

So that-

I understand that, but some of your context of the cup and the apple, if you read about

people have a stroke, they can't recognize basic objects sometimes.

Well here's another possibility is if we can just interface the brain at any point, regardless,

and we can call different parts of the brain, but basically like if we have the transmission

protocol of the brain, which I like to, it maybe has to alternate between, you know,

the two, like UDP and have, you know, self-authorizations like HTTP, but for the brain.

And once you have that communication protocol, if you have a brain port, no matter where

you're connecting to the brain, you should be able to request any other information

from any part of the brain.

I don't know the word.

Have you ever heard of the claustrum?

Of course, yeah.

The claustrum.

The claustrum, yes.

So the claustrum was Francis Crick of Crick and Watson.

He was working on that until he died.

I recently became very interested in it.

It might be another topic for one of my talks, but he basically posthumously had published

a paper on the claustrum.

He was working on that until the day he died.

He really believed he found the center where consciousness exists in the human brain.

Whether or not you believe that, I mean, there's some, there is some evidence that it actually

plays a role integrating information from all the different areas and could in turn

actually be related to generating consciousness.

So I don't know, you know, think we even know the real target point of where we want

to enter the brain.

I think if we want to have the neural lace, be able to like lift your arm up, you know,

we might want to hit deeper in there, but we want to like imagine a spaceship floating

on the table that we're going to want to hit the higher level parts of the brain.

Great.

Great to debate with you.

These are all great ideas.

Definitely, you know, it's not, it's not a really what our debate is pointing to the

fact that there is no secret sauce for solving our lace.

We can talk about neuroscience all day long, but you're not going to, you're not going

to get the magic trick from, from a single podcast or a series of podcast series is too

much information.

I think we're early enough along in the neural lace race, the neural lace, turn to a poor

mentor, but basically no one knows how it's going to be solved yet.

I think there's some leads and some, you know, hunches and, but what we can say is that we

can see that there are things that we can do that have never been done before and we're

going to do them.

Yeah.

I mean, I definitely, uh, I'm bullish.

I think that we'll reach some level where it doesn't make sense to, I'm talking about,

I'm talking about applying self-driving car type technology, artificial intelligence,

computer vision, all this stuff hasn't been done yet.

Why not?

Let's do it.

Well, Mike, I hope you do it.

I would love a neural lace.

Let me know.

All right.

Cool.

All right.

Thanks, man.

Thank you.
