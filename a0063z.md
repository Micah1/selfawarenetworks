a0063z ctp&utg
This note is from Oct 5, 2012, 9:04 AM
 (hebb, dendrite, causation, graphics, qualia, category, theory) Neural Darwinism
Computer vision is much more computationally expensive than computer graphics —Induction is the expensive direction of causation
by Juan Carlos Kuri Pinto on Thursday, October 4, 2012 at 10:07am ·
Computer vision is much more computationally expensive than computer graphics —Induction is the expensive direction of causation
 
When I see video in a computer, I think: "This is programmed in C because playing video is computationally expensive." And then I see my own visual consciousness and I think: "This should be programmed in C, not Lisp, because my vision operates in real time and it is much more computationally expensive than playing video." Sometimes I think Lisp cons cells are not the appropriate universal for high performance. That's the reason why SBCL is slow in comparison to pure C.
 
C gcc speed / Lisp SBCL speed — Computer Language Benchmarks Game   
http://shootout.alioth.debian.org/u64q/benchmark.php?test=all&lang=gcc&lang2=sbcl
 
If computer graphics flows in the normal direction of causation, deduction, and computer vision flows in the inverted direction of causation, induction, then computer vision is much more computationally expensive than computer graphics because the inverted direction of causation has diverging branches whereas the normal direction of causation has only one path. For example: 3D rendering has only one straightforward pathway (http://jckuri2005.comyr.com/applets/functions3d.html). Playing video in a computer is one path that is very computationally expensive due to its high bandwidth. And inducing 3D patterns from 2D patterns is even more computationally expensive because we have to select the appropriate causal pathway from multiple causal pathways. We need to explore in real time a little part of all causal pathways of vision in order to discard the wrong pathways. That's why flowing in the inverted direction of causation is much more expensive. And it is even more expensive when you flow through multiple high-bandwidth pathways at the same time!
 
This is the normal direction of visual causation. Just imagine how many patterns and pathways need to be explored in order to induce the correct ones. Induction is the inverted direction of causation.
 

Unlike ·  · Unfollow Post · Share
You, Anand Muglikar and Juan Carlos Kuri Pinto like this.
50 of 55
View previous comments

Micah Blumberg Okay so think about this twice before you answer. Blind people who can do echolocation do not render objects in color, they don't know light from dark, but they still have spatial...See More

Blind Scientist Explains How Sightless 'See' Reality
www.huffingtonpost.com
By: Natalie Wolchover Published: 10/03/2012 12:31 PM EDT on Lifes Little Mysteri...See More
21 hours ago · Like · Remove Preview

Juan Carlos Kuri Pinto Echolocation is less computationally expensive than normal vision because echolocation has less bandwidth.
21 hours ago · Edited · Like

Juan Carlos Kuri Pinto David Dalrymple programs AI in pure C. Monica Anderson wants to program her AGI in pure C too.

Linus Torvalds puts it, “C is the only sane choice”.
http://crypto.stanford.edu/~blynn/c/ch01.html
C Craft - Chapter 1. Introduction
crypto.stanford.edu
This is my favourite statement froma talk Rob Pike gave in 2001. Despite its a...See More
21 hours ago · Like

Micah Blumberg "Echolocation is less computationally expensive than normal vision." how do you figure?
21 hours ago · Like

Juan Carlos Kuri Pinto Because echolocation has less bandwidth than normal vision.
21 hours ago · Like

Micah Blumberg Less bandwidth means your getting less information, less information means lower resolution, lower resolution means it's harder to figure out, harder to figure out means more computational resources.
21 hours ago · Like · 1

Micah Blumberg A high resolution Ipad book costs the less computation resources for the brain reading it, because it's easier to read
21 hours ago · Like · 1

Juan Carlos Kuri Pinto How many dimensions does a visual image have? Red, Green, Blue, X, Y, and Time for each eye. 

How many dimensions does a echolocation image have? Intensity, Frequency, and Time for each ear.

Moreover, visual images have much more resolution than auditory patterns.

Thus, echolocation has less bandwidth than normal vision.
21 hours ago · Like

Micah Blumberg But you still have to put the intensity frequency and time in the right location spatially, with a lot less information about where things are, so there is a lot of figuring out space with little information to go on
21 hours ago · Like · 1

Micah Blumberg if echolocation is easier than seeing, then you should pick it up, are you reaching for a blindfold to learn it?
21 hours ago · Like · 1

Juan Carlos Kuri Pinto You are right in that point. I read elders who hear less actually use more cognitive processing at the higher-levels of the cortical hierarchy in order to compensate such disability.
21 hours ago · Unlike · 1

Juan Carlos Kuri Pinto We don't have the Color, X and Y dimensions in echolocation. Blind people use the intensity disparity of both ears and the intensity disparity in time in order to induce spatial structure. Blind people use the frequency asymmetry of patterns in order to induce the different sound sources.
21 hours ago · Like

Micah Blumberg The brain's efficiency making pattern will overtime take any computationally intense process and reduce it to the most energy efficient process. How can a computer reduce it's own computationally intense process into the simplest possible procedure that is computationally minimal?
21 hours ago · Like

Micah Blumberg How does intelligence rewrite it's code path to solve for the most efficient logic?
21 hours ago · Like

Juan Carlos Kuri Pinto Most frequent pathways for specific stimuli are reinforced through Hebbian learning. That's how the brain gets so efficient. ;)
21 hours ago · Unlike · 1

Juan Carlos Kuri Pinto That is incredibly difficult to program. But it is very easy to talk.
21 hours ago · Unlike · 1

Micah Blumberg Hebbian learning is the programming challenge then, has anyone done it?
21 hours ago · Like

Juan Carlos Kuri Pinto I'm programming exactly that. And I've been doing it for many years. Hehe
21 hours ago · Unlike · 1

Micah Blumberg cells that fire together wire together, this is the message to describe in category theory-haskell, wait you've been doing that for years? with what programming languages?
21 hours ago · Like

Juan Carlos Kuri Pinto Java. But I'm migrating to Lisp.
21 hours ago · Like

Juan Carlos Kuri Pinto I'm programming an easier demo which is not computationally expensive. But when I will confront the general problem of vision, maybe I'll do it in pure C. Or maybe Haskell. I don't know.
21 hours ago · Like

Micah Blumberg http://en.wikibooks.org/wiki/Artificial_Neural_Networks/Hebbian_Learning

Artificial Neural Networks/Hebbian Learning - Wikibooks, open books for an open world
en.wikibooks.org
Hebbian learning is one of the oldest learning algorithms, and is based in large...See More
21 hours ago · Like · 1 · Remove Preview

Micah Blumberg You still think the logic of vision is more computationally expensive, is that because you have equations for every aspect of vision? every point of data? every line? every curve? do you have a way of reducing the work load by working on smaller bits in a many level hierarchy, to create a very sparse representation?
21 hours ago · Like · 1

Micah Blumberg imagine that you could do spatial representation without color, you could do echolocation style seeing.
21 hours ago · Like · 1

Micah Blumberg amazingly less of a workload, very very sparse, and yet still useful
21 hours ago · Like · 1

Micah Blumberg We look around and we see this amazingly detailed picture right? but I don't think that picture is really there, I mean I think vision is sort of a trick of memory. The detail we think is there isn't rendered all at once, but instead in waves, in pieces, only giving the illusion that it's rendered all at once
21 hours ago · Like · 1

Juan Carlos Kuri Pinto My ultimate goal, perfect vision, requires to induce all aspects of vision. Except color qualia. I'll use just numbers.
21 hours ago · Unlike · 1

Micah Blumberg imagine that only one thin line of your vision is rendered in each 1/1000 thousands of a second, way less computational than rendering the whole image at once
21 hours ago · Like

Juan Carlos Kuri Pinto "every line? every curve?"

Those are features to be learned by using unsupervised methods like the ones explained in this course:

https://class.coursera.org/neuralnets-2012-001/class/index

Neural Networks for Machine Learning
class.coursera.org
21 hours ago · Unlike · 1

Juan Carlos Kuri Pinto "We look around and we see this amazingly detailed picture right? but I don't think that picture is really there, I mean I think vision is sort of a trick of memory. The detail we ...See More
21 hours ago · Unlike · 1

Micah Blumberg You say selection, I say reaction
21 hours ago · Like

Micah Blumberg every cell is reactive, cells do not select, they do not choose, and they do not act, they react
21 hours ago · Like

Juan Carlos Kuri Pinto Neural networks can actually select.
21 hours ago · Like

Micah Blumberg maybe artificial ones, imagine you have three branching choices, a hotdog, or a protein shake, or eat later when you can get a third option. which one do you choose? why do you cho...See More
21 hours ago · Like · 1

Juan Carlos Kuri Pinto Selection algorithms are learned through the interaction with Reality. That creates the illusion we have free-will. But we are totally-deterministic machines, chaotic ones indeed. Our behavior is totally caused by our genes and memes.
21 hours ago · Edited · Unlike · 1

Micah Blumberg I seem to have selected as an effect to which the cause was reason, so in retrospect (my choice A) -> (reaction to reason B)
21 hours ago · Like

Micah Blumberg that's why you say selection, and I say reaction
21 hours ago · Like

Micah Blumberg does it make a difference, to you, in how you think about programming Hebbian learning, to program thinking about the whole program as a sequence of reactions instead the inclusion of points where selection is done?
21 hours ago · Like

Juan Carlos Kuri Pinto http://www.amazon.com/Neural-Darwinism-Theory-Neuronal-Selection/dp/0465049346/ref=la_B000APRWGK_1_7?ie=UTF8&qid=1349377120&sr=1-7

Neural Darwinism: The Theory Of Neuronal Group Selection
www.amazon.com
Already the subject of considerable pre-publication discussion, this magisteria...See More
21 hours ago · Like

Juan Carlos Kuri Pinto http://www.amazon.com/Mindful-Brain-Cortical-Organization-Group-Selective/dp/0262550075/ref=la_B000APRWGK_1_9?ie=UTF8&qid=1349377120&sr=1-9

The Mindful Brain: Cortical Organization and the Group-Selective Theory of Higher Brain Function
www.amazon.com
This significant contribution to neuroscience consists of two papers, the first...See More
21 hours ago · Like

Anand Muglikar "You still think the logic of vision is more computationally expensive, is that because you have equations for every aspect of vision? every point of data? every line? every curve?...See More
Bay Area Vision Meeting: Unsupervised Feature Learning and Deep Learning
www.youtube.com
Bay Area Vision Meeting (more info below) Unsupervised Feature Learning and Deep...See More
12 hours ago · Unlike · 2

Anand Muglikar Btw, this hs been an interesting post to follow! Thanks to both of u! :)
12 hours ago · Unlike · 2

Juan Carlos Kuri Pinto ;)
12 hours ago · Like · 1

Anand Muglikar Hope u both have watched this sparse matrix representation & processing of images!
4 hours ago · Unlike · 1

Micah Blumberg Wow! Anand! That was awesome! Why don't you distort each image, prior to making it sparse, by 10hz, or 10 degrees, to give it two eyes or two ears? Then you have a hierarchy that is all about matching two eye or two ear hierarchies. The hierarchy to unite the two sense hierarchies is the time hierarchy. It's job is to build a representation when there are close synchronicity between the two sense hierarchies. So that their is a timeline association of sensory data, when new data comes in. The timeline association data becomes like a google search

for example, if you give the program a sentence like: Cats are fun! it does a google search to find similar sentences.

funny cats,
funny cats in water, epic,
cats funny, 
probably the funniest cat video you'll ever see, 
fun cats, 
funny cat pictures

sending that data to v1, it might create a visual concept of a funny cat, that is now associated with the words "cats are fun!" and the 

each eye in v1 + timeline representation is creating a sort of temporal and spatial representation of all the bits of data coming in at each level.

so we have spatial/temporal representations of the lower bits (not knowing what's in those lower bits, what's in them is the job of the v1 alone for example)

and spatial/temporal representations of each higher level of the hierarchy (again without knowing the data in that hierarchy, it's filled in by the hierarchy, the spatial/temporal association is built by the frequency of activity in each sector.

or you could store the timeline locally, on the actual hierarchy, like a dendrite. each node in the hierarchy has nodes that count the frequency in which they become part of a representation identified by the upper level or lower level.

The program can share what it's representations mean by doing google searches, to find things that are similar to what it has learned, and in so doing, it can build representations of it's representations and associate words, and pictures with it's internal representations, a sort of out put channel, communication
3 hours ago · Edited · Like · 1

Micah Blumberg Anand, is an SDR a set or a matrix? or is an SDR a set that represents a matrix? Can an matrix represent a combination of SDR? Or does a matrix represent a combination of SDR?
52 minutes ago · Like

Micah Blumberg If you have a matrix for a letter, in the next level being a word/noun/verb, in the next level a phrase, in the next level a sentence that has encoded meaning. Do you create an SDR or a Matrix that represents the whole sentence in memory? So that the meaning of one sentence can be associated with the meaning of other sentences is should be an SDR right? SDR is more simple to compare than a Matrix I presume? What happens when you start comparing an SDR from text with SDR from pictures with SDR from sound?

Does the heirarchy gain the ability to associate words with pictures with sounds, and can you decode the result by having it compare it's own results with with more pictures, words, and sounds?

What about a feedback loop by feeding the top of the heirarchy back into the bottom with new pictures, words, and sounds?
46 minutes ago · Like

Micah Blumberg The top result then gets associated with newly added SDR's. Driving a new top result that is a convergence between what was previously learned, and what is newly added, and at the same time, we can see how the stream of what was previously added is conceptually associated with the stream of new information.
43 minutes ago · Like

Micah Blumberg I mean numerically associated. The SDR of a picture, and the SDR of a word have a commonality in SDR and combine (how to combine two SDR into a new SDR?) to becomes a new SDR, and then new pictures and new words come in, and we want to see what gets associated with the SDR that represents a common pattern between the word and the picture in the original SDR. Whatever gets associated is the decoded meaning of the combined SDR
38 minutes ago · Like

Micah Blumberg If we can relate new picture associations with pieces of syntax, one could create visual sentences, movies, from the artificial mind.
37 minutes ago · Like

