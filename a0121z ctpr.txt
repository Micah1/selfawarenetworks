a0121z ctpr

Tim Mullen & Multimodal Sensor Collection, its interesting because how Neuropype works is useful for abstractly imagining at least one way that the human brain could combine different types of unstructured data sets. (with temporary synchronicity among oscillating brainwaves)

Tim Mullen built the famous Glass Brain EEG Visualization Demo, but his first product was an open source software Neuropype that combines different types of sensor data into one program with synchronized time codes so you can find a spike in ekg and match it with a spike in EEG and match it with a eye tracking and pupil dilation sensors.

Neuropype Tim Mullen
Sloretta low resolution
Method for source localization

Loretta

Nebul from Stanford University showed us a tool he is using on a project he can’t talk about yet. He is using a tool called Neuropype 

Created by Tim Mullen at Intheon and UCSD

Tim Mullen flew to San Francisco to speak at one of my events, I’ve had a few powerful 1 to 1 conversations with him that were mind expanding for me each time, he is a brilliant person who has done brilliant work. I think he could vouch for my professionalism

Neuropype Tim Mullen

This note could belong in the section that is about brain computer interface bci and also about multi-modal sensory gathering for applications. There was a company that was previously mentioned in this note whose name I am removing (A*******) because I now have suspicious beliefs about this company based on emails exchanged with them. I don't want to promote a company that offered me shares to promote them but never issued to me what they promised. It's like a company that says "your with us" but doesn't give you anything real in exchange.

tags (perception, field, graph, emotion)
In 2012 I wrote about a dream device

“the eeg spikes are associated with eye movement, and AI vision, pin pointing spikes between all three, with an extra operator behind a desk. So you have three points being tracked by a fourth operator. Resulting in a superior tracking system, that isn't a permanent alteration to sensory perception, it's just something worn like a helmet. Augmentations to hearing and sight and balance come in the form of ear implants, chest strips, tongue strips, these things can be worn externally, and a Texas company (associated with David Eagleman) is working on sonar sense via a chest strip. In addition to that a chest strip that has wall street information on it, so the brain can be applied to raw wall street data.”

The point is this: What if we could combine heart rate sensors, eeg sensors, eye tracking and pupil dilation sensors with artificial intelligence so that we can translate our emotions and intentions into our storytelling in new ways, improving the user interface such that there is less technology getting in the way of the artist and their artistic intention.


They want to build white label AR VR headset, for brands, that combines sensors such as from (I*****************), with artificial intelligence and a cloud based information system by (Q*****), and that’s just the beginning.

I want to share with you the actual text of an article written by by Jim Marggraff the CEO of “Eyefluence” a company now owned by Google.
http://innovationexcellence.com/blog/2016/10/07/the-wearable-foreseeable-future-of-the-workplace/

“Workplace Implications of Eye-interaction Technology in AR and VR headsets:
As augmented reality and virtual reality technology advance, AR and VR headsets are, or will soon be, entering the workplace.
Enterprises are scrambling to establish a competitive edge through the use of these glasses, goggles, and phone carriers, and pilots are rolling out globally in factories, construction sites, hospitals, and offices.
There are two major takeaways from these early pilots. First, employees critically value the comfort and wearability of headsets. If they’re too heavy, too hot, or an impediment to the user’s normal behavior, users simply will not use them. Second, user interaction with the headsets through head motion, hand gestures, and voice is inadequate.
So how about the eyes? Up until now, the only methods of interacting with information or controlling objects with your eyes was based on staring and waiting (dwelling) or winking or blinking to emulate a “mouse click.”
But a new interaction model has been developed — based on the biomechanics of the eye and the eye-brain connections — that enables virtually instantaneous interaction simply by looking.
The implications of eye-interaction technology in AR and VR headsets for employees are extraordinary.
There are four areas in which eye-interaction technology will profoundly impact workers:
(1) Productivity
Eye interaction transforms a user’s intent into action through their eyes without requiring head motions, hand motions, or speaking. This means that the 40 million desk-less, hands-free workers in the U.S. — and hundreds of millions more around the world — will access remote information, survey constructions sites, or complete quality assurance checklists as fast as they can think and look. And considering that the brain can process images in about 13 milliseconds, productivity and efficiency will naturally speed up. So a surgeon can assess the condition of the heart simply by looking at the heart and invoking a menu of options with her eyes.
(2) Intelligence
Eye interaction in a virtual environment of 20 simultaneous screens around a user’s head, navigable simply by thinking and looking, provides more information to a user than ever thought imaginable.
A researcher can cut and paste key text, graphs, and images into an eye-controlled clipboard, overcoming short-term memory limits, to improve human working memory — which is a proxy for intelligence — and boost a user’s IQ in solving problems. With the eye’s ability to move between locations in tens of milliseconds, the speed of collection is much faster than moving a mouse.
(3) Security
The pattern of your iris is the best biometric identifier available if it can conveniently be scanned without requiring cumbersome hardware or inconvenient gymnastics. Eye interaction is built upon eye-tracking technology, which directs a camera toward your eye at all times, that can be used to continuously validate your identity. This continuous biometric identification (CBID) may then be invoked at whatever frequency is appropriate to assure the proper level of security for a given transaction.
If you’re reading a confidential document, your identity may be validated once per minute. If you’re reviewing a top-secret government dossier, your identity may be validated once per second — or more frequently. The eye-interaction software might look at the movements of your eye and even invoke a response to ensure that it’s you and that you’re alive — and that a glass ersatz eye isn’t being used.
(4) Creativity
The combination of improved productivity and intelligence can already contribute to stimulating out-of-the-box thinking. Further, research suggests that prescribed eye movements stimulate the flow of information across the corpus callosum in your brain (the nerve bundle that connects your right and left hemispheres), which has been quantitatively shown to increase human creativity.
The difference between immersive eye interaction and basic point-and-click interfaces can’t be overstated. Imagine never needing a mouse to navigate again, as the monitor responds to the movements of your eyes. Action and your desired intent occur based on your thought and a combination of purposeful and non-purposeful eye signals.
While the underlying mechanics of eye-interaction technology are complex, its implementation is both immediately intuitive and immensely beneficial. Imagine removing the middlemen — hands and fingers — from the equation and giving your eyes the power to do what your brain wants instantly. How much more could we accomplish? What could we do that we’ve never been capable of doing before?
Employees might be wary of this evolution, especially considering that 37 percent of employees don’t trust their employers to use data from wearables — like VR or AR headsets — without prejudice. And new technology can be frightening, especially when it’s as disruptive as eye-interaction tech promises to be. However, with appropriate security and privacy, user adoption will be enthusiastic. And due to the pleasurable sensuality of the eye-interaction experience, uptake will be rapid.
“
After you have ready that article I want to point out how:

We can expand this vision further to include brainwaves and heart rate so we are ever so fastly minimizing the distance between technology and our lives. We have all the capabilities of eyefluence here and capabilities beyond it including the ability to predict a person’s emotion experience that allows a computer to make itself a better tool. 



state of the art micro-sensor technology allowing for non-pressure contact wearable to capture neuro-metrics
exclusive access to proprietary ultra sensitive sensor with the ability to collect the widest range of EEG, ECG signals
cloud based interpretation and correlation platform 
3rd party access control to allow developers/service providers to share proprietary collected data
Presentation and packaging for developers
Over time ability to build big data from neuro/bio metrics and other shared data from 3rd parties
Global developer software dev kit
Multiple large market capabilities with minimal alterations 
Leadership team with experience in: device/wearables; cloud based architecture; global developer deployments; entertainment industry; and IP/technology commercialization

The technology sounds a little bit unbelievable so if it is true what they are doing, and I think it is, then what they are doing is a revolution. 

The point I am making is that this is a fundamental shift in medical grade technology that has a core value to a lot of companies, and it will make for a series of great consumer products as well.



VERY POSITIVE THOUGHTS!

So I went to San Diego to see for myself.

So I went to San Diego, and the sensor is real but the way I was describing it before is wrong. It does have dramatically improved sensitivity and some amazing new capabilities but its strongest selling points are that each electrode with a battery is going to fit under a bandaid, cost 12 dollars, be disposable, and it's a wireless dry sensor intended to replace EEG and ECG sensors currently in use.
It's capable of detecting a human from across a room, but not specific neurons from across a room.
I'm back in San Francisco. The new technology I traveled to verify this past week is real, it's incredibly cheap, practical, disposable, its wireless with cutting edge capabilities. It will replace most standard EEG, ECG, and half a dozen other types of sensor equipment in the short and long term, without question, which was something I didn't expect. This entire time I have only really been interested in the exotic possibilities, meaning I was thinking about using this to accurately map brain activity in real time in a computer. Now that I understand how it actually works I'm simultaneously encouraged and daunted by the task ahead of me in that regard, but I'm also heartened by the accessibility of this new hardware for developers making arguably more simple AR and VR applications.

With eye tracking, eeg, ecg, and deep learning AI alone we learn the emotions and intentions of users and it can and will be easily integrated into VR headsets.

The result is a new user interface that responds to your eye movement, heart flutter, actual emotion, so that interacting with virtual objects feels natural and intuitive, the computer gets you, so it understands what you mean.



With the tools that exist now, given what I know about deep learning

start with eye tracking and deep learning and see what I can build from there, but there is something special and something real about the sensors they have from (I*****************)

I'm super excited about this new company in San Diego, they claim that they can detect a 1 billion times better image resolution than current EEG technology, and alternatively the sensor works from up to 33 feet away. So they can track whether or not someone is in the room and locate that person. That company is  (I*****************) 
 
TechCrunch reports “In the field of affective computing, sensors and other devices are also getting very good at observing and interpreting facial features, body posture, gestures, speech and physical states, another key ingredient in emotional intelligence. Innovative companies across a range of industries are now using computing systems that can augment, and even improve on, human emotional intelligence.” 
READ THIS ARTICLE https://techcrunch.com/2016/12/02/emotionally-intelligent-computers-may-already-have-a-higher-eq-than-you/

The headset will have EEG sensors, eye tracking sensors, and pupil dilation sensors, which will send data into the cloud so that we are able to 

I spent 58 minutes talking to the CEO of (Q****)  today! Amazing conversation that I think you would be very interested in.

Type error its actually  (Q****) 



The (I*****************) device is supposed to have 1 billion times improvement in the resolution for EEG wave frequency signal detections.

These sensors will cost 12 dollars each.

I froze at the moment they told me that. They are talking about a disposable EEG sensor that can track a single electron passing across a cell membrane, and it’s disposable, as in you can just throw it away when your done because it’s so cheap.

I spoke to Viserath In, PHD at length about the capabilities of his sensor at a high level. When we got to questions he didn’t know the answer to he didn’t bullshit me around he just told me that he didn’t know. Viserath is a genius and his company (I*****************) is a goldmine.

(Q****)  is really at the heart of this, and the passion of (Q****) 

To begin with (Q****)  is integrating different streams of sensory data like EEG and EKG and Eye tracking for basic research. Beyond basic research then is also the question of making data usable to programmers, it would be like creating an API that is based on tracking neural signals that are common across all human brains. In other words simple tools that would enable humans to operate programs more intuitively.

The kinds of interfaces they are designing will simply be a joy to use because they will predict our intentions with our eye movements and our feelings with our eeg signals and combine them together to give us more of what we want and less of what we don’t effortlessly.

They accomplish this by applying the same kind of technology that Google is applying everywhere in the news, it’s called Deep Learning.

What  (I*****************) has achieved in hardware combined with what (Q****)  achieved in software, you get a device that utilizes and combines sensory data to create an optimal user interface that is unlike any other.

One that we can sell to brands all over.

This is going to be bigger than Magic Leap.

Just think about how iOS quickly took over, with it’s really slicks screens that would bounce a little if you scrolled too hard. It’s all normal on our phones now, but at the time it was revolutionary, it was a new interface to the computer, 

It has the Genius of (Q****)  leading it in the sense of how Amazon Web Services will lead, protect, and help companies using it’s services.


