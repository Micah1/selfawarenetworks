Not really relevant to the Self Aware Networks topic or book
b0018y 12,939 words 64,605 characters

Claude AI Summary:

This is a casual conversation between two professionals in the volumetric capture/3D scanning industry. One is working on brain-computer interfaces and writing a book about neuroscience and neural networks, while the other works at Volograms, a company specializing in volumetric video capture. They discuss various technical aspects of 3D capture technologies, the evolution of depth sensors, and their views on the metaverse while having beers at a venue in San Francisco.

Most interesting arguments/points:

Depth Sensor Technology Evolution:

Detailed explanation of different depth sensing approaches (structured light vs time of flight)

Why multiple Kinect sensors interfere with each other

How Intel RealSense used a different approach (stereo analysis) but had lower resolution

The limitations of current LiDAR technology in capturing moving subjects

Volumetric Capture Innovation:

Volograms' development of phone-based volumetric capture using neural networks

The concept of a "crapifying GAN" that deliberately degrades perfect depth maps to match real sensor data

The importance of data collection and curation in training neural networks

The trade-offs between mesh resolution and performance on different devices

Metaverse Perspective:

Criticism of the Ready Player One vision of the metaverse

Preference for augmented reality over fully virtual worlds

Vision of enhancing real-world social interactions rather than replacing them

Discussion of Microsoft's acquisition of Activision Blizzard in the context of metaverse strategy


Business Insights:

The high barriers to entry in volumetric capture technology

The value of proprietary datasets in defending against competitors

The importance of data curation over raw algorithm development

Why big tech companies might prefer acquisitions over building their own solutions


Technical Challenges:

The complexity of processing volumetric video for different platforms
The challenge of maintaining consistent quality across different capture methods

The importance of proper data filtering and selection for training

The balance between quality and accessibility in consumer applications

The conversation provides a unique insider's perspective on the current state and future direction of volumetric capture technology, while touching on broader industry trends and technical challenges.

In this conversation, there is only a brief mention of "Self-Aware Networks Institute" when one of the participants discusses their future plans. 

Specifically, they mention:

"It's going to be part of the Self-Aware Networks Institute... And we're going to have a research institute on the web, virtual and augmented reality software, for people to do biological research and artificial intelligence research, and brain computer interface research."

The participant compares it to the Redwood Neuroscience Institute (started by Jeff Hawkins) and describes it as evolving from their "hack night" concept. They mention it would focus on:

New brain computer interfaces
New kinds of neural networks
VR and AR applications
The study of biology

However, they don't go into detail about what makes networks "self-aware" or elaborate on the institute's specific approach to achieving this goal. The conversation quickly moves on to other topics about volumetric capture and 3D scanning technology.

The mention of Self-Aware Networks appears to be more of a future project/initiative rather than a central topic of the conversation. Most of the discussion focuses on technical aspects of 3D capture, depth sensing, and volumetric video technology.

Edited Transcription:
I've been here in San Francisco for 10 years.

So I've met a lot of people in that time.

So it's glad to connect with you.

Definitely.

So you're a journalist or something like that?

Well, okay, so the real story is that I write software for brain-computer interfaces,

and I'm currently writing a book about brain-computer interfaces, but it's also about the human brain,

and it's about artificial neural networks and all of the interesting things you can learn

from studying both neuroscience and artificial neural networks and brain-computer interfaces.

So it's about all three topics.

And intermix is there is some physics that I'm going to be talking about.

So the book is like a work of journalism from someone who's read a lot.

Okay.

So when I was reading more or less, you were thinking, for some reason,

I thought that you owned some medium or something like that.

Yeah.

I wasn't sure.

So apologies, because it's an understatement, like saying you're a journalist,

and you're writing software for a human computer, like brain-computer interfaces.

Yeah, yeah.

Really sophisticated software.

So interesting.

Yeah, if you look on my Twitter, you can see video examples of software that I've written,

VR and AR applications with WebXR.

So really sophisticated stuff.

One of the things I did was, so I hosted a monthly meetup at a hackerspace here in San Francisco

called Noisebridge.

Sorry, it was a weekly meetup.

And every week I'd get...

Weekly meetup?

Weekly meetup.



It was like every Friday for most of the year.

And then towards the end of the year, I switched to Thursday.

But what I did was, I invited people, I said, okay, I want to see if I can bring the EEG,

electroencephalogram, into virtual reality, into WebVR, so we can experience a representation,

a 3D representation of our brain activity.

Amazing.

And I'm curious...

Do you want to map the, the, the, the, the activity that's coming in the brain represent

in VR, so you can analyze it directly on a 3D environment?

Yeah.





So, okay, so the, the out, the way the program came out was we ended up, uh, just creating

like a 3D time series of, uh, ten different, ten different, uh, ten different rows of signals.

So the, the EEG headset had two sensors, and then you could divide them into five, five

different frequency bands each.

So we had the, the delta, the theta, the beta, the gamma, and the alpha for the left and

right sensors.

And so that's ten channels, and each one is going up or down, represented by, uh, a plane,

like just, uh, just a, you know, like a square in space, right?

Mm-hmm.

And, uh, it goes up and down.

As it goes up, it changes color to, like, the, the red spectrum and then down to, like,

blue.

And, uh, so what happens is the sensors, the sensors are measuring the voltage on your forehead,

and that's causing, uh, the, the voltage to go up or down.

And this, we're, we're turning it into a number.

We're streaming it into the webpage via WebSocket.

And we're telling, uh, I'm telling a frame, I'm telling, uh, which is the, the, um, which

is the library, uh, that I want the, um, uh, the numbers to correlate with the height and

the color of the plane.

And then, uh, after that happens with one, with one interval, then we're gonna pass that

value on to the next set.

And so we have, like, basically, you see this, like, wave of, like, blocks going up and down

in a time series in 3D.

And then you get up in the VR headset and walk amongst the representation of your brain activity.

Oh, your own brain activity.

Yeah.

And you can see, well, that's my beta waves right there.

They're spiking because I'm paying, paying attention to this.

And you're, so the purpose for you is to write about it, or, or are you wanting to commercialize

this in some way, or is it not about...

So I feel like that was, uh, sort of a, a proof of con-, a proof of concept, um, of just

one thing that we tried.

I didn't expect to get a lot more than a proof of concept out of it because the nature of

EEG being very noisy in VR, uh, just adding more noise to your...

It's just not a good combination.

Uh, on paper.

It w-, it looks cool, but the, but the functional, like, I don't know what people would use it

for.

It's just really too noisy.

Uh, other than, you know, it's cool.

So the next iteration that I'm working on is gonna use functional mirror for spectroscopy.

I, I have a sensor back at my office, uh, here in San Francisco, that, um, is going to,

we're basically gonna do the same thing, but this time I don't have to worry about the

person moving around in the VR headset because, uh, the, the signals are, it's, they're not

gonna be, uh, fluctuating because of noise.

Because, yeah, because it's all, it's about the, the light is gonna go into the skull and,

um, so it doesn't matter and, and come out again to the sensor and you're measuring the

differences in the light spectrum in order to determine the, the, the, the number value

to create your time series.

And so in that case, you can move as much as you want.

You're not going to, uh, disturb your signal quality.

It reminds me, I assume you've heard of this company, OpenBCI.

Mm-hmm.

Yeah.

I interviewed them.

Oh, cool.

Yeah.

I met them a couple of months ago in person.

Um, because they, they have, um, one of the engineers studied in the same university

as I did.

It was a kind of a random coincidence, you know, um, it was in the, um, method world export,

like, uh, back in November.

So I met them in person.

I was just amazed because I didn't know there was, like, a direct relation to the VR world.

I mean, they were, I think they were, they had some kind of partnership with HPC or something

like that, where they were trying to build some of the sensors and do some prototype headsets

and to understand a little bit the user, what were the emotions and, well, the signals.

I know nothing about the brain signals, of course.

But my background is actually telecommunications engineer, so I would be, like, electrical engineer.

So, I do know about signal processing.

Mm-hmm.

I haven't done signal processing, like, proper signal processing in some time.

But, uh, nothing about the brain.

No idea.

Yeah.

Um, yeah, I've been studying the brain for about 14 years.

And, uh, so, I'm writing a book as a journalist, but I have new things to say about the brain

that have never been said before, so.

Cool.

Yeah.

So, are you going to be in polemic?

Are you going to go against the current or something?

Mm-hmm.

Yeah.

Yeah.

I'm going to make, I have, I have really interesting ideas that other people have to be, have to

decide if my ideas are valid.

They'll have to make up their own minds, of course.

But, I have ideas that I don't think anyone has really talked about before.

Novel new ideas.

Yeah.

Novel new ideas.

And I, and I say that, like, a little tentatively because there could be something out there that

someone has written that I don't know about.

Yeah, of course.

Right?

We don't know everything about it.

Mm-hmm.

Very interesting.




And just so I can like understand it firsthand, like maybe someone will make a VR representation of it,

but volumetric VR of the world and then I can...



Um.

Yeah.

But, um.

It's like, if we have, if we have.

So, I mean, I've done a lot of interface design.

And there's more videos.

But, like, some of the stuff is like I, I led another project where it was an online hack,

hack night that we met every week.

And I invited programmers and I taught them how to use A-Frame, which is what I was using

with the NeuroHackster project, bringing easy into VR.

And what we, what I had the folks do was, I said, okay, we're going to make the gravity

gloves from Half-Life Alyx.

Have you played that game?

No, yes.

I was a big fan of the original Half-Life when I was a teenager.

But, uh, the thing is that we do have an Oculus, but it's in the office.

So, we're in the, uh, through the whole pandemic we haven't been through the office, so I've

never gotten to the tennis team.

Oh.

That's it.

Yeah.

I've seen all the videos and, uh, we don't know how to do it.

So, okay, so you're very familiar with it.

Yeah, I know, I know Half-Life.

Yeah.

You know the, uh, the, uh, the, uh, the gravity gun?

Yeah.

Yeah, that was cool.

Um, and the, so the gloves are a similar concept, of course, but you just hold out your hand

for any object that you want, and sort of, but it specifically, it, like, flies up in the

air at an arc, and then you grab it.

Which is weird, because, like, in Half-Life, in the beginning of the game, they practice

throwing out a gun, and then they throw out bullets, and you're supposed to catch them

out of the air.

And if you don't, they replay the sequence.

You can try again.

It's cool.

Um, but I, but I was like, without it even trying the game, we reverse engineered it,

and we made it, like, so that it actually flies up into an arc.

It doesn't fly straight to your hand, which is this whole separate piece of code.

Um, that we had to, like, we had to, like, think about, okay, well, how is the arc, how

are we going to make an arc, you know, like, cause you have, you gotta do some math, and

we did it.

Uh, and you can see that in my videos, and there's also, I linked the code, so you can

get it on my GitHub.

It's cool.

Uh, and, um, but yeah, so, like, I've got this 3D carousel that changes scale, rotation,

and position.

So we can, it works in augmented reality, so, so that means that any kind of interface,

you put it in there, and we can just, like, you know, go through the carousel, and then

if you want something, you click it, and it can fly into your hand, or you can send it back

into your carousel, or you can pull stuff from one carousel and put it in another, and

you can put something else in this place.

And this is, like, to figure out how to do this is pretty complex, but, and that's, so

that is my website, but it's not done yet.

It's going to be part of the self-aware, sorry, I don't want to give away the name yet, but

it's going to be part of the Institute.

Okay, it's the Self-Aware Networks Institute.

Um, Sentient AI.

And we're going to have a research institute on the web, virtual and augmented reality software,

for people to do, uh, biological research and artificial intelligence research, and brain

computer interface research.

Based on, like, sort of like the hack night concept that I've, that I've been doing for

the past couple years, is becoming, like, the Institute.

So it's like an online hack night, but dedicated to very specific goals, which are new brain

computer interfaces, new kinds of neural networks, and, um, new VR and AR applications,

and the study of biology.

So...

That sounds ambitious.

Yeah.

And you're doing that on your own, and...

I'm, I'm, I'm, I'm technically on my own.

Uh, I do have support from people, but on paper, I'm, on paper, I'm on my own, yeah.

So nobody, nobody else is, yeah, nobody else is collecting, but I have definitely no support.

The idea is to build all these interfaces for, for profit, for non-profit, to, um...

Um...

Is it, uh, because it's, uh, supposed to teach people to...

Do you think they might use them?

No, so the Institute, yeah, the Institute is like a, um...

I think of it as like the, um, the Redwood Neuroscience Institute, started by Jeff Hawkins.







So, what are you personally interested in volumetric capture?

Do you have any experience with it?

Yeah.

Personally.

So, I own a Microsoft Azure DK.

Oh, cool.

Which one?

The new one?

The new one.

The new one.

Amazing.

The one that came out like three years ago, yeah.

The Azure DK 2?

Or, I forget the exact name, but...

Yeah, I mean, if it's a connected Azure, that's the one.

Yeah, that's...

I guess I'm...

Yeah.

I'm surprised, like, everyone wants it.

You can't buy it.

Simple to buy it, yeah.

But why don't they come out with another one?

It's a hobby for Microsoft.

It's not really part of the business.

I think their business is Azure.

That's why they have that name Azure.

But I think no one is actually using the Azure to refine the nodes or refine the point cloud.

Everybody's just with a low power.

And then it's kind of a shame that, like, Intel, they had the RealSense.

It was such a great series.

They had multiple different lines of the RealSense.

And then they just quit and, like...

Yeah, so the same with Intel.

And they had one of those, too.

Yeah, the RealSense.

I'm not a big fan of the RealSense.

I have a lot of experience with that.

I've never tried the Azure because I'm typical to guess.

But I did my PhD with Kinect 1 and Kinect 2.

I was part of the LibFreeNet, you know?

That was the kind of hacking community that...

What's that called? LibFreeNet?

LibFreeNet, yeah.

That was...

L-I-B?

Like, L-I-B, free, and net.

Like, instead of Kinect, free net.

I'm just gonna write that down to look it up later.

So, I mean, I don't know if it's still active.

I mean, I used to do that when I was in my PhD, but...

I would be checking my GitHub because I know that was while they have it.

...

...

...

Would that be the right thing?

Yeah, well, free net.

Free net!

Like Kinect, so free net.

Oh, that was a Kinect thing?

Yeah, that's it.

It was a community that was basically hacking the Kinect to make it work on Linux.

Yeah.

So that's a great part of that.

And helped.

So going back to new kinds of artificial neural networks

One of the things I'm really super interested in is not just volumetric video, but doing 3D semantic segmentation on point clouds

GAN synthesis

Shape completion

Interpolation of shapes when you have like a couch and an avocado and you can make something that's like half couch and half avocado

Like the gp3

Oh, yeah

Yeah, there's a

You can look up point net plus plus

Point net plus plus. Yeah, that's the one is the point net. Okay. Yeah

There's this guy his name is as or litany if you look up his name, he's he's a

Really like a top engineer on 3d semantic segmentation

Yeah

Oh

That was actually the one

Hacking the connect to make it work on yeah

Yeah, I actually wrote some of the drivers

The ones that were doing the point cloud actually mapping because they were not working really well

Yeah, I'm friends with with with Jasper Breckel who makes Breckel software

Ah, yeah, I know

So that's like the black kid competitor, right?

Yeah

Like reading different sensors and

Yeah, I mean to think of them as competitors like I don't think Jasper would think of himself as a competitor

I mean

Because he could be

He like he like he like to game kind of gives away the software a little bit for free like you can just download it

Yeah, it doesn't look like it's something that he would do for a living. It looks more like what a scientist would do

Yeah, and so he's not like he's not like in the business mindset of maximizing his profits or anything because you know you look at

A depth kit, you know, they've got this like monthly subscription or like

It's a business

They were

But not him. He's like oh just give me a you know like one-time thing and it's yours. You can use the software

It's nice

Yeah, I don't know. I've been very active in that community when I have a few years back

I was one of the first testers also of the depth kit when it was called RGBD kit

So that was the very first version and basically had to attach these other cameras on top of a Kinect because the RGB camera on the Kinect was so bad

So I understand and I know what you really don't know

So do you know about scan truck company?

It's like a truck with a little camera

Yeah, so you know them

Yeah, I talked to them a couple of years back because we wanted to help them move to volumetric capture

So instead of doing 3D scanning that they were doing but they were very happy with their 3D scanning and they didn't ever want to

They said that they had like a I don't know how many cameras and it would be like a big hassle for them

The funny story is I interviewed them with the Microsoft Desert DK so I captured them in volumetric video

I was just like that's their business

I never actually got the interview finished though, I never got the video finished

And I also captured Jules Erbach, the CEO of Otoy

Captured Jules Erbach, he's the CEO of Otoy

And so back in 2015 at SIGGraph, you know at Seagraph conference?

Yeah, on the board of youth

So he was one of the people who like introduced like this light field capture with a DSLR camera mounted on a tripod

And another DSLR camera that was only used to balance the first DSLR camera

I mean it's perfect, right?

Who else would have thought of that?

So he's really famous for like sort of like introducing light field video to the Seagraph and to the world

The only company that really was doing that before Erbach was Disney

I think Disney was first

But they did, so he's been involved with lots of different cool like experiments

That he sort of did on the side to what his bread and butter business is

I mean this is like a fun science project for him on the side, I think, in a way

Because he's always pushing like the limits of innovation

And I think I've interviewed him like four times actually

But unfortunately, so I captured his video with DepthKit

And I exported it as like an RGB depth

And then what it gave me was like a bunch of material files and object files

And I'm like I don't know what to do with this

Like what? Huh?

You mean the OBJs

The OBJs, yeah the OBJs and MTLs

Right?

And I don't know what to do with this stuff

I'm like

So the OBJs is basically telling you to do the whole other community

So what you normally do is you normally do?

If it's not over the other day, do you feel why or do you do that?

I didn't get that far

I was just like

I had to stop

I was like

I had too many other things to do

Okay

But that's where I got

I got to that point where I interviewed him

I captured him

I got this stuff

But I don't know what to do with it

Like I'm supposed to learn Unity from scratch now

Like so I can import this stuff

It's very easy in Unity

Is it?

I mean

You've done your PL and stuff

I have the feeling that

It's so much simpler than what you did

Yeah, yeah, yeah

I mean Unity is starting for all planets

That's what

Yeah, yeah

The thing is

The thing is

I use Unity without coding

And I know how to code

But just for the basic stuff

You don't need to

Actually, the question is

They provide some integration

community and everything

So you can just load the plugin

Load the sequence for the days

Yeah

And they will play out

But I recently

Just this week

I sent the files over to a friend

Who's in the volumetric video

And the RGB def stuff

And he doesn't know how to

Turn it back into a volumetric

Like it's just RGB

RGB stuff

The thing is that RGB is

It's basically like an image

Or like a video

That also has an extra layer

And you can look up to how far

And so on

But you cannot really put them

Into a point cloud

Or into a 3D model

Unless you have a calibration

You need to know information

About the sensor

And you can do the mapping

From the depth

To the real world

So that's

Normally I mean

You do that

That's

Every sensor has its own parameters

And normally you can do the

You can do it natively

Most of the sensors

Provide a library

If you do that

Or you can do the calibration yourself

I think I can pull up my

My hard drive

If you want to

Yeah, I know

I know

Let's see

Let's go to

Drive

And I'll show you the files

And

Let's look at

I think it's under

Seagraph 2019

Seagraph

Seagraph

Seagraph 2019

The last Seagraph

Yeah, right?

That was

I came here with the whole team

We were

We were exhibiting

In the exhibitor area

Let's see

I'll type

The whole team back then

Was only fact people

In 2019

I would think

Oh

Okay

So

So

This is the

That's the Jules Zurbach RGBD

Feel free to look at

Any of the files in that folder

Yeah

Any of the subfolders

Yeah

It's actually

This is exactly the parameter

I was telling you

So you can do the

Using the

This is the intrinsic

Yeah

So this tells you

What is the fundamental level

Of the sensor

Of the principal point

Which is to be

The middle of the image

Plus the resolution

And with that

You can convert

Whatever depth information

You have

Into a point block

Wow

So I can save that

Okay

Yeah you can do it

I mean you just have to do

A major multiplication

No

No problem

Yeah no problem

I

I can do math

Yeah yeah

It's pretty simple

So you don't have to

Your intrinsics

Because I'm assuming

Would you

Would you guys consider

Maybe helping me with this

And

I don't know

I can tell

I can give you what I think it is

Okay

Or maybe just give it a try

But

We're not doing depth

Like RGBT

Okay okay

Not really here

No

I used to do it

I used to play out

When I was doing my B&T

I mean when I saw the first keynote

Well wait here's what I was thinking

Sure

Here's what I was thinking

Okay

I was thinking that

We could do like an online webinar

Create some content

That shows other people how to do it

And promote your company at the same time

Well I mean

The thing is that

For that in particular

We wouldn't be

I don't know if

Probably the right person to contribute

I mean

I know there's people that work a lot better than me nowadays

I mean

Maybe a few years back

So

I'll tell you where my background comes from

So I was

I did my PhD on 3D reconstruction

So

Computer Vision

In general

Kind of the intersection between computer vision and telegraphics

So

The very first project that I was working on

Was a

Let's say

A

Forty-matchly great

Kind of

But it was static

So the purpose was to actually capture yourself in the T-post

And then I made it

And so on

So we did it with a big tech telco company in Spain

Called Telefonica

That's actually like

T-Mobile from here

Or something

Pretty good

And they have like

They had

A very strong

From the R&D team

Doing the H4 scene

And stuff like that

So

We did this system

To do very much

Capture

3D reconstruction

Of one single frame

With normal cameras

No dead sensors

Or anything like that

So I worked a lot on that

It was multi-view 3D reconstruction

It was kind of the

Origin

Of

Now

Like

Using some of that technology

But then

When I continued my PhD

I

Suddenly

I had some experience with

Time-of-flight sensors

And

Stereo sensors

But

Those type of sensors

Back then

They were like

Incredibly expensive

And they were only used for bubbles

And stuff like that

And stuff like that

So we had one in our lab

That cost like

You know

6,000 euros

Or something like that

So

7,000 dollars

Incredible

So

I was gonna say

Some of these

LiDAR sensors

I mean

I've seen them go for

Like a hundred and thirty thousand

This wasn't even LiDAR

It was

It was

It was

Like

Giving you like

That

This very small camera

That normally

You were on board

And robots

But that was before Kinect

So some Kinect comes out

As a

Something to play with

With the

With the

Xbox



In our lab

We got like

Ten of those

So I started working

With the Kinect one

But of course

I could only start working on it

When they hacked it

And they created

The Kinect

Which is what I understand

It was a library

To be able to use it

From Linux

Because everything that I was doing

Was in Linux

Will do

Nothing on Linux

Did you ever make

A

A depth camera

From a Raspberry Pi?

No

With a Raspberry Pi

You can!

With a camera

Or you had to use like

A stereo pair

Or something like that

You had to use two cameras

Oh cool

I know

I guess

I mean

You can

I built a lot of

Like a

Death Estimation algorithm

With a stereo pair

So I ended up building

This 3D reconstruction algorithm

That would allow you to move

While you were doing

Your own capture

So that wasn't part of my research

And when I was doing that

Suddenly they released

The second Kinect

So

I said let's try it

Because it will work

Everything will work

I've done it

Nothing works

Yeah

But suddenly

Everybody was the same

As me

We actually

Were part of the beta

So they sent us

A development kit

That had like

Massive

Power source

And everything

Like a big computer

So we had to start

Hacking it again

From scratch

But the good thing

Is that I now

Have the experience

For the present

And so I helped

Building some of these

Mapping libraries

And also

I also built the interface

With OpenCV

So the very first interface

For OpenCV for Kinect

Was built by me

So have you ever thought

That maybe

What a human being sees

Is actually a volumetric video?

Oh yeah

It totally is

It makes sense

It's just

So we

It's just

Because basically

What we're seeing

Is a stereo

And whenever

So our eyes

Are able to perceive that

Because of that too

So you get rid of one eye

And it's nothing like this

You'll start struggling

With some of your

But your brain

Is so good

That it's managed

How do you use

Depth estimation

From one single view

That's basically

A monocular depth estimation

Via a neural network

Yeah

So it's basically

What Google's building

And all these companies



Once I was doing that

And I think it might be

The

I wanted to continue

Stuff with volume

Manufacturing

But

But

I was not a big fan

Of using multiple depth centers

So I tried

During this time

Calling multiple connects



No, I know

I had a friend who did that

Both with the first connect

And both with the second connect

And this is actually also a problem

With the Azure connect

Although they have

Kind of fixing

Because now you can sync them

But the thing is that

They didn't fix the

They didn't fix the

The

The

Interference

What they did

Is that they are now

Synchronized

So there's

One flashes

And the other is turned up

And then it's the opposite

So they are basically going

Like this

With the

So the problem is

So I'll explain

I'll explain

So

The next one

The next one

Is basically

A pattern

Of dogs

Projected

In infrared light

And an infrared camera

So the infrared camera

Has

In

In the memory

The shape

That the pattern

Has to have

Okay

So it's

Imagine that it's

Right

So here

That it's

It's

It's

It's

Yeah

That's it

So when the camera

Sees the dial

Depending on

How big they are

We will tell you

Okay

This car

Is not far

Are you talking about

Using a marker

To anchor the camera

It does use markers

Okay

It's a pattern of dogs

But it's basically

Little squares

Okay

So that's how it works

So the program

It works great

It works on the device

It gives you real time

It's perfect

Okay

The problem is that

Now you have two kinetics

And you flash both

At the same thing

The pattern from the one

And the pattern from the other

Get mixed

So the kinetics are like

I don't know what's going on here

So if they don't give you depth

Instead of f**king you

They f**k you up

Basically

So you can't really put

Two kinetics at the same time

Because the pattern

That they are flushing

It gets interference

Oh because they're

They're doing time of flight

No they're not

No they're not

No they're not

It's only doing a

Projecting a pattern

It's called a structured line

Okay

So that's what

The first kinetics is doing

The second kinetics

Is a time of flight

So in that case

What they are doing

Is basically flashing

Different photons

At different

At different frequencies

Or

So they're alternating

Yeah

So they have an array

Of different photons

With different frequencies

Or wavelengths

Different wavelengths

Okay

They have different wavelengths

Yeah

And are they timed

So that they're like

What is the word

What is the word

So what it basically does

Is measure the time

That goes

Going from that

That's the time of flight

So what happens

If for instance

You have a

You have a

Black pants

The photon gets absorbed

By your pants

And never comes back

So black is never

Where really well represented

So black is an issue

For instance

For the panic fight center

So that's why sometimes

I was

With the connect two

One day it will appear

At the lab

With black jeans

And suddenly

I was losing my legs

When I was doing

The reconstruction

So that's one of the issues

But the other issues

You have two

Happens the same again

The photons that come from one

Might arrive at the other center

So the time

The time of the flight

Of the photon

Is not right

Because it gets interference

From one sensor to the other

So that's why

You cannot class

At the same thing

With two sensors

The real sense

Was doing something

Completely different

The real sense

Was not analyzing

The pattern

Or measuring

The time of flight

What he was doing

Was basically

Was doing something

Called multi-view stereo

Or stereo analysis

In real time

That's why

The problem that he was having

Is that the resolution

Was crappy

Because you're

To be able to do it

In real time

You're making a lot

Of assumptions

And then you're losing

Accuracy

So that's why

The real sense

Is a real sense

The real sense

Is a way worse

Especially not in

Part of the distance

However

If you put two

Real senses

One next to the other

They have what

Is called

Cooperative interference

So it gets better

So it gets better

Instead of getting worse

So it's good

To have multiple

Real senses

While it's terrible

To have multiple

Genes

But still

The resolution

Of the real sense

Is so bad

To capture

Full body body

To capture

Full body body

To capture

Real senses

More than this

They're gonna give you

Such huge errors

That your

Your points

Are gonna be moving

Up to five centimeters

From one frame

To the other

So if you're

Like two meters away

Or three meters away

From the center

And you analyze

The point that

You're getting frame by frame

Points are doing like this

It's terrible

So that's why

I've never wanted

I've never wanted

To use

Death sensor

For our company

Because I knew that

There's a lot of

There's a lot of

Bars around here

If you want to walk around

Sure

That's why

I've never wanted

To use that

And now

We're considering it

With a lidar

And a foam

Of course

That's really

That's why

I've never wanted

To use that

And now

We're considering it

With a lidar

And a foam

Of course

That's really

Mm-hmm

Do you feel more like

An Irish pub

Or a Jewish pub

Or

I don't even know

What's a Jewish pub

There's a

Yeah, there's a tiny

Little Jewish pub

I'm not sure

They're usually open

At this time

Whatever you say

Maybe

Maybe you would like

There's an Irish pub

Up the street

Up one block

Yeah, let's get time there

Alright

Good

So anyway

That's a

A real home story

About different systems

And so on

I think the reason

Why Intel

Is not

Sighted

Not to continue

With the real sense

Or anything

Is because they were

Basically losing

The battle of the chips

Oh, there's a place

Right here

There's a

Yeah

Oh, it's actually

Yeah

Sorry

There's another one

Right here

Yeah

Yeah, that's great

Yeah, I prefer to actually

I'd probably like

Even like stand after sitting for so long

Maybe like

Stand outside

Yeah

Yeah

Yeah

Maybe maybe stay here or not

Yeah, that's great

Uh, well

Will they let us

What do you think

Do you think

Do you think

Do you think

Do you think

Do you think

Do you think

Yeah

Yeah

There's actually

There's this one

And there's another one

Right here

Yeah, that's great

Yeah, I prefer to actually

I'd probably like

Even like stand after sitting

For so long

Maybe like

Stand outside

Yeah

Will you maybe stay here

Or not

Yeah, that's great

Uh, well

Will they let us drink drinks outside?

Yeah, there's this one right here

Where is

Where is

I don't know if they're open?

We're looking at

We're saying

We could probably

Maybe we can

Uh

Yeah, I don't know if they're open

Yeah, I think so there

Are they

Yeah

I mean

It's fine

I think they are open

Yeah, okay

Let's

Check it out, I guess

And then

And then

Maybe we'll let us take it outside

I guess it's just because of

Uh

I guess it's just because of

Uh

So what, Tuesday?

Oh, it's Tuesday

Yeah, so

That's why

It's Tuesday

Oh yeah

It's Tuesday

Yeah

What do you want?

Uh

What do you have?

What do you have?

I don't know

I don't know

Oh, is it?

That's what we call it

I work with them

Alright

So someone tell me

Yeah

When they bring something out

You don't mind

Maybe

I'm never trying to space stuff

Oh yeah

Space stuff

Yeah

Oh

Eight time two

Do you want to get two of those?

Yeah

Alright

I'll get two of these later

Sure

Can you guys have your, uh, back to the phone?

Sorry

What did he ask for?

To the vaccination

Oh, vaccination

Oh, vaccination, okay

Are you guys going to be sitting as far as home?

Um, maybe

Yeah

Well, I think it's definitely too bad

Or five minutes

Uh

If you don't mind maybe we'll

We'll have to go

Yeah

That's how it's already

Oh, okay

Oh, okay

Sorry, nevermind

Backstage

So yeah

They basically killed everything that is out there

So for instance

They were

We saw that they were installing this

Massive cell cameras in

Like NBA speedy

Stuff like that

The preview point video

The instant replays

And stuff like that

They killed that project

Uh

They killed the real senses

They killed everything

Everything that is out there

Everything that is out there

So

I think I'm going to take this one

And

Uh

Yeah

So

That's why you feel it

I don't think they're never going to get back to the real senses

Sounds good

I mean

It's a shame

Yeah

It's a shame

It's a shame, right

Yeah

It's a shame, right

I think now

We're going

Are you thinking about getting that VR camera from Canon

That's the

The

The

The

The

The

The

The

The

The

I mean they really haven't said anything

Have you seen the

The Barhow VR headset

That

Combines

Oreo

Yeah

I've seen it but I've never tried it

Have you?

Yes I have

It is

It is the very best VR headset I've ever tried

Really?

Yeah

It's just a little pricey

No

It's

For the money it's totally worth it

I

It's on my shopping list

This is nice because before like in California like

Do you want to go to the road or?

No this is

Well we

Not

We kind of

So the weird thing is in California like you

In theory you're not like supposed to bring your drinks outside

But since the pandemic happened

You know and they

Yeah

Do you want to stay or stand or sit?

Uh

I'm good with either one

Whatever

I kind of like the fire though

Do you want to sit?

I don't know but then we're in their conversation

Whenever they finish with whatever you do

Yeah

So then

So yeah for us for

Volograms

When we started the company what we wanted basically is to do something similar to what Microsoft was doing

The mixed reality capture studios but simpler because doing it with 106 cameras was a ridiculous

So I mean

I just I was looking at the volume

The volume

The volume

The volume

Right

And I'm thinking you're making a mesh

You have to make there's a trade-off between

Wow

This is strong

You know when you make when you decide on the resolution you're going to have your mesh PS

You have to think about something like what is something that you're

The performance of the computer is going to be able to handle

If you want to display this on the phone

Yeah

You don't want the phone to catch fire

So you've got to think in terms like lower polys

Yeah

You want to have a decent frame rate

Yeah

So we are

We're doing basically two different things

We're doing like

On the one hand

I actually have professional captures in the studio

And we're powering different types of studios

For instance

We work with the Samsung studio

So we work with them a couple of times

We have our own studio in Dublin

But we also work with a couple of other studios in the UK

So we're trying to be flexible technology

That works with different types of camera setups

Bigger setups

Smaller setups

But yeah

We do

For the professional client

What we end up doing is generating the assets into three different resolutions

So you can decide if you want to use it in a very high-end headset

That's plugged to a computer

If you want to do it in a phone

Or if you want to do it in the HoloLens

Or something like that

That it's going to require you to be very small

And yeah

We have a trade-off between resolution of the mesh

So the number of polygons

And the resolution of the texture

So if you want to have 2K texture atlas

Like 4K texture atlas

If you want to go something smaller

If your device is not able to go smaller

Yeah

And

And

And

And

Yeah

So that's for the professional side

And then we are

I don't know if you saw that we have this new app

That's actually able to do volumetric capture with one single phone

With a phone

I don't know if I

I don't know if I

I can show you

Okay

Okay

Demo time

Hey

So

It's called Volume

Is this an app that you've already distributed that I can

Yeah, it's in the app store

You can download it

So

The app is running

When you get here

It's basically showing you how you can use it

How you can create the app and so on

I'm just trying to get that right now

What's the name of it?

Volume

Volume

Okay, great, of course

If you search for volograms

You will find it

Because it's called volume by volograms

So it allows you to actually capture somebody

I'm probably spotting everyone

So for instance

That's me, right?

So yeah

There you go

Oh, I already have it

Oh, we have 4 stars

Not too bad

Yeah

Okay, so you maybe had it at some point

And maybe you didn't try

This is me, for instance

Capture

Well, I mean, I think I have a ton of volumetric

Oh, really?

Yeah, no

When you said you're in San Francisco

And you're in

I wanted to hang out because I'm interested in the topic

Yeah

There I am

So this is me

Whoa, that's good

But this is me capture just with the phone

Wow

So of course

I mean, if I look at the back

The back is lower resolution

Because of course

It's been recorded only from the front

What?

Yeah

So we basically

What we're doing

Is that shape completion?

Yeah

So what it does is basically

We have a neural network

That does volume estimation from one single view

So you can actually record somebody very easily

You trained it on a bunch of models

And all the models that we have ever captured

So just to capture it

It's as simple as using a normal camera

You just get there

That's excellent

No, that was

Yeah

Yeah

So it's funny

I mean, you can

You can do even like more complex movements

Of course

If you put it to test

You will be able to break it

I'm assuming

But

See?

I mean

Yeah

This is

See, it's

All 3D models

Okay, so my

So what I want to do with this

Is I want to bring it in

I want to export and bring it to WebExstar

So that as a journalist

I can talk to people as myself

Yeah

So right now with the app

You can only use it here

But we're now actually building the export feature

So you can actually do it whatever you want

And I'll show you because I'm

I have a few examples

I've looked

Because maybe

Yeah, we can go

Hey, what's up, Greg?

How's it going?

Good to see you, my friend

Take over the fire spot

Yeah

I can feel you

Yeah

I mean, this morning I was

Wearing like a lighter jacket

But it was pretty cold today

It's like, what the hell?

It changed

The weather changed

But I'm going to show you like

For instance, some things that

I've been doing

Oh, yeah

I've been doing

They tweet a lot

That music reminds me of

Billboard 7 for some reason

Yeah

Look, this guy for instance

He posted this video

This is created with the app

Nice

So this is very simple

Because this is done with the app

But if you export the models

You can actually 3D print them

Wow

That's the models that you want

These are factory as well

Wow

This is a project that we did with Hugo Boss

There's so many companies

That could just be built on top of this

Right?

You can support an industry in a sense

That's what we want

What we want

I mean, what we want

Basically

Like I was just thinking for my own years as a journalist

But

Okay

This is in Blender

So basically this is my family

I captured them with the app

Exported them

Put them in Blender

And created this nice

Nice

There's a Christmas card

It says Merry Christmas

Are you capturing environments yet?

Or no

I mean, right now?

Obviously you've done it before

But

Environments

No, for now we're only capturing people

So if you want to do environments

There's plenty of mobs that will do it better

Right

Yeah

We focus on this one

But you can see that

This is actually a rendering Blender

It looks actually pretty good

Especially given the fact that

We did it with a phone

So it's not

Yeah

It's not a professional voice that comes to your

No, I mean

I

Just myself

I can think of 7 business ideas

It's like

Exactly our marketing life

In the last 60 seconds

I've had like

Maybe 8 business ideas

Just watching this

Yeah

So you see

I mean

You can actually use it yourself

Try it

I mean

The first time

It takes like around 20 minutes or so

To do the reconstruction

We're now improving it

So it takes half of that

But

That's progress

Let's say

I mean

The app was launched

Just a few months ago

So it's been out only for

500

This fire is really nice

Oh yes

That's really good

Actually

Opening my jacket

Yeah

But yeah

So what we want

So when I talk about

The future of Polymetric Video

For me the future is

Is that it works on a phone

It doesn't work on a studio

I mean the studio is fine

For very high end Poly stuff

Or maybe the celebrities

But if

If we want this technology to be

In every application

In every

Every home

Let's say

It needs to work on a phone

It needs to be as simple as

I record a video

And then I get the video

Do you want to do a selfie here?

This is a good spot

You want to?

Mm-hmm

Okay

With the fire in the middle

Yeah we'll put the beer down

You don't want to really like

Kind of hide the beer a little bit

Cause

Yeah I'll put it down

I was kind of like

Yeah

Yeah

And I already know that Pete Davidson

Is just gonna see that

I always find it like

Impossible to

Share photos

If there's a beer in it

Because it's

It's not

It's not bad

But you never

Like I don't

Maybe my nephews

Are gonna see it

And I'm like

Maybe it's

Maybe

Because it

I always found that

There's like

A bad connotation

With alcohol

In the US

That it's not in Europe

Okay okay

I have the feeling

Because you know

That I'm

For some time

I had in my description

Like I'm addicted to concerts

Sports and beer

And somebody told me

Like one day

Like

It's not too nice

To put there

That you're addicted to beer

I mean I like beer

And like

Try new beers and stuff

But I mean

I don't drink it everyday

Yeah we have

There's like a cultural thing

There was like

There was like

There was like

There was a bunch of advertisements

About

Two things

One was

Alcoholics

And how they

Hurt family

Family members

Or something

And drunk drivers

And we had decades

Of this messaging

In our media

That I realized

In Europe

They never had this

And in Japan

They never had it

But here

We had it

So yeah

I realize

I've been conditioned

I ended up removing that

From my

Twitter description

And I don't have something

Like I'm addicted to coffee

Instead

I don't know if coffee

Has such bad connotation

Of alcohol

But yeah

I understand what you say

And I would be the opposite

I mean if I'm having a beer

I will share a photo

You know like

They're saying

Frost

It's like evidence

That you're having a good time

Which you know

People want you to have a good time

And then they can share with you

And so I like

Like theoretically

I understand the value of it

But I've been conditioned

I know totally

I'm also thinking

I'm a little bit conditioned



Because it's impossible to follow them

Yeah I wouldn't try it

I'm not a heavy drinker at all

Yeah me neither

But I enjoy the

You know

Chatting

And drinking

And so on

Actually since Christmas

I hadn't drink

Any alcohol

So today

Are my first three beers

Of January

Yeah it's weird

There was an Indiana Jones movie

The first one

Where

They had this

Sort of like

The beginning of the movie

The first scene was

This beer

It was like

A drinking contest

Between

Between the hero character

And this

This woman that he was interested in

They just kept drinking

To see who would be the first person to pass out

And at that point in time

It was pot

That was an okay idea

In American

Culture

And

That was

I think that was the last

Last time we saw that

In the movies

So

What else are you doing

With

With

With

With the

Volumetric

So

We're basically building the technology

And what we want is to integrate it in as many people

And as many places as possible

So we are

We're talking to companies

So they could

Do the reconstruction

And integrate it into other platforms

Say

I don't know

Roblox for instance

You can do a capture there

And then have it in

Roblox?

This is an example

It's the first company that came to my mind

Because you can import stuff in Roblox

Yeah I mean

And sell it

I think Roblox with all this kind of cartoonish looks

It's not the best example

This is the first one that came to my mind

But

Say that for instance

All the big

All the big platforms

Well Microsoft just bought Blizzard

And

They have Halo

I was shocked when I saw today

Like 70 billion

What?

Well

Yeah the price tag is high

But

But

Well actually I don't know if it is high

It's high

But it's probably worth it

But I'm surprised

They are this video game company

Right?

Yeah

And well

Microsoft has done a lot of video game acquisitions

Including like an Irish one

Multi-usual

Hava

They were doing the physics engine

That's not an Irish company

But

But in the context of the metaverse

I think that's the big picture for Microsoft

Totally

I think it's worth it

Because actually

Activision has the world of Warcraft

Doesn't it?

Yeah

I think

And that's basically a metaverse type of company

Yeah

But they already own Halo

They

They know

They have

Call of Duty

Yeah

I mean

I think it's a really good move

But actually

Activision was also in the middle of some

This pandemic again

With some sexual harassment and so on

So I think

I think they

The right time is the other company

Oh yeah

Oh yeah

And they're probably gonna kill all that from Microsoft

I actually think that Microsoft is doing really good at persistence in the gaming space

But the problem is that I don't see them taking the metaverse seriously

So they presented this mesh platform

But

That was already like a year ago

That was already like a year ago

And

I heard anything

Again

I mean

There's so many rumors that they're killing the HoloLens

And they're not able to deliver

But they were promising to the army

Yeah

You know how they win like

They won like a massive contract with the army

But I think that

I think meta has really sort of changed the entire technology industry by changing their name

I mean

That sent a powerful message

I agree

And

I was at AWE

And

They were calling it

The M word

The meta

The metaverse

Right

Because

I have like mixed feelings about that

On the one hand

I liked it

Because

They basically change

I mean

I've never been a hater on Facebook or Instagram

I'm not a fan either

But

But

I'm not such a big hater as some people is

And

And

I think that

Making that

Radical change

Is a good thing

Because it put our industry in the spotlight

And actually I was interviewed on the TV

Just because I work in this space

And

They were looking for Spanish people working in this space

So

I mean I have to thank Facebook for that

But on the other hand

I think the approach that they're looking for

Is not aligned with what I think the future is going to be

I don't think that we're going to be living in

Ready Player One

You know

I didn't see it that way

If it's

That way

It might be

In the next 20 years

Not in the next five

I don't think

And I hope that we don't live in that way

No I don't think

So I think there's something

Fundamentally wrong about

The Ready Player One

And

Snow Crash Visions

In that

I think

What might be

Slightly more plausible

Is sort of a mixed reality

Where we're bringing the 3D

The 3D programs into this reality

And sort of like

Maybe reskinning this reality

That's exactly what I think

Yeah

It's like

I've maintained the normal reality

In my opinion

It's a lot more valuable than

Forgetting about the real world

Than going into a new virtual world

Where you're going to be doing all your activity

No I would rather just put my glasses on

And it will tell me

This is the beer

This is

You rank this beer

And you have it in your library

And this is the score you gave it last time

You drank it

And I have your profile available for you

And I don't know

But you can make the bartender look like an auger

Yeah

Or we can actually say

Did you see the last game of the Giants?

And also you will see it here

You know

The replay in full 3D

And that's a lot more exciting

And both of us will be seen at the same time

In a shared experience

Social experience

Being in person

Yeah

Not being each of us in a different country

Which if we want to do that

We can do it too

But not as the normal thing

Yeah

I think that's way more

What you just described

Is way more realistic

Than that Ready Player One

Yeah

I actually really like the book

And really like the movie

But as a movie and as a book

Not as a reality

I wouldn't like that to be real

Yeah I mean I like the

I've read the book

Of course

I mean like everyone in the VR industry

I've read the book

Of course

Yeah I mean I

But the idea of going into movies like

Like War Games

And getting to like

War Games

That's a really funny movie

I loved it

I actually watched it only like last year

Or two years ago

Or something like this

I mean I saw it back in the 80's

When I was still a kid

God

So old

You're only five years older than me

But I've never seen that movie

Until

Yeah

Recently

And I actually liked it a lot

It was a very funny movie

How they were hacking the

With the chess

Teaching

That was actually

The beginning of a

Of a

What a neural network would be

Right?

Like at some point

Teaching

It's learning all the different games

Yeah

And figuring out that it's not worth it

Right

I mean yeah

That was like the first

The first

The first sort of the danger

Of automating your nuclear weapons

Or something

Yeah

I think that was even

I think that was really pre-neural

In a sense

I think that

It was sort of pre-neural networks

At least from the writers of the movie

Like I don't think they were aware

Of neural networks

But they were aware of computation

Yeah

And automation

Yeah

Totally

So yeah

I think that

I just started reading Ready Player 2 by the way

Like

So many people told me not to read that

I just started

So I've only read like

20 pages or something

For now

It's going

Everything's happening very fast

So I can tell you too much

I think I'm going to enjoy the book

Because I

I haven't read that one

That's the one that came with the concept of the metaverse

Right

That's from

That is actually where the metaverse term came from

Yeah

Yeah so I haven't read that

So I actually read Ready Player 1 only like 5 years ago or something

When I was starting with the company

That's when I read

Okay

So when you're disappointed by Ready Player 2

I will be

But at the same time

I'm probably going to enjoy it

Because you know

I'm not expecting anything from it

It's just entertainment

And I like nerd references and stuff like that

When I was reading the first one

I was thinking

I already knew that they were doing the movie

I had to know when they were going to release it

And I was like

It's impossible

That they were going to put all these references into one movie

They had to buy like so many different licenses of movies, books, music

And then the movie had many of them

Although they were different from the ones in the book

Like completely different

But okay

I enjoyed the movie

I mean it's not a great movie

But I had fun watching it

Yeah

I would lie if I told you that I didn't have a good time watching that movie

I wouldn't watch it again probably

But I liked it

It was fun

This fire made this whole conversation really work

I mean it's good





Going back to

LiDAR fusion

Are you doing LiDAR fusion

With video fusion?

Not yet

So the fusion

LiDAR fusion

You're assuming like

Kinect fusion

Like when you use a neural network

To combine video with LiDAR

Oh yeah we do that

You do do that

Well we do it

Kind of

So we do use the LiDAR

But we don't put it into the neural network

Wow

Because

The problem with LiDAR right now

Is similar to the problem that you had

With the depth sensors

They are

Very incoherent with time

Which means that

You still have a lot of variation

Points

And if you have like thin structures

Like an arm

You're doing like this

You will see a lot of flickering

In terms of the arm

So if you feed the LiDAR information

Into the neural network

So basically

The program is training

How do you train it?

You have to train it with some data

Synthetic data

Or some data that you have

Previously captured

And for that data

You have perfect depth

Let's say

You have your 3D model

You can render the depth

That is a perfect depth

Then

You teach the network with that

The network learns to follow the depth

Because it's perfect

Very accurately

And then you feed it

A crappy depth map

From the LiDAR

Which is not bad

But it's flickering

What you end up getting

Is a lot of flickering result

So what we've been doing recently

Is what we call

A crappy-fying GAN

It's a GAN network

That turns perfect depth

Into depth that looks like the LiDAR

So it makes it worse

Okay

It makes it work

So you can train a network

Yeah

To expect worse input

So when you feed it to bad input

It does it well

It doesn't follow the network

It doesn't follow the LiDAR map

As good as it would follow

That's interesting

So we are calling it a crappy-fying GAN

Yeah

Because we're making it worse

Wow

Do you have a paper out on that?

I

We're actually writing it right now

It's basically

It's a style transfer type of GAN

Let me send you a message to send me the paper when you're done with it

Yeah

Actually we did it with this guy

Kind of a student

He was doing it for his master

And right now we're trying to validate the results

Because the problem is that it's very difficult to get both perfect data

And also LiDAR data from that same dataset

So what we did recently is that we went to the studio

Captured people in the studio very high-end

And we captured them at the same time as with the LiDAR

So we can have the correspondence between the perfect depth and the LiDAR depth

And that's what we're going to be using to measure

So it's a big pain to do that

But we will be submitting it soon

What I definitely want to do is we like

Is to invite you in the next couple weeks or months

To do a clubhouse chat where we can sort of like present a lot of this conversation

But go even beyond because like the community of folks out there will have questions I never thought of

Yeah, I'm not the expert in your network so hopefully I will be able to answer

Otherwise I can get some money from our team

Yeah, the great thing is it's kind of a group chat

So if you can bring like a team of folks, we can all chat

And we'll record it and it will be on the internet forever

So

Don't say any secret or anything that you don't want to say

Don't say anything secret, yeah we're not going to be editing it live

It's like a radio show though

I normally am pretty open in general

Because I always think that if you're able to replicate what we're doing by what I'm saying

Then you deserve to use it

Because I'm of course not giving any details on implementation

And implementation is open

And I always say that also like what we're doing

It requires a lot of data to train

So do you have like a database of 20,000 3D models?

Because I mean that's a good advantage I would have

Because we've been doing 3D scanning for years

But if you don't have that

You know what's weird is like

So

In terms of like

You know there's been a lot of emphasis on like

Well you can't really succeed in AI if you don't have these huge models

The big companies are the ones that have huge models

I don't think the big companies have them

Big companies have the money to buy them

But that's it

Most of them don't have them

What I'm thinking is

I'm coming at this from like the perspective of biology

Where we have too much data

We have too much data

I'm like well if you're just

If you're filming live reality

You're getting more data than you can handle

Yeah

So we're not using all the models that we have

There are too many

Because you just have to think that

When we do one minute of volumetric capture

You're getting 30 3D models per second of volumetric capture

So if you're doing one minute you have like

Almost 2,000 models of the same person

You cannot feed all that to a network

Otherwise it will learn to follow that person very accurately

So you really have to filter the data out

Yeah

And we have captures of 10 minutes

So imagine 10 minutes

It's like several thousands of models

What I can tell that your company has done obviously

Is a huge amount of optimization and parameterization

To fit to sort of like

And we do different

Many different types of networks now

So we used to do only like segmentation

You know the Microsoft Reactor volumetric studio

Like they're like

Okay we're going to do this

We're going to put all this cube studio

There's a bunch of cameras

And we're going to charge customers

Like a million dollars a minute

And I'm like

Wait what?

A million dollars a minute for volumetric?

Come on

Or something like that

It's not that expensive

But a million dollars to set it up?

Yeah

I forget what the price was

But I think it's

It must have come down

A million dollars to set it up

Which means that

To get a return on the investment

For every minute of volumetric capture

You have to charge like

20, 30k

That's basically what it will cost you on that stage

So I understand what you've done on the phone

It's freaking amazing

That's just like

I mean the quality is not the same

Of course

But it's giving it to everyone

And it's going to get better

I mean with more data that we're gathering

From the usage from people

We're also building our own networks now

So for instance

When we were trying to train the

Crapifying gun

That one that I was telling you

We only had one phone with LiDAR

Mine

So I was basically capturing everybody that I know

And I think the very first version of the network

We only trained it with

20 or 30 different models

Like maybe a hundred different models

For each of them

But now we have the data for

Everybody who has ever captured

Anything with a phone with LiDAR

I mean we're not going to use it to anything other than training the network

We're not going to release it or sell it or anything like that

It's completely anonymized

We don't even know who is it

But it's a lot of data that we can actually use now

Yeah

So that's a good thing also

To have all this access to data

But it is true

At some point you have more data than you can actually handle

Filtering that data

Preparing it to make it work with your network

And then running your network

That can take maybe two weeks to train

Yeah

Yeah

I mean that happens also

No I mean you can learn how to run a neural network in a weekend

But curating a data set is

That's where the work is

There's a lot of people that say

If I only had one day to do a neural network model

I would spend 15 hours preparing the data

Right

That's it

That's it

Yeah exactly

The rest

Just training

Yeah

It is very important

And we've seen

So we have a big database

And it's annotated

And now we have also ways of

Doing automated labeling of the data

So you can

From a 3D mesh

You can know

This is the t-shirt

This is the hair

This is the face

This is the beard

That helps a lot also to train other hours

And it kind of makes your company a little bit

Invincible

Not invincible

But a little bit

Defensible

Against potential competitors

Because that is a lot of work

Yeah exactly

That's why

Every time I talk to investors

They ask me about defensive

Of course they are expecting an answer regarding patents

We do have a couple of patent applications

But I think the most important part

Is the barriers to entry

They are very high

I mean only

People who are not engineers

They don't understand how hard engineering is

Yeah

And even

Even if you

You have the knowledge

You would

Want to recreate something

That's what we have

You have the knowledge

And you have the code

And you have everything

You also need to have the data

And having the data

If you haven't been doing this for years

You have to acquire it somehow

That's very expensive

And

Yeah

Facebook might be able to do it

Just buy something

You spend

Yeah

By making acquisitions

Yeah

I know

And there's a couple of data sets

That you can buy

But typically

Each model costs you

It will cost you around 40

Or something like that

40 dollars

Yeah

Or acquisitions of

People who know how to do it

But yeah

But still

Even if you know how to do it

Then you have to spend some time creating the data

Right

That's a lot of effort too

They might be just working to buy us

Yeah

And then you get the data

Plus the knowledge

Plus the people

And everything

Yeah

So that's why I'm not

I'm not really scared about the big tech either

I think that the big tech at some point

Will implement something similar to what we're doing

That's good

But I think before they do it

They will talk to us

I'm sure

Well

I mean this conversation has a lot of value

For me in terms of filling in some

Some gaps I didn't know I had

Right

And

And then

Sort of my

One of my missions in life

Is to sort of like give back

And

Sort of like

Help everyone I can

And

So

If you want to tell the story in some way

And I can help in some way to tell the story

Yeah definitely

Thank you

And

Gladly accept the offer

I don't know in what form of course

But

Yeah I mean

I didn't have any objective with this

And dinner and beers today

More than just meeting you

And

Having a chat with somebody

That probably had an interesting story to tell

So

I'm happy to help

If I can help you in any way

And put you in touch with anybody

In the brain

But

I don't know I know anyone besides the

OpenBCI people

That you already know

So

My

My big goal is just to

Find somebody who

Maybe wants to help me get that

Jules Urbach interview done

Cause I

That's

That's not

That big of a goal

So

Yeah

I can

Just remind me that

And I'll

I'll tell you what I think you need to do

To get the

Say it best right now

What you need to do to get the

You know

A point cloud out of

The

The RGB data that you have

So

You did record it with the Azure Kinect right?

You said?

Yeah

I think

The very first Kinect

At least

They had a library

So it would allow you to do the transformation

From

The

Depth coordinates

Into the word coordinates

So



Yes

So

I register for a zip card

Have you ever used it?

The what?

The zip?

The zip card?

Actually

I haven't

So

I have a service that is similar in Ireland

And I use it all the time

Because I don't own a car

You know

My friend

I don't own a car either

But my friend

There's no reason for me to tell this

That's a stupid brag

But he's

He's

He's gonna like

Put a Tesla in my hands

That we're gonna share

And I just get to

But I get to sit on it

Here

He's in Vegas

So he's gonna be gone most of the time

And buy Tesla

I'm a huge Tesla fan

I'm all the time thinking

So my wife hates me for this

I'm all the time saying

No no

When we need to buy a car

We'll buy a Tesla

I'm like

No

We're not gonna buy a Tesla

That's very expensive





No

That's funny

Alright

Cool

Very nice meeting you

Yeah

Nice meeting you

It was a very interesting conversation

And I hope we continue it

Even if we speak Italy

Yes

I will continue coming

So

I mean

The pandemic was a little bit weird

But

Before the pandemic

I was coming

I don't know

Every two to three months

Or something

Because we always have

Either partners

Or events

Or investors

Or something like that

So

Yeah

There's always a reason for me to come

So anyway

I'm sure we will

We'll cross paths again

I'm looking forward to it

Yeah

Even if it's at Seagraph

Nice year

Although I think next year is in Vancouver again

I don't know

I don't know

Next time it's in LA

Yeah

Alright

Have a great goal

I'm gonna see what direction we have to go

I think it's that way

Okay

Have a good night

I'm heading out

Very nice meeting you

Likewise

I don't know
