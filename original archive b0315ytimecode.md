b0315y (Time Code + OpenAI's Whisper Transcription)

Title: Microtubules Consciousness Recursion


[00:00.000 --> 00:03.760]  But having some discussions about them early on is kind of important
[00:04.480 --> 00:07.080]  When you start to think about how these entities interact with each other
[00:07.080 --> 00:09.080]  So that's what this when it gets a little bit interesting
[00:09.160 --> 00:12.040]  So this is something that I've been thinking about and wanted to bring up
[00:12.360 --> 00:16.320]  It's that you know people people keep coming into the groups and they start
[00:16.880 --> 00:21.320]  sharing the panpsychist viewpoint and I have debated that so many times and
[00:22.040 --> 00:27.760]  Sometimes I just don't have the energy for it, but it's like but I but I guess it still needs to be discussed and it's like it's kind of like
[00:27.760 --> 00:32.000]  I think that the basic plot it like the most
[00:32.480 --> 00:38.380]  In the the biggest reason it can't move forward is that it sort of violates like information theory
[00:39.560 --> 00:42.960]  You can't have like in order to have consciousness
[00:43.000 --> 00:45.000]  That means that you're observing
[00:45.360 --> 00:52.440]  There's an observer who's observing right and you're observing something and that means that and like if you're observing music then then
[00:52.440 --> 00:57.720]  What's necessary to observe music like you can someone can hand you a record and you're not going to hear sound
[00:58.440 --> 01:03.200]  Someone can hand you a CD CD rom just like without the player and you're not going to hear sound, right?
[01:03.520 --> 01:10.560]  You have to have a decoder in order to turn the bumps on the record or the dots on the CD rom into
[01:11.440 --> 01:15.560]  Into sound and and it's the same thing with your brain. There's like there's no observation
[01:15.920 --> 01:22.200]  If there's a if unless there's a decoder there's information has no meaning unless it's the unless it's decoded
[01:22.200 --> 01:28.320]  So it has to be encoded and decoded and that means that like if there's no observation happening because there's no because
[01:28.320 --> 01:31.080]  There's no decoder, then there can't be any consciousness
[01:31.080 --> 01:37.760]  and like so like there can't be any consciousness in a rock because there's no observation happening and if if if
[01:39.120 --> 01:41.560]  There's no observation happening because there's no decoder in the rock
[01:41.560 --> 01:47.280]  We can see that there's no decoder in the rock where like in the human brain people our work. There's no there's no
[01:48.260 --> 01:51.240]  in my mind the the the there's no
[01:51.240 --> 02:03.200]  I guess the way I phrase it was the idea that the human brain is computationally interpreting
[02:03.200 --> 02:11.680]  the incoming signals from the environment in order to process them and decode them and
[02:11.680 --> 02:14.760]  perceive them is not a controversial idea.
[02:14.760 --> 02:21.200]  This is what most neuroscientists and I think most educated people believe.
[02:21.200 --> 02:26.200]  And yeah, so it's so, yeah, I'll start there.
[02:26.200 --> 02:32.880]  Yeah, great. I like the way you're talking thinking and I would love to actually make a
[02:32.880 --> 02:40.520]  bridge between what you're saying right now and what Ben was saying earlier about the relation
[02:40.520 --> 02:48.080]  to predicting the cosmos. Not the right word, but you know what he was referring to earlier with.
[02:48.080 --> 02:58.600]  So first of all, I am actually talking with philosophers, contemporary philosophers,
[02:58.600 --> 03:10.920]  about reintroducing parts of panpsychism, but not all. And the essence there is that we learn
[03:10.920 --> 03:18.040]  to understand in a different way. First of all, let me refer back to a good reason why
[03:18.040 --> 03:27.000]  we actually would think that consciousness is prior to matter. And I think that the way we are
[03:27.000 --> 03:34.480]  looking at it at the moment is explaining some things like the fine tune problems and giving
[03:34.480 --> 03:41.920]  an alternative to the whole idea of the multiverse. The thing I'm doing as an applied science,
[03:41.920 --> 03:49.080]  I want to have it grounded in the world and really fully emerged into the world. And like I said,
[03:49.080 --> 03:58.960]  I've always rejected any of this spiritual stuff and panpsychic stuff until the world itself showed
[03:58.960 --> 04:07.080]  me that the research I was doing on this was exactly that. It made that there's another way.
[04:07.080 --> 04:15.440]  Let me do that metaphor. You can think about it in a different way. Think about a vacuum,
[04:15.440 --> 04:22.440]  right? Now, on this planet, we actually only have artificial vacuum, but we have it everywhere.
[04:22.440 --> 04:28.840]  We use it for our combustion engines to make a lot of work for us. So we're using it everywhere,
[04:28.840 --> 04:37.000]  but it's fabricated in an artificial lens. So in an atmosphere, in a...
[04:43.000 --> 04:54.240]  Your mic is breaking up a lot. Oh, so maybe the connection is this better? I hope this will be
[04:54.240 --> 04:59.280]  better. Difficult to know without a statistically significant sound for me, so keep going. Okay,
[04:59.280 --> 05:08.480]  so what I'm trying to say is that think about your brain as a container to keep an electrical
[05:08.480 --> 05:18.240]  storm together. If I would take atmosphere into space without a container, it would disperse.
[05:18.240 --> 05:25.800]  It wouldn't exist at all. So in my view, the stones and the plant and many of those material
[05:25.800 --> 05:33.920]  stuff around us is in that kind of sense not linked to consciousness. Consciousness is to me
[05:33.920 --> 05:41.240]  like this whirlpool of electrical lightning, let's say, but that can actually exist outside the
[05:41.240 --> 05:52.800]  brain as well. And if I link it to much larger scale like creating the conditions to get to the
[05:52.800 --> 06:00.560]  fine-tuning structure where life can emerge, that's a very different balance game. So it is
[06:00.560 --> 06:07.400]  pantheism, but it's not pantheism. And something that Ben said about what if all the different
[06:07.400 --> 06:13.920]  consciousnesses are emerging at the same time, that's exactly the thing that we are at the
[06:13.920 --> 06:20.360]  moment working on with the concept of what is called tribo-poasis. So you have auto-poasis,
[06:20.360 --> 06:25.040]  which is basically the system creating itself, but then you have like a little bit of a problem
[06:25.040 --> 06:33.080]  like where the hell is this coming from? And while with tribo-poasis, it is actually more in line
[06:33.080 --> 06:41.600]  with also the fact that we are a multicellular creature. So like a multicellular creature has
[06:41.600 --> 06:49.480]  at one level cells that can autonomously reproduce and do whatever they want. But on a very, on
[06:49.480 --> 06:57.480]  an aggregated level, there is an influence because if I stop eating, I will eat my muscles to keep
[06:57.480 --> 07:05.040]  my brains alive. So there is a higher order at play. And the same way we are looking now at this
[07:05.040 --> 07:11.160]  concept of tribo-poasis. And what I'm trying to do like in the research on the origin of life is
[07:11.160 --> 07:18.360]  look how convection cells, they always emerge out of the fields that gets far from equilibrium,
[07:18.360 --> 07:24.400]  but then they emerge together. It's not one convection cells, it's a collective of convection
[07:24.400 --> 07:30.520]  cells. And you can only understand that because there is like this feedback structure that will
[07:30.520 --> 07:36.280]  improve the flow. And many of the things I'm learning and understanding about consciousness is
[07:36.280 --> 07:40.880]  exactly this. I mean, you can wake up in the morning and you can be in a state like where the hell am
[07:40.880 --> 07:48.120]  I? So you're not really conscious of the things that, while your brain is perfectly functional,
[07:48.120 --> 07:54.040]  but then the flow gets a little bit going and then you're back into it. And there's
[07:54.040 --> 08:03.560]  similar kind of, well, exercise you can do like if you dream, you can wake up and be in a mindset
[08:03.560 --> 08:08.240]  of a lot of work and forget your dream right away. But sometimes you can actually try to get back
[08:08.240 --> 08:13.640]  to the dream by going and standing in the feeling you had when you were dreaming. And then where
[08:13.640 --> 08:20.560]  you stand in the feeling of that dream, suddenly all the memory comes back in. So in that way,
[08:20.560 --> 08:25.360]  that there's this whole link and there's a strong link with things that we see in Zen Buddhism and
[08:25.360 --> 08:33.960]  so on, where you go via meditation into a different kind of awareness. So there are many, well,
[08:33.960 --> 08:39.640]  there's still a lot of mystery around all of that, right? But we try by not being too concrete,
[08:39.640 --> 08:46.920]  like many of the examples you gave, Mikhai, is for me too concrete. If you try to be a little bit
[08:46.920 --> 08:57.440]  more abstract, then things may actually get explained by first seeing how like a cybernetic
[08:57.440 --> 09:04.600]  world can actually start changing the physical world and how that's kind of a bootstrapping
[09:04.600 --> 09:10.400]  relation and Mixland not speaking. And by the way, I probably need to go for a meeting in a few
[09:10.400 --> 09:19.760]  minutes. Okay, I can respond to that. So it sounds like you're, I guess to try to paraphrase
[09:19.760 --> 09:26.480]  what you're saying, something about consciousness being just the electrical activity or just the
[09:26.480 --> 09:32.560]  electromagnetic activity, and the brain just being a container for that activity to keep it
[09:32.560 --> 09:40.880]  together. But maybe you think that if that electrical activity could occur in a different
[09:40.880 --> 09:46.120]  kind of container, like maybe inside the storm of Jupiter, then there could be consciousness there,
[09:46.120 --> 09:51.760]  right? Something like it doesn't have to be in the brain, it could reformulate somewhere else,
[09:51.760 --> 09:56.120]  it's not necessarily connected to the brain. The brain is serving it.
[09:56.120 --> 10:01.400]  It's happening in our culture. What? It's already happening in our culture. You cannot disconnect
[10:01.400 --> 10:05.880]  your own consciousness from the language you speak and the culture you're living. So you don't
[10:05.880 --> 10:13.120]  even need to go to so far away. Okay. So on the topic of on the topic of neuroscience,
[10:13.120 --> 10:17.760]  though, I mean, I guess I guess what I want to share is what I think is the missing missing piece
[10:17.760 --> 10:25.360]  here. And that I like the painting, but I think there's one missing piece and I wanted to share
[10:25.360 --> 10:33.880]  it is that, you know, from neuroscience, I think that there's a well, okay, so there's a there's
[10:33.880 --> 10:37.840]  like a ton of examples that and I have to kind of piece it together. I know you may not have
[10:37.840 --> 10:44.120]  enough time. But so what's the one example is that, you know, they do like the the split break,
[10:44.120 --> 10:53.000]  the split break brain patients, if the famous scientists. So anyway, the idea was that every
[10:53.000 --> 10:57.480]  time you cut it, if you do like a major incision that that, you know, cuts across the corpus
[10:57.480 --> 11:04.400]  glosum or just divides the brain half or a major surgery or if there's a major injury and, you
[11:04.400 --> 11:11.240]  know, like a brain surgery, like they we see personality changes, people's personality changes.
[11:11.240 --> 11:19.360]  And so that that's like the that's the first sign that the brain tissue is directly causing the
[11:19.360 --> 11:26.400]  electrical pattern. If the soul if the soul was the electrical pattern, and it was in the brain
[11:26.400 --> 11:32.040]  tissue was just a container, then then how is changing the brain tissue changing that like
[11:32.040 --> 11:36.720]  changing that person's personality. But that's what it's one big thing that people have to juggle
[11:36.720 --> 11:43.960]  with. Yeah, so I want to respond to that thanks because it makes it a lot more concrete. So the
[11:43.960 --> 11:51.000]  way I look at the mind brain, so conscience as it's like a workbook, right? So it's a feedback
[11:51.000 --> 11:58.760]  mechanism keeping itself in a kind of artificial construct. So what happened if you cut the brain,
[11:58.760 --> 12:04.680]  you should get two workbooks. And that's exactly what we see if we can do experiments with them
[12:04.680 --> 12:12.120]  and have the left brain and right brain ask them different questions and actually capture how they
[12:12.120 --> 12:17.400]  have the different answers to that. Like you let them write and you let them say something. And you
[12:17.400 --> 12:22.200]  answer the question and you see that what's been written and what's been said is different. So this
[12:22.200 --> 12:28.640]  kind of stuff really fits perfectly with what we call that tribal poesis. So it's the whole idea
[12:28.640 --> 12:35.720]  that everything you see around you can actually be understood from a process perspective. Everything
[12:35.720 --> 12:46.280]  is feedback structures creating feedback. Okay, well, so there's another there's some other
[12:46.280 --> 12:51.600]  problems with that. And so like, but what's what's really interesting, I think in it is a missing
[12:51.600 --> 12:59.640]  piece is how much the brain tissue is involved in generating that electrical storm. And, and like,
[12:59.640 --> 13:05.640]  you know, every time every time a neuron fires or a neural circuit fires, you know, a micro a micro
[13:05.640 --> 13:14.920]  column of activity spikes, a column or a region spikes, you have one region and another like
[13:14.920 --> 13:25.160]  there's the dipoles firing off. Do you know the global workspace hypothesis? Yeah, I do. But I
[13:25.160 --> 13:31.320]  have but I but I have issues with that too. I have had my own hypothesis. The reason I'm
[13:31.320 --> 13:37.960]  mentioning it is because I've noticed that. So that's the first time I noticed that the system
[13:37.960 --> 13:44.000]  of creation. So the feedback system I was working with in our team was popping up again. So I was
[13:44.000 --> 13:49.600]  wondering, like, how come that this neuroscience is actually explaining something that I'm from
[13:49.600 --> 13:54.960]  artificial intelligence totally disconnected from that research also had as a system. So
[13:54.960 --> 14:03.160]  that's one of the first time. So the end the deeper how like having this for different kind of
[14:03.160 --> 14:10.960]  leverages can actually create feedback on creation. And if you understand how you can create that
[14:10.960 --> 14:17.640]  feedback on creation, you can actually see how even cultures are consciousness and aware as a
[14:17.640 --> 14:28.080]  feedback structure like that, but just on a way slower rate. So the like speeding up plans and
[14:28.080 --> 14:34.680]  see then how plans behave like animals. It's a little bit the same. So there is like, as well,
[14:34.680 --> 14:41.120]  but it's moving so slow, it's called science guys. So but the thing that that you can see is if
[14:41.120 --> 14:52.400]  your system of creation, you will get more aware. And you can. So it's already hard coding our
[14:52.400 --> 15:02.200]  brains. But our actions can actually also improve that kind of flow. I'm mixed up. So okay, look,
[15:02.200 --> 15:07.080]  so I still I still think the idea that you're saying violates information information theory,
[15:07.080 --> 15:12.880]  which is the final part of what I was going to say, because, you know, you're not only are
[15:12.880 --> 15:17.160]  neurons generating this electrical storm, but they're, but the neurons themselves are the ones
[15:17.160 --> 15:22.960]  that are decoding and the the information in the brain that includes any includes any result of
[15:22.960 --> 15:31.320]  this electrical storm that there is is is the electrical storm might be altering the positions
[15:31.320 --> 15:36.960]  of electrons, which change the firing rate and change the brain activity patterns. And but
[15:36.960 --> 15:43.640]  the but the only way for the for information to be detected or for for an observation to happen is
[15:43.640 --> 15:48.280]  because the neurons are making that observation that the brain activity itself doesn't have
[15:48.280 --> 15:54.080]  decoders and therefore it has no sense of information. And it's not conscious. It's not not
[15:54.080 --> 16:04.000]  an observer. But that's because you believe that information is in your brain. No, no, that's
[16:04.000 --> 16:13.440]  because I believe that that information has has to be has to have that in a that an observer has
[16:13.440 --> 16:21.280]  to have a decoder for an observation to happen. Otherwise, information has no meaning. But the
[16:21.280 --> 16:27.880]  thing is that that embodiment can be extremely simplified to four forces interacting on each
[16:27.880 --> 16:36.360]  other. I don't know if we're gonna solve this debate tonight if you have to leave in a minute
[16:36.360 --> 16:42.520]  because this requires some maybe another hour of argument. But maybe you come back another
[16:42.520 --> 16:48.680]  time and I would do that, by the way. I'm really I'm really like the way you're thinking and the
[16:48.680 --> 16:55.040]  fact that you're critical because if the way because if you that kind of critical response
[16:55.040 --> 17:01.960]  allows to create a language to express the things that we can see from like Mars mathematical
[17:01.960 --> 17:11.320]  perspective. But like, yeah, if you want to have like some idea of what that four forces creating
[17:11.320 --> 17:19.280]  the system of creation looks like, the most easiest case I can see around me is sailing
[17:19.280 --> 17:25.600]  against the wind. In the case of sailing against the wind, you are also using four forces to
[17:25.600 --> 17:35.920]  actually create a very strange, well, or paradoxical movement. But there is like a cost. And the
[17:35.920 --> 17:44.200]  cost relates to this person you do in the the the fluid, where you can do the sailing. And the
[17:44.200 --> 17:49.640]  thing with the sailing against the wind, the four forces is that there is no constructive knowledge
[17:49.640 --> 17:54.520]  to build on top of. So if you then start doing that with things like machine learning, you can
[17:54.520 --> 18:02.840]  actually see how you can create an evolution of creativity. But yeah, you're right. Actually,
[18:02.840 --> 18:13.400]  my date is not yet here. So I keep going until I have. So I so I so I don't I haven't heard of this
[18:13.400 --> 18:21.240]  thing called the four forces system of creation. I looked it up. And are you talking about the four
[18:21.240 --> 18:26.600]  basic forces of physics, like gravitational force, electromagnetic force, nuclear force, and strong
[18:26.600 --> 18:39.280]  nuclear force? I'll send you the link of my. So because the whole idea is that the four forces
[18:39.280 --> 18:46.840]  always work on workspace, but workspace is embodied in the space. Well, there's a there's a system
[18:46.840 --> 18:52.440]  that tries to create the user workspace. But there's also an environment that has influence on
[18:52.440 --> 18:57.920]  that workspace. The in case of sailing against the wind, the four forces are like this. You have
[18:57.920 --> 19:05.880]  first, it's called lift and drag. So those are the forces on the sail. And what happens if you
[19:05.880 --> 19:11.600]  combine those two? Well, if you play with those two forces, you can actually create a force that
[19:11.600 --> 19:17.560]  has like a 90 degree angle against the wind. But you are not yet against the wind. So it seems
[19:17.560 --> 19:23.440]  impossible to sail against the wind. So what actually happens is that you are in a medium,
[19:23.440 --> 19:29.800]  your boat is in the water. And there's like this keel in the boat that makes the boat go only in
[19:29.800 --> 19:34.760]  one direction, basically. So it's easy to go back and forth. But it's hard to go sideways. That's
[19:34.760 --> 19:38.080]  what you have with the boat. That's why you turn the boat if you want to go in a different direction.
[19:38.080 --> 19:43.320]  So what you do is you put your position in the boat in such a way that the lift force,
[19:43.320 --> 19:50.480]  like this 90 degree angle against the wind has maximum effect on the viscosity of the water.
[19:50.480 --> 19:57.840]  And the viscosity of the water will actually be a reaction force. And so that's the cost you pay
[19:57.840 --> 20:04.320]  to actually move across the 90 degree angle. And then you cannot sail straight against the wind
[20:04.320 --> 20:11.760]  when you need to do is like zigzag all the time. So and if you translate all that kind of forces
[20:11.760 --> 20:18.600]  to how we learn, you actually get to understanding how there's a feedback structure on learning
[20:18.600 --> 20:24.720]  that allows us to create novelty. So creating something fundamentally that doesn't exist yet
[20:24.720 --> 20:32.480]  in your existing knowledge. But I'll send you the link to the PhD so you can actually see the
[20:32.480 --> 20:40.000]  forces, see how it works, and how I make the link with this combination. Okay, okay, okay,
[20:40.000 --> 20:44.600]  but I just want to say that I am not hearing like a counter argument to what I'm saying that,
[20:44.600 --> 20:51.200]  and I've had further thought. So the argument was that, you know, electromagnetism, the
[20:51.200 --> 20:59.080]  electromagnetism in the brain, the electrical storm has no way to decode information that the
[20:59.080 --> 21:05.600]  neurons are passing through it. It would, and I think it would, if like, if you're combining,
[21:05.600 --> 21:13.520]  you know, information together that it might, yeah, it just like I don't, so yeah, so if you
[21:13.520 --> 21:19.520]  look up information theory and decoding, like that is just, that's essential for an observation to
[21:19.520 --> 21:25.480]  happen is that the information that's received has to be decoded for in order for it to be
[21:25.480 --> 21:31.600]  transmitted has to be encoded. And if you don't have a physical mechanism to, yeah, I follow you,
[21:31.600 --> 21:40.280]  but they do make one big mistake. And that mistake relates to the concept we know in cybernetics
[21:40.280 --> 21:46.480]  as bootstrapping. And the best example I can give is called the bootstrapping compiler,
[21:46.480 --> 21:57.400]  where you actually create a non-existing compiler in a language that doesn't, is decoded yet.
[21:57.400 --> 22:02.160]  What you make a trick, what you will do is instead of creating a compiler, you create an
[22:02.160 --> 22:07.560]  interpreter in that language. So that's a virtual space. But in that virtual space,
[22:07.560 --> 22:16.160]  you actually give the program itself as input. And what it will do by the recursive structure
[22:16.160 --> 22:23.880]  it will actually create a compiler in a programming language. You never wrote one code line of
[22:23.880 --> 22:29.280]  coded because it's bootstrapping itself through itself. So it's like the snake eating itself.
[22:29.280 --> 22:35.280]  If you start playing with that idea that if you can create a virtual space that can decode,
[22:35.280 --> 22:43.360]  that then can actually create the code to run itself, you actually solve the problem. And that's
[22:43.360 --> 22:48.720]  like, that's why I'm referring to the whirlpool as, I mean, first the fluid is just going in
[22:48.720 --> 22:55.240]  any direction, but as soon as you create that feedback structure, you can start playing with
[22:55.240 --> 23:03.280]  that. And if you go to programming, that playing with recursion is something we do all the time.
[23:03.280 --> 23:09.440]  But it's pretty ill understood outside of the domain of programming modeling. So the thing
[23:09.440 --> 23:15.280]  that you need to do then is use a lot of the programming modeling techniques to understand
[23:15.280 --> 23:22.160]  how we can understand cognition. So I know it requires quite interdisciplinary approach and
[23:22.160 --> 23:33.960]  that's the biggest challenge, I guess. Okay, so you're saying that the topology of the ionic
[23:33.960 --> 23:44.560]  activity in the brain is like a whirlpool that is, you're suggesting that somehow it's gathering
[23:44.560 --> 23:52.440]  information and transmitting information and understanding information? There's an advanced
[23:52.440 --> 24:00.440]  case in, so if you look up convection cells, you will see that convection cells. So there is actually
[24:00.440 --> 24:08.920]  a Nobel Prize in chemistry in 1977 given to it because it was break through science. But it's
[24:08.920 --> 24:15.760]  the thing that you see, so the convection cells show you how a system far from equilibrium can
[24:15.760 --> 24:22.280]  actually start self organizing. So that's one aspect, but the convection itself doesn't yet
[24:22.280 --> 24:30.080]  tell you how it's coding. There has been an advanced experiment where they start heating
[24:30.080 --> 24:35.600]  the sides of those convection cells and what happened is depending on a certain temperature,
[24:35.600 --> 24:42.600]  you actually get a different pattern and you see clearly, purely from topology, how information is
[24:42.600 --> 24:53.960]  coded in flow and that can easily happen in your brain as well. How then that kind of patterns
[24:53.960 --> 25:01.280]  start changing the world is yet another bootstrap. But if you start understanding how that processing
[25:01.280 --> 25:08.000]  of bootstrapping is working, so A creates B, but B is not good, it's artificial and temporal. But
[25:08.000 --> 25:13.480]  the temporal and artificial thing like a scaffold is used to create something structural. And then
[25:13.480 --> 25:18.320]  when that structural thing exists, the scaffold can go away again. And that's exactly the idea of
[25:18.320 --> 25:28.160]  a scaffold, right? I'm sorry, I'll pause. I need to take a break. You go ahead and respond first.
[25:28.160 --> 25:32.440]  I just wanted to bring a couple of things back down to it. So as much as I like thinking about
[25:32.440 --> 25:36.320]  the fate of the universe and stuff, I also like to make sure I keep this shit real. So when you
[25:36.320 --> 25:41.120]  talk about convection cells, of course, in everybody's house right now, you've probably got
[25:41.120 --> 25:44.840]  some double glazing. So there's a perfect example of that type of convection cell. You're talking
[25:44.840 --> 25:49.960]  about metal, literally a pair. An example would be double glazing and why you need the correct gap
[25:49.960 --> 25:54.920]  between the panes of glass if it's too small. Then you've just not got enough, you just not
[25:54.920 --> 25:59.240]  got enough depth of air to be, to be a depth of an insulator. And then when the gap between the
[25:59.240 --> 26:04.160]  glass gets too big, you start to get these convection cells set up. They emerge in the space
[26:04.160 --> 26:08.680]  and then they create a transport mechanism from heat from the cold out of pain, obviously,
[26:08.680 --> 26:13.680]  the way from the hot inner pain to the outer pain through conduction and convection. I mean,
[26:13.680 --> 26:18.200]  that's a perfect example of something's actually in people's hopes, right? That's a nice one. I
[26:18.200 --> 26:23.880]  didn't know that one. But yes, that those are also convection cells. But you are talking now
[26:23.880 --> 26:31.400]  about convection cells of air, right? Yes, if you go to your original experiment, because you try
[26:31.400 --> 26:38.720]  to make it as feasible as possible, they use oil. And so heating up the oil actually leads to this
[26:38.720 --> 26:48.640]  honey cube grid of cells transporting heat from the bottom to the top. So the glass is a very good
[26:48.640 --> 26:54.680]  example. Thanks for that, Ben. Yeah, you can see it in there just with the puffer smoke. So that's
[26:54.680 --> 26:58.880]  that. And then there was another thing that Micah just said that really made me think of another
[26:58.880 --> 27:02.440]  example where we can really simply bring it back down to earth. Micah, just remind me what you
[27:02.440 --> 27:07.600]  said at the end of what you said and it will spark the recall of that once the last thing
[27:07.600 --> 27:14.840]  I was thinking I was thinking about how I was thinking about how the brain is a as a whirlpool
[27:14.840 --> 27:24.920]  of activity. That was what I asked if it was gathering and then transmitting patterns. But
[27:24.920 --> 27:33.240]  but also maintain its maintaining its own equilibrium. So so so I guess I guess the point
[27:33.240 --> 27:39.840]  was that convection pattern is I guess it's gathering a pattern and transmitting a pattern
[27:39.840 --> 27:50.040]  as an example. Back to the back to your brain back to the observer who sees it. But so I guess
[27:50.040 --> 27:56.400]  you can make an argument that there's a way to transmit patterns. But but I'm still but but you
[27:56.400 --> 28:08.760]  know, from and sort of like from that's that's interesting. Um, and I guess I didn't figure
[28:08.760 --> 28:19.480]  Ben to remember. Yeah, no, I haven't looked at the clubhouse thing of trying to balance
[28:19.480 --> 28:23.400]  come contribute to the conversation with having a shave with realizing I need a shout before
[28:23.400 --> 28:27.920]  my 9 30 meeting. And so I'm just trying to listen in. But no, I can't remember what the other
[28:27.920 --> 28:32.320]  concrete example was. But there was there was a really good just grounding example of
[28:32.320 --> 28:41.480]  something that might renew the mix of other than USA. No, so that's okay. Let's go. Was
[28:41.480 --> 28:53.360]  it about the the whirlpool of of electromagnetic flux creating coding on itself? No, it was
[28:53.360 --> 28:56.520]  not. I mean, there was one other comment about recursion is, of course, in programming,
[28:56.520 --> 29:00.800]  anything you can express recursively, you can express iteratively. So if you can't get your
[29:00.800 --> 29:06.640]  head around a recursive way of describing things, I, you know, like the towers of annoyance
[29:06.640 --> 29:10.640]  a great example. When I learned to solve that both as a program, but also as a person, you
[29:10.640 --> 29:15.760]  know, I think Henry's a toy shop with those those three annoying posts with the with these
[29:15.760 --> 29:19.840]  decreasingly small discs on them. And you know, your challenges that you've got like
[29:19.840 --> 29:23.440]  five discs on a post with holes in them, like donuts, you know, five increasingly small
[29:23.440 --> 29:27.200]  donuts, making a little tower, and you have to move those all to another tower. But the
[29:27.200 --> 29:30.200]  rule is that you never have to put a bigger one on top of a smaller one. How else can
[29:30.200 --> 29:34.840]  you do it if that's a recursive, perfect recursive problem? A very simple way of doing it is
[29:34.840 --> 29:38.800]  you move you imagine them in a circle, rather than in a line, you say, remove the smallest
[29:38.800 --> 29:43.280]  this one space left, and then do the only other allowed move, and then keep repeating
[29:43.280 --> 29:47.720]  that. That's an iterative way of describing how to do the towers of annoy a recursive way
[29:47.720 --> 29:53.360]  that would be to say, well, move one disc to another post, and then everything else becomes
[29:53.360 --> 29:58.240]  a sub problem. So recursions about like the problem being that when a problem has itself
[29:58.240 --> 30:01.480]  described as a sub problem, a fractal kind of nature, you can be recursive, but you can
[30:01.480 --> 30:06.840]  also translate that to iterative. It's just, in some problem spaces, an iterative solution
[30:06.840 --> 30:09.760]  to the towers of annoyance is a good programming challenge, by the way, for any programmers
[30:09.760 --> 30:13.080]  out there, if you want to try and write some code to tell you how to solve the town of
[30:13.080 --> 30:17.840]  annoyance, you're not allowed to use recursion, I suspect your program could be a lot longer.
[30:17.840 --> 30:21.200]  I'd be very interested to know, actually, if anybody has the knowledge whether it fundamentally
[30:21.200 --> 30:26.000]  needs to be longer than a recursive algorithm. That's interesting because that talks to compression
[30:26.000 --> 30:31.080]  and simplification and representation, but it certainly would appear to be longer. So
[30:31.080 --> 30:35.480]  that's just another random thought I had, that the recursion is not sacred, if you like,
[30:35.480 --> 30:39.360]  it's just a way of representing a problem. And I don't know if it impresses down to
[30:39.360 --> 30:46.360]  the same sort of size program as an iterative version, fundamentally. It probably does because
[30:46.360 --> 30:50.560]  I think they compiled down to similar code with a good compiler, so I think I've answered
[30:50.560 --> 30:56.360]  my own question, but it's stuff about compression and compilers and that way of looking at how
[30:56.360 --> 30:59.400]  information works is quite interesting because it grounds the conversation nicely when you
[30:59.400 --> 31:02.440]  start talking about things like compilers and bootstrapping compilers and the likes.
[31:02.440 --> 31:05.920]  Yeah, I like that direction. I'm done. In fact, I'm just going to listen in for the
[31:05.920 --> 31:11.120]  next five minutes because I've got a shower. Okay, thanks Ben, and I like the way you think,
[31:11.120 --> 31:22.080]  so I'm happy we met here. I actually wonder if, I mean, now you put that name in my head.
[31:22.080 --> 31:26.600]  This would be rude if I didn't un-move and say the feelings of mutual, of course, it's
[31:26.600 --> 31:29.320]  always great to meet people on here, but if I don't reply now, it's because I literally
[31:29.320 --> 31:33.800]  am in the shower. Yeah, sure, go ahead. You know how it goes on club high. You never
[31:33.800 --> 31:39.600]  need to apologize for not responding, right? And saying that, I mean, now we actually got
[31:39.600 --> 31:44.520]  this meme in my head, like trying to figure out if the length actually says something
[31:44.520 --> 31:52.520]  about the compression when it's a recursion. So I actually had this, I'm just going to
[31:52.520 --> 31:59.800]  share it, I mean, it's happy hour, right? I was combining the research on artificial
[31:59.800 --> 32:05.640]  intelligence. So that's one lab with research on programming modeling lab. And I got kind
[32:05.640 --> 32:12.120]  of in trouble with that because the two labs don't speak at all. And at a certain moment,
[32:12.120 --> 32:16.680]  like when you're talking about this expressivity, I was thinking like, you know, in artificial
[32:16.680 --> 32:22.520]  intelligence, there's a lot of interest in cultural language and how that affects the
[32:22.520 --> 32:28.840]  thing you can express. So I was wondering, is it vice versa as well? Can I actually start
[32:28.840 --> 32:36.520]  talking about a programming languages and the poetic ability of a programming language?
[32:36.520 --> 32:41.600]  And what I did is I start looking at different programming languages and finding this very
[32:41.600 --> 32:47.800]  strange expression, you only have in one language. And then I give a presentation about like
[32:47.800 --> 32:53.400]  the poetic nature of a programming language. And as of that day, I'm not allowed in the
[32:53.400 --> 33:03.880]  lab anymore. You know, there was this great, there was a great, this guy, he wrote this
[33:03.880 --> 33:11.560]  programming language that basically like, like, it's all like rock music. Let me see
[33:11.560 --> 33:19.240]  if I can find it real quick. But it's like, every line that you write in the library is
[33:19.240 --> 33:29.840]  basically like us like a like a rock song. I look it up.
[33:29.840 --> 33:34.160]  There are way more people in the room. So if anyone likes to come up on stage, let me
[33:34.160 --> 33:43.040]  reset the topic. It's happy hour. So basically, you can talk about anything you want. We are
[33:43.040 --> 33:55.680]  talking related to neuroscience, but editing goes. So come on stage and let's talk.
[33:55.680 --> 34:02.160]  I found it. So if you if you'd like, if you look on, it's just called the rock star language.
[34:02.160 --> 34:05.880]  And you can find it on you can just Google rock star programming language. I'm sure it
[34:05.880 --> 34:12.080]  will come up or could you send me the link via Twitter? I just send you my PhD as well.
[34:12.080 --> 34:17.720]  So you should have gotten the link. Yep, I'll do that.
[34:17.720 --> 34:23.360]  Hi, Dasha. Hi, this has been a fascinating wormhole of
[34:23.360 --> 34:29.320]  whirlpools to listen to for the last 20 minutes before I thought I was going to go to bed.
[34:29.320 --> 34:38.000]  I'm sure my nerves in my brain will have a lot more material for my dreams tonight.
[34:38.000 --> 34:49.200]  Yeah, I just want to say that I'm really amazed. Actually, after talking to Mixol, my brain
[34:49.200 --> 34:54.240]  is firing because here's the thing. It's like, you know, one of one of the one of the things
[34:54.240 --> 35:00.600]  is like, you know, there there is a, you know, the human being is a dissipative system. Like
[35:00.600 --> 35:06.920]  we are vortex that there are oscillations in the brain. There's there are cycles and
[35:06.920 --> 35:14.040]  feedback loops and self maintaining physics and the physics of every cell and the body
[35:14.040 --> 35:20.240]  itself and in a fractal way is trying to maintain equilibrium. Like these are all concepts that
[35:20.240 --> 35:29.400]  are sort of like tied into the what what the concept of a whirlpool needing to, you know,
[35:29.400 --> 35:37.920]  maintain its its own order with the, I guess, the topology of recursion was was the was what
[35:37.920 --> 35:43.000]  Mixol said that I kind of wanted to hear more about that because when because when Mixol
[35:43.000 --> 35:47.640]  said recursion, I thought of, wait, is he talking about like a recursive neural network? What
[35:47.640 --> 35:54.840]  is what is this? It's spot on with the dissipative systems really spot on. So so it is really
[35:54.840 --> 36:00.120]  like a recursion of recursions of dissipative systems. So and it that actually goes back
[36:00.120 --> 36:07.480]  to topic discussions we had in 2004 on machine consciousness. So we had several people in
[36:07.480 --> 36:14.440]  the field of cognition coming together, starting to talk about machine consciousness. And one
[36:14.440 --> 36:18.840]  of the things that came out of it is like, maybe we really need to look at the mind as
[36:18.840 --> 36:27.080]  a virtual machine, like we know from Java. And now all nice. And it's a good metaphor.
[36:27.080 --> 36:33.160]  But mean, we try to get grounded. How do you do it as an applied scientist. And so when
[36:33.160 --> 36:38.720]  we realizing that we are actually also in that kind of recursion with with our culture,
[36:38.720 --> 36:45.640]  I start digging into digital cultures. And at a certain moment, that's 2010, I start interviewing
[36:45.640 --> 36:54.520]  every founder of a starter baker system that was really successful between 2005 and 2010.
[36:54.520 --> 36:59.320]  And I was asking all of them, like, why are you doing open innovation? So my assumption
[36:59.320 --> 37:07.680]  was that they would have taken the culture of open source development and just projected
[37:07.680 --> 37:14.560]  it on the business development. But when I told them what open innovation is about,
[37:14.560 --> 37:19.360]  they all start like, okay, that's a bad idea. We're not going to do that. So then I confronted
[37:19.360 --> 37:22.800]  them with the fact that it was happening in the ecosystem. So how can it happen in the
[37:22.800 --> 37:27.520]  ecosystem? If that's not the thing you want to have. And what we figured out is that there
[37:27.520 --> 37:34.400]  was actually a dissipative system at the cultural level. And so first, the dissipation, what
[37:34.400 --> 37:40.800]  is getting dispersed is the ability to do to do work. So in open source, you have the
[37:40.800 --> 37:47.560]  policy of community contribution, 10%, 20%, sometimes even 100%. You can contribute to
[37:47.560 --> 37:54.440]  your own project in the community. And that kind of disperses all the codes in relation
[37:54.440 --> 38:02.560]  to anything that can grow, basically. But there was a second policy. And it's the second
[38:02.560 --> 38:08.840]  policy that really created the feedback leading to the Whirlpool, basically. So it wasn't
[38:08.840 --> 38:15.320]  open innovation, it was self organizing innovation. The second policy was that all entrepreneurs
[38:15.320 --> 38:21.320]  were very aware that they were living in a fragile ecosystem. And that their action
[38:21.320 --> 38:29.720]  as a body, as an organization, had effect on the ecosystem. And they needed to take
[38:29.720 --> 38:34.640]  care of that ecosystem. So all the time they were questioning, if I do this from a business
[38:34.640 --> 38:41.960]  perspective, is it bad, good, or neutral to the ecosystem? And if it was bad, it basically
[38:41.960 --> 38:48.240]  was a no go. The prioritization was always when it was good for the ecosystem. Neutral,
[38:48.240 --> 38:54.760]  you could do it, but good, you got priority. And by that very simple structure, there was
[38:54.760 --> 39:02.720]  this kind of self organizing innovation happening during those moments. After 2010, there were
[39:02.720 --> 39:11.480]  a lot of corporates getting into the ecosystem. And I kind of got shut out of all the back
[39:11.480 --> 39:16.680]  office discussions. So I kind of stopped doing participation research in that community.
[39:16.680 --> 39:24.560]  But just to let you know how that cultural has this effect on the awareness and the
[39:24.560 --> 39:33.760]  ability and the embodiment that the agents below could do. I hope that helps. But basically,
[39:33.760 --> 39:44.080]  it's all about the dissipative systems.
[39:44.080 --> 39:50.440]  Okay, while you were talking, I was just, so I mean, I have, like I said, I have my
[39:50.440 --> 39:54.320]  own, like, see, there's global workspace theory and there's different theories of consciousness
[39:54.320 --> 39:59.320]  and dashes on the stage. Welcome. If you want to say something, go ahead.
[39:59.320 --> 40:06.560]  Oh, I mean, I was just fascinated by the conversation. I also am very sad. Mix L, I don't know if
[40:06.560 --> 40:11.600]  I'm pronouncing your name correctly, that you were ejected from a group after you brought
[40:11.600 --> 40:18.800]  poetry and art into the equation about AI and learning. My mother works at the National
[40:18.800 --> 40:25.240]  Science Foundation as a program director in exactly the areas of, you know, language and
[40:25.240 --> 40:37.400]  technology. And she's very fascinated by the fact that, you know, cultural language issues
[40:37.400 --> 40:46.320]  are really what is stagnating AI a lot of, you know, in jokes, just cultural in jokes.
[40:46.320 --> 40:59.560]  And humor is just so difficult for truly AI to encompass and possess. So it's, I'm kind
[40:59.560 --> 41:04.840]  of sad to hear that you were, when you were bringing these topics into the conversation
[41:04.840 --> 41:11.080]  that you were met with resistance. I'll let me quickly respond. Don't be sad. I mean,
[41:11.080 --> 41:16.800]  it's all rock and roll, basically. I'm scaring people all the time by the fact of creating.
[41:16.800 --> 41:21.840]  I mean, I'm all the time, I mean, the thing I really love is being part of that system
[41:21.840 --> 41:27.440]  of creation. And like the most recent thing I start scaring is basically everyone around
[41:27.440 --> 41:33.960]  me when I start moving up as a digital sharp man. So I mean, I don't mind that so much
[41:33.960 --> 41:39.960]  that you keep on moving and it's the part that's the essence, right? So if people don't
[41:39.960 --> 41:45.760]  want to continue that, that's okay. It's everyone is, it's their own issues they need to work
[41:45.760 --> 41:51.040]  on there. You can just keep on going. So, but thanks, I'm personally sad for, for just
[41:51.040 --> 41:56.640]  the moving forward of that concept, because that is really at the core of what needs to
[41:56.640 --> 42:02.040]  be focused on how we're going to get there. How are we going to have an AI that understands
[42:02.040 --> 42:12.360]  and can create real poetry just beyond abstractions and art and, and humor and, and language
[42:12.360 --> 42:22.440]  with playful learning and understanding. And yeah, it's, I think, yeah, keep, keep carving
[42:22.440 --> 42:28.760]  away. And, you know, if you ever are looking for an NSF grant, you know, there's a, there
[42:28.760 --> 42:35.080]  are people in that area that, that definitely do care about those things. So interesting.
[42:35.080 --> 42:42.160]  So very quick response again, that I actually working on a concept called Interversity.
[42:42.160 --> 42:48.320]  And it's the whole idea of if you have, if you look at the before the internet, you had
[42:48.320 --> 42:53.600]  information locked up in books. And it was hard to be creative with that information.
[42:53.600 --> 43:00.160]  Today we can create apps and be really creative. And so the information gets alive. The same
[43:00.160 --> 43:05.880]  actually exists with research. Research is at academia is quite conservative, locked
[43:05.880 --> 43:12.280]  in schools of thought and locked in labs. But now it's going digital. You actually start
[43:12.280 --> 43:18.800]  doing creative research. And I'll send you an article I have about the Interversity and
[43:18.800 --> 43:24.280]  how I'm trying to work on that concept. So, so thank you, Dasha. But I see there are more
[43:24.280 --> 43:29.200]  people on stage. So take, take the floor. Go ahead.
[43:29.200 --> 43:34.440]  Hi, I did I see Mark raising his hand.
[43:34.440 --> 43:50.760]  I had something to say as well. There's a thread that I was following in quantum biology
[43:50.760 --> 43:56.480]  that had something to do with neuroscience. And I was, I wanted to see if anybody had more
[43:56.480 --> 44:04.920]  information around this idea of quantum tunneling and how that processes information or what
[44:04.920 --> 44:11.080]  it had might have to do with processing information in the brain and what it might have to be
[44:11.080 --> 44:19.280]  consciousness. I'm not sure if anybody else is following that thread. It was a talk at
[44:19.280 --> 44:28.000]  the Royal Institute that got me on to this idea of a thread of quantum biology. It's
[44:28.000 --> 44:34.560]  all in neuroscience and consciousness.
[44:34.560 --> 44:46.280]  So are you are you talking about Orc or Yeah, yeah. Orc or and his name, but yes.
[44:46.280 --> 44:53.160]  Um, well, you know, I wish they would I wish they would say more. It's not it's not it's
[44:53.160 --> 45:01.440]  not clear to me and I think I'm not clear to a lot a lot a lot of other people how exactly
[45:01.440 --> 45:08.080]  the theory is supposed to. I mean, there's like there's too many there's too many missing
[45:08.080 --> 45:12.960]  pieces in terms of how it's supposed to. So, so my I have many thoughts about this. So,
[45:12.960 --> 45:20.800]  I in terms of what the issues are that I think they need to solve. And maybe so so with with
[45:20.800 --> 45:29.880]  Orc or like briefly like there's a lot of research that set that points to people having
[45:29.880 --> 45:37.960]  memory issues if their microtubules are collapsed, right. And so there's a there's a correlation
[45:37.960 --> 45:45.160]  between microtubules and memory. And microtubules are basically kind of a mystery in terms of
[45:45.160 --> 45:50.720]  like, what do they have to do with memory, they just they seem to be just the structure
[45:50.720 --> 46:00.760]  that holds up the that helps create the cell. And so this the the I what I think happened
[46:00.760 --> 46:06.680]  and I don't know exactly is I think the neuroscientists who came up with Orc or theory, he really
[46:06.680 --> 46:12.960]  needed a way to explain what what microtubules did that somehow related to memory that that
[46:12.960 --> 46:18.560]  you know result that like when they were destroyed that memory became an issue. And so he he
[46:18.560 --> 46:28.560]  came up with the idea that that their structure was good for storing and interacting with
[46:28.560 --> 46:36.040]  quantum effects at the at the temperature of the brain. Whereas maybe other parts of
[46:36.040 --> 46:41.360]  the brain would would not have a structure that would be would it would enable information
[46:41.360 --> 46:48.640]  to to exist or traffic back and forth at the at a quantum sort of level. So so he created
[46:48.640 --> 46:56.160]  this Orc or theory and it's like, yeah, so the the information could be stored in in
[46:56.160 --> 47:02.120]  the microtubules like as like a quantum vibration or something. I am not 100% clear. And then
[47:02.120 --> 47:08.240]  somehow it could travel out of the microtubule again. And and maybe even it could be like
[47:08.240 --> 47:12.480]  a random fluctuation could travel out of the microtubule and it could affect the firing
[47:12.480 --> 47:20.120]  of of a neuron. So so this is like, it's just like a really sort of like obscure idea. But
[47:20.120 --> 47:23.520]  I mean, I think what's sort of problematic is that there's there's actually an alternative
[47:23.520 --> 47:31.480]  explanation for why microtubules, microtubule collapse could be causing memory issues in
[47:31.480 --> 47:39.680]  the and that would be like the it is the it is like so like, like for the way memory is
[47:39.680 --> 47:46.680]  thought to work is that memory involves basically, there's short term memory, long term memory,
[47:46.680 --> 47:54.920]  and there's and the long term memory. You're gonna you can look up like long term potentiation.
[47:54.920 --> 47:59.400]  You can look up long term depression. And these are like the major mechanisms of long
[47:59.400 --> 48:05.760]  term memory. And then short term memory is, I guess, it's thought to be about neural circuits,
[48:05.760 --> 48:17.080]  triggering sequences of firing. And over over, you know, maybe 10 seconds. And but the but
[48:17.080 --> 48:22.000]  the long term memory, it's like, what has to happen apparently, based on there's this
[48:22.000 --> 48:28.360]  research of Alzheimer's patients where, you know, it's been it's been like for so for
[48:28.360 --> 48:32.120]  a long term memory potential for long term potentiation, a long term memory, you need
[48:32.120 --> 48:37.120]  to have to store memory, you need to a new protein has to be synthesized. It's just part
[48:37.120 --> 48:43.200]  of the process. And in order for a new protein to be synthesized, there needs to be a space
[48:43.200 --> 48:48.440]  for that protein synthesis to happen. And if the cell has collapsed, because the microtubules
[48:48.440 --> 48:52.480]  have collapsed, then there's no space for that new protein synthesis to happen. And
[48:52.480 --> 48:56.040]  that's a new memory could inform. And thus, there is a correlation between the collapse
[48:56.040 --> 49:01.520]  of the microtubule. But because it's like the support structure for the new protein
[49:01.520 --> 49:06.360]  synthesis to happen. That's that's this is what I so this time saying is like there's
[49:06.360 --> 49:12.320]  an alternate reason for why a microtubule collapse could be involved, could be correlated
[49:12.320 --> 49:16.480]  with memory that may have nothing to do with the microtubule itself being a storage place
[49:16.480 --> 49:22.040]  for memory. So there's to that the first issues that there may be just no reason for microtubules
[49:22.040 --> 49:28.160]  to do that. And the second reason is that there's no clear like, okay, so I guess the
[49:28.160 --> 49:33.880]  fluctuation that comes out of a microtubule could somehow affect the, maybe the ionic
[49:33.880 --> 49:40.640]  charge of. So so so neurons, so neurons when they when they fire when they create their
[49:40.640 --> 49:46.360]  firing pattern, it has to do with like the separation of positive and negative charges.
[49:46.360 --> 49:53.200]  Maybe a tiny fluctuation that comes a quantum fluctuation could could change the position
[49:53.200 --> 49:59.800]  of an ion or something. And that might affect the the the firing rate of an neuron. But
[49:59.800 --> 50:03.440]  it's just it's such a subtle it would be such I think it would be such a subtle effect
[50:03.440 --> 50:09.480]  that and and, you know, so maybe maybe it does or maybe it doesn't. But maybe the brain
[50:09.480 --> 50:15.720]  has sort of like random stuff happening at that scale or even larger all the time anyways.
[50:15.720 --> 50:22.240]  And it just has the brain is very good at sort of like getting rid of noise. So if there
[50:22.240 --> 50:25.920]  is like sort of like signals coming out of the microtubule, it might not be sufficient
[50:25.920 --> 50:30.880]  to because the brain has like is trying to filter out noise. So it can just focus on
[50:30.880 --> 50:35.200]  what isn't noise, which is information, you know, you have brain has to separate noise
[50:35.200 --> 50:41.240]  from information. So I don't know that it would wreck that the brain is set up to recognize
[50:41.240 --> 50:46.800]  the quantum signaling that's supposed to come from microtubules. I mean, I would like to
[50:46.800 --> 50:53.840]  find out. And but I think we need more research and I don't know if anyone's doing this research.
[50:53.840 --> 50:57.560]  But but I think that's where we're kind of I think so what I just I think we're kind
[50:57.560 --> 51:01.640]  of stuck right now with Oracle and right and right now the neuroscience community, because
[51:01.640 --> 51:07.120]  because we're so stuck on the neuroscience community is not to my knowledge, not kind
[51:07.120 --> 51:13.600]  of not really focused on it, and kind of like not trying to talk about it in the end. So
[51:13.600 --> 51:19.440]  it's not it's not a popular topic, because the issues will work. It we're kind of I
[51:19.440 --> 51:25.960]  think that it's really it's a really hard sort of thing to solve. And there are other
[51:25.960 --> 51:31.600]  things that are easier to solve right now. And another point is that like, whether there
[51:31.600 --> 51:40.360]  is a role for quantum effects in memory or not, it doesn't really matter. Because what
[51:40.360 --> 51:46.440]  I'm saying is like, memory is sort of the I think that there's a really good, you know,
[51:46.440 --> 51:50.600]  I guess they call it Hebe and learning or the idea that your that your neurons are detecting
[51:50.600 --> 51:56.840]  coincidence patterns and in the coincidence patterns or like, or like the basis of of
[51:56.840 --> 52:01.960]  or the detection of coincidence patterns like the basis of detecting information. And and
[52:01.960 --> 52:10.800]  so the idea is that whether there's quantum effects happening or not with the microtubules,
[52:10.800 --> 52:15.920]  that doesn't change the basic realities of how memory works. It wouldn't say we wouldn't
[52:15.920 --> 52:20.800]  say that long term potentiation is invalidated or long term depression is invalidated invalidated
[52:20.800 --> 52:25.760]  or that the existing knowledge of short term or long term memory changes, we would just
[52:25.760 --> 52:31.480]  say, Okay, well, there's additional information that's coming from and going to microtubules
[52:31.480 --> 52:36.520]  in addition to what's already true. So it doesn't, it's not a revolution per se, but
[52:36.520 --> 52:43.160]  it might so so maybe down the road, 50 years from now, or I don't know how long there,
[52:43.160 --> 52:49.720]  people will say, Oh, well, there's this additional thing where quantum effects are are in are,
[52:49.720 --> 52:55.720]  you know, have a sort of two way traffic that are that are that are involved with interactions
[52:55.720 --> 53:00.640]  in human memory. But it wouldn't, it wouldn't change the it's it's sort of like it's it's
[53:00.640 --> 53:07.280]  not wouldn't change the paradigm, it just would would add a little more complexity to
[53:07.280 --> 53:09.280]  how everything works. That's my thoughts.
[53:09.280 --> 53:17.680]  I just got an insight when you told that like, oh, my crap, maybe the whole microtubules
[53:17.680 --> 53:26.720]  are yet another dissipative system that exists below the embodiment of the the current other
[53:26.720 --> 53:31.480]  static of DNA and stuff. It's kind of triggered my mind. So thanks a lot.
[53:31.480 --> 53:37.680]  I mean, I think honestly, that the entire universe is fractal dissipative systems. So
[53:37.680 --> 53:47.600]  just that's my feeling, but I mean, so like a cause, a cosmos is a dissipative system
[53:47.600 --> 53:54.360]  it is a cyclone, it has a it has a rotation, it has to maintain its equilibrium, it exports
[53:54.360 --> 53:59.600]  increasing extra p to the surrounding environment has all the that's that's that's is that that
[53:59.600 --> 54:03.800]  is that I did I say cosmos I meant galaxy galaxy, you can see if you look at a picture
[54:03.800 --> 54:10.840]  of a galaxy, it's a cyclone, just like a hurricane. If you look at the the diagram of of an atom,
[54:10.840 --> 54:15.240]  it's a cyclone. These are like, so we have cyclones in the atoms and we have cyclones
[54:15.240 --> 54:20.000]  in our galaxies and everything in between is a cyclone as well. A human being is as is
[54:20.000 --> 54:27.800]  a dissipative system is a human being as a life form that is a cyclone. In on paper,
[54:27.800 --> 54:32.320]  if you look at the physics that a human being has to maintain equilibrium or maintain life
[54:32.320 --> 54:37.120]  has to export increasing entropy to the surrounding environment to survive to maintain that equilibrium.
[54:37.120 --> 54:42.600]  A cell does the same thing as another cyclone does a human cell. So like the entire universe
[54:42.600 --> 54:49.440]  is is is a fractal of cyclones. That's so funny. I mean, one of the things that inspired
[54:49.440 --> 54:56.720]  me a lot. So when I was earlier talking about the convection cells, so there is a guy, a
[54:56.720 --> 55:04.360]  professor in the University of Brussels, who got a Nobel Prize for that in the 1970s.
[55:04.360 --> 55:11.840]  And in the last thing he actually wrote was a book in 1996, called The End of Certainty.
[55:11.840 --> 55:18.000]  So it's The End of Certainty, Time, Chaos, and the New Laws of Nature. And I really like
[55:18.000 --> 55:25.320]  the way that he's actually starting from what we really understand from advanced thermodynamics
[55:25.320 --> 55:33.360]  to get a better insight of what is happening at the quantum level. And that aligns exactly
[55:33.360 --> 55:39.760]  with this whole idea of everything is like fractal structure. Well, fractal. The problem
[55:39.760 --> 55:46.640]  is fractals are very often just considered the recursive structure. Well, if you actually
[55:46.640 --> 55:54.840]  look at the essence of a fractal is about fracture, it's about being rough edges, and
[55:54.840 --> 55:58.800]  how you can actually start measuring with that. So that I know there's always quite
[55:58.800 --> 56:07.480]  some confusion of the recursive nature and the fractal nature. But I follow you mean,
[56:07.480 --> 56:16.680]  it's always all the time this dissipative system popping up at the next emerging level.
[56:16.680 --> 56:20.800]  And that kind of aligns with the stuff that the philosophers are doing with the emerging
[56:20.800 --> 56:29.880]  vector theory. The emerging what? Emerging vector theory. So that's it's something philosophers
[56:29.880 --> 56:40.120]  are pretty heavy on at the moment. To stop trying to, yeah, I'll try to look it up and
[56:40.120 --> 56:44.160]  see if I can give you a link. But if you want, there is actually, I recently had a talk with
[56:44.160 --> 56:54.840]  a guy called Alexander Bart. And we talk about things like tribal poesis and emerging vector
[56:54.840 --> 57:01.640]  theory. I know he's a big defender of those kind of theories. But we actually have this
[57:01.640 --> 57:09.000]  group called the Intellectual Deep Web, where we have like this meaningless discussions
[57:09.000 --> 57:11.600]  talking deeper on these topics.
[57:11.600 --> 57:20.760]  All right. Let's see. I was, so Carl or anyone else, does anyone else want to say something
[57:20.760 --> 57:30.240]  on this pause? Well, I was more sorry, not to interrupt Carl. If not, I just wanted to
[57:30.240 --> 57:36.560]  bring back the conversation there to chronology for just a second. And kind of tunneling in
[57:36.560 --> 57:42.560]  the role of whether there might be any space there to discover more about free will, because
[57:42.560 --> 57:47.120]  it seems like there's absolutely no evidence for free will and plenty of evidence against
[57:47.120 --> 57:56.800]  it. But I'm wondering, you know, some of the questions that quantum biology bring up for
[57:56.800 --> 58:03.280]  us in consciousness have anything to do with free will at all. And so what might that be?
[58:03.280 --> 58:10.680]  I would like to refer to Don Hoffman then. I mean, if you look at the work Don Hoffman
[58:10.680 --> 58:17.920]  has been doing around brain activities and eventually coming to mathematics of the free
[58:17.920 --> 58:26.240]  wave particle, there's kind of, we have a very different way of looking at the whole
[58:26.240 --> 58:33.800]  quantum level. But you may actually try to have a look at his work. So that's Don Hoffman
[58:33.800 --> 58:44.840]  and I think it's the case against reality. So I have a different recommendation. I recommend
[58:44.840 --> 58:54.720]  a book called The Neurobasis of Free Will by the Neurobasis of Free Will Criterial Causation
[58:54.720 --> 59:14.160]  by Peter TSE, Peter Ulrich TSE. So Peter is making the argument sort of that. So it's
[59:14.160 --> 59:25.000]  like, so organisms are making decisions and that our thoughts are, our neurons are detecting
[59:25.000 --> 59:30.920]  information, they're detecting information versus coincidence patterns and then detecting
[59:30.920 --> 59:44.880]  features and objects and our neural networks are, what am I trying to say? So at some point
[59:44.880 --> 59:49.160]  where our neural networks are considering information, they're considering the models
[59:49.160 --> 59:58.160]  of reality that they've collected and they're predicting possible futures and then they're
[59:58.160 --> 01:00:03.920]  sort of like making a decision towards the, some future that they liked more or that you
[01:00:03.920 --> 01:00:10.000]  liked more. You like, maybe you like the orange or maybe you like the apple, but it's your,
[01:00:10.000 --> 01:00:16.080]  it's your mind, your consciousness that has received information, considered information
[01:00:16.080 --> 01:00:23.120]  and made a decision towards one possible future or another. And it's, it's like your, but
[01:00:23.120 --> 01:00:27.680]  it's like that mind is not, I think of that mind is not separate from physics, that decision
[01:00:27.680 --> 01:00:32.200]  is not separate from physics. And I think people get into trouble and say, okay, well,
[01:00:32.200 --> 01:00:37.160]  well, you know, if you had, okay, so if you had no consciousness itself, no one had consciousness,
[01:00:37.160 --> 01:00:44.880]  but our brains just made decisions. So like grab oranges or grab apples, would anyone like,
[01:00:44.880 --> 01:00:51.360]  someone else made up this argument, but would anyone like, you know, need to, well, I mean,
[01:00:51.360 --> 01:00:55.320]  so if someone was conscious and they were watching this happen, you know, like if we're, in
[01:00:55.320 --> 01:00:58.920]  that scenario, why would we need, why would anyone, if there's no consciousness, why would
[01:00:58.920 --> 01:01:05.240]  anyone need free will? So this idea is like, the idea of free will came, came from, I think
[01:01:05.240 --> 01:01:10.040]  this idea that the free will is sort of separate from physics, or that the mind is separate
[01:01:10.040 --> 01:01:14.800]  from the brain, you know, that it's not the same thing that it's not the mind isn't the
[01:01:14.800 --> 01:01:22.440]  brain somehow. And, but if, if, if, if you don't have a separate self, if you just are
[01:01:22.440 --> 01:01:33.240]  one with the universe, then there's no separate self to be making any decisions. It's just,
[01:01:33.240 --> 01:01:39.080]  it's just, you know, the organism that is you is making decisions, right, but it's not separate
[01:01:39.080 --> 01:01:45.640]  from the universe. And so there's like, there's no conflict between you making, doing what
[01:01:45.640 --> 01:01:52.080]  you want. And so, and in making the choices that you want to make. But those choices are
[01:01:52.080 --> 01:01:56.160]  not going to be free from physics. And as soon as you like phrase it in terms of like,
[01:01:56.160 --> 01:02:00.720]  are you saying, wait, are you saying that my choices are free from physics? And everybody
[01:02:00.720 --> 01:02:04.400]  says no, no, they're not, they're not free from physics. That's not what we're saying.
[01:02:04.400 --> 01:02:09.600]  But aren't you saying like that my, my mind is not connected to, to my brain. It's like,
[01:02:09.600 --> 01:02:13.000]  somehow like it doesn't interact with my, with my brain. It's not what you're saying.
[01:02:13.000 --> 01:02:17.240]  And then they say, no, no, that's not what we're saying. It's like, it only works. The
[01:02:17.240 --> 01:02:25.040]  argument only works if the, if you don't let, if they let you prevent them from, from getting
[01:02:25.040 --> 01:02:28.360]  into the fine details. Like as soon as, as soon as like, yeah, your mind is based on
[01:02:28.360 --> 01:02:35.320]  physics, your, your, your decisions are going to be based on physics. You're not going to
[01:02:35.320 --> 01:02:40.480]  escape physics. So what exactly are you trying to say your will is free from? What is your,
[01:02:40.480 --> 01:02:44.720]  what do you need your will to be free from? Do you need your will to be free from physics
[01:02:44.720 --> 01:02:48.120]  or, or like really, like, what does it, what does the idea come from? It's like, so I think,
[01:02:48.120 --> 01:02:54.600]  I think that maybe the idea of free will on our culture is like, it's like, you know,
[01:02:54.600 --> 01:02:59.320]  my brain controls my hand and my brain doesn't control my friend's hand. It doesn't control
[01:02:59.320 --> 01:03:06.360]  your hand. So your will is free from my will. Your will is free from other people's will.
[01:03:06.360 --> 01:03:11.640]  And everyone has their own free will from each other to some extent. And I think that's
[01:03:11.640 --> 01:03:17.880]  what it means. But there's no separate, there's no separate self to be making decisions. There's
[01:03:17.880 --> 01:03:22.200]  just this idea of a self and there's like conscious thoughts that are part of the process
[01:03:22.200 --> 01:03:28.200]  of making decisions. And, and with the quantum, with the quantum realm gives us, if it gives
[01:03:28.200 --> 01:03:34.360]  us additional data patterns through microturbules, you know, maybe, maybe it gives us like some
[01:03:34.360 --> 01:03:38.600]  randomness or maybe it gives us some additional information we didn't have before, or sends
[01:03:38.600 --> 01:03:41.360]  and receive more information. But if what I'm saying is like, if all you're doing is
[01:03:41.360 --> 01:03:45.760]  adding and receiving more information. And it's like, basically, like it's like, imagine,
[01:03:45.760 --> 01:03:51.880]  imagine you have like a computer, and the, and, and the computer processes 10 applications
[01:03:51.880 --> 01:03:56.480]  a day. And then you say, okay, we're going to like add some quantum effects. All right,
[01:03:56.480 --> 01:04:00.840]  adding quantum effects to the computer. Okay, so that's a quantum computer. What's different?
[01:04:00.840 --> 01:04:07.280]  Well, instead of processing 10, 10 files per day. Now with the quantum effects, the computer
[01:04:07.280 --> 01:04:11.720]  is going to process 11 files per day. And we're going to make an argument that that
[01:04:11.720 --> 01:04:18.120]  means that the computer has free will. It's still a computer, even if you add quantum effects
[01:04:18.120 --> 01:04:22.920]  to it. And so a human being is still a human being, even if you add quantum effects. That's
[01:04:22.920 --> 01:04:27.200]  what I'm saying. It's just like the same, it's going to be the same process. Yeah.
[01:04:27.200 --> 01:04:41.120]  Just wondering if Carl is wanting to say something he's been waiting for. Yeah. Yeah, no. Yeah,
[01:04:41.120 --> 01:04:54.800]  we can be enlightened through the use of multidiscipline infection.
[01:04:54.800 --> 01:05:04.960]  You're breaking up a little bit, Cecil. And I think all Carl is also. Thank you, Cecil.
[01:05:04.960 --> 01:05:11.280]  So Carl, the floor is yours. Can you hear me? Yeah, I can hear you. But I'm very well,
[01:05:11.280 --> 01:05:17.760]  Cecil. You're breaking up a lot. Maybe I'll just cut off. But anyway, what I wanted to
[01:05:17.760 --> 01:05:26.800]  say earlier is that the point of being down. Sorry, Cecil. You're breaking up again. It's
[01:05:26.800 --> 01:05:31.520]  too bad you have this bad connection. So and I really want to hear Carl if you don't mind.
[01:05:31.520 --> 01:05:37.720]  Hi there, no problem. The conversation's been fantastic. I've been in here for about, I
[01:05:37.720 --> 01:05:43.960]  don't know, about 30, 45 minutes now. A lot of it has been over my head a little bit.
[01:05:43.960 --> 01:05:51.880]  Not so much over my head, but rapidly expanding my horizons. I don't have a background in neuroscience
[01:05:51.880 --> 01:06:00.080]  or philosophy. But this has been incredibly interesting. And one thing that has entered
[01:06:00.080 --> 01:06:06.680]  my mind is as everybody's been talking and giving sort of the back and forth and whatnot
[01:06:06.680 --> 01:06:13.960]  has been that a lot of this just seems about perspective. As in, you know, we were talking
[01:06:13.960 --> 01:06:22.840]  and talking about trying to define consciousness and whatnot. And you know, we're saying, you
[01:06:22.840 --> 01:06:27.760]  know, rock doesn't have consciousness, but then could a tree have consciousness? You
[01:06:27.760 --> 01:06:33.280]  know, a human has consciousness because can can an ape have consciousness? And it just
[01:06:33.280 --> 01:06:38.120]  seems and then we went all the way up to a sort of cosmic level or a solar system level.
[01:06:38.120 --> 01:06:42.600]  And is it not? I mean, this may be a very fundamental thing that I've just never heard
[01:06:42.600 --> 01:06:48.520]  about before. But is it not just the case where we're really talking about here is perspective
[01:06:48.520 --> 01:06:54.080]  as in if you take a perspective of size and the perspective of time as in if you take a
[01:06:54.080 --> 01:06:58.640]  tree, for instance, and you were to invite someone along who hadn't studied before and
[01:06:58.640 --> 01:07:01.720]  to look at a tree and say he's the tree conscious and the person would say, well, no, the tree
[01:07:01.720 --> 01:07:05.080]  isn't conscious. It's not it's not doing anything. It's not reacting to anything. You should
[01:07:05.080 --> 01:07:12.560]  have a personal time lapse of a day with the sun going overhead or maybe external factors
[01:07:12.560 --> 01:07:17.680]  acting on the tree. Then you would see reactions. You would see processes happening as an ex
[01:07:17.680 --> 01:07:21.240]  as a reaction to an external stimulus. So at that point, then you could say, well, the
[01:07:21.240 --> 01:07:27.640]  tree is conscious of something it has. I love that definition of of a decoder. I think it
[01:07:27.640 --> 01:07:33.920]  was I can't remember who was talking. I think it was Micah. Yeah. Yeah. Sorry. If I'm just
[01:07:33.920 --> 01:07:41.520]  much, you know, yeah, I love that because yeah, that kind of makes logical sense to
[01:07:41.520 --> 01:07:46.640]  my very untrained mind. But then if you keep going up, you know, you say, well, okay, humans
[01:07:46.640 --> 01:07:50.400]  are conscious to something. And if you go up even further, you say, well, a country,
[01:07:50.400 --> 01:07:58.320]  mentioned a country or state earlier. Well, if the state if the state is is has these
[01:07:58.320 --> 01:08:05.320]  individual points of electrical firings, which are as individual people. And then as a whole
[01:08:05.320 --> 01:08:09.760]  is then reacting to other states as an organism is that they're not a level of consciousness.
[01:08:09.760 --> 01:08:13.360]  And if you go up even further, you say the solar system, you spread across solar system
[01:08:13.360 --> 01:08:20.880]  or a galaxy versus a galaxy, if you have one galaxy filled with people or you know, a spreading
[01:08:20.880 --> 01:08:24.040]  of people, different people, different alien civilizations, whatnot, you know, we're talking
[01:08:24.040 --> 01:08:30.800]  about those bubbles joining together. I think it was Ben, when I first came to the room.
[01:08:30.800 --> 01:08:35.240]  So that then again, you've got consciousness acting. So then, so then in my mind, what
[01:08:35.240 --> 01:08:39.160]  you're saying is, and I'm literally verbal verbally regurgitating, trying to make sense
[01:08:39.160 --> 01:08:45.440]  of this, I do apologize if this isn't very coherent. At that point, then, are we talking
[01:08:45.440 --> 01:08:50.560]  about just when there is a lens is in when you have a system, a chaotic system. So if
[01:08:50.560 --> 01:08:54.720]  you look at our brain, you have that chaotic system even down to the quantum level of things
[01:08:54.720 --> 01:09:01.440]  firing at such a level, which for our current technology is very hard to map and predict.
[01:09:01.440 --> 01:09:06.240]  And then above that, you have a cognitive lens, which tries to make sense of that and
[01:09:06.240 --> 01:09:14.080]  order it. And I imagine you have that you have that sort of, you know, is that is that
[01:09:14.080 --> 01:09:20.680]  the defining point of it? Or is there something special about us as human beings in our current
[01:09:20.680 --> 01:09:27.520]  state, which defines consciousness, different, you know, do we have something special or is
[01:09:27.520 --> 01:09:34.480]  there something unique that happens, you know, either as a whole in the system, or with that
[01:09:34.480 --> 01:09:41.360]  lens, that cognitive lens, which is, which is trying to tease out chaos, order out of
[01:09:41.360 --> 01:09:47.120]  chaos, or direct a system of chaos, is there something special, or does it just depend
[01:09:47.120 --> 01:09:54.080]  on your perspective, you know, if you change your your frame of reference of time and size,
[01:09:54.080 --> 01:09:57.320]  could you classify anything as conscious?
[01:09:57.320 --> 01:10:06.440]  Right, that's the essential question. If you the second you can define consciousness as
[01:10:06.440 --> 01:10:11.600]  so as, you know, something is conscious, and something isn't conscious, then you've proven
[01:10:11.600 --> 01:10:17.600]  to yourself at least that, and maybe you still can't convince everyone else, but you've proven
[01:10:17.600 --> 01:10:23.200]  to yourself at least, that that not everything is conscious, as soon as you have a definition
[01:10:23.200 --> 01:10:29.560]  of what of consciousness that isn't absolutely everything, then you then then you don't then
[01:10:29.560 --> 01:10:35.000]  you no longer have hand psychism, right?
[01:10:35.000 --> 01:10:36.000]  I think that
[01:10:36.000 --> 01:10:42.000]  I like the earlier spectrum, yeah, like that you were discussing about, you know, that it
[01:10:42.000 --> 01:10:47.200]  seems more like a sliding scale than a binary with consciousness, or is that just from our
[01:10:47.200 --> 01:10:51.720]  perspective, it looks that way from our current lens, you know, and that, you know, past the
[01:10:51.720 --> 01:10:56.680]  sliding scale, it's just that we just can't see, because, you know, on another dimension,
[01:10:56.680 --> 01:11:00.720]  we're just, we just can't see it, you know, we can't have them in.
[01:11:00.720 --> 01:11:05.440]  Yeah, exactly, I had that exact thought as we were, we were, I was that year listening,
[01:11:05.440 --> 01:11:10.280]  is that we're defining conscious, consciousness given our current referential frame, you know,
[01:11:10.280 --> 01:11:15.200]  we have, we have eyes, we have ears, we have taste buds, we have these senses, and we have
[01:11:15.200 --> 01:11:21.200]  the ability to, to introspectively sort of look inwards, but then you could just as easily
[01:11:21.200 --> 01:11:28.960]  have an organism above us, which has, I say above us just, you know, as a reference, above
[01:11:28.960 --> 01:11:33.840]  us with an extra sense, you know, with the ability to sense, I don't know, let's be
[01:11:33.840 --> 01:11:40.960]  ridiculous, the ability to sense sort of electromagnetic fields built in, built into that, that creature,
[01:11:40.960 --> 01:11:46.240]  to a degree where it could actually use it to sense and interpret its way around the world.
[01:11:46.240 --> 01:11:49.880]  That different set of sensory inputs is going to give it a completely different sense of
[01:11:49.880 --> 01:11:55.520]  reality, and it may look at us, and our way of defining consciousness is ridiculous, because
[01:11:55.520 --> 01:12:02.520]  how could we possibly be defining consciousness or even contemplate consciousness without,
[01:12:02.520 --> 01:12:08.000]  while we're missing this unique frame of reference, or this sense that, that this higher being
[01:12:08.000 --> 01:12:14.440]  in air quotes has, so then you're back to, well, it's just, you know, your point of reference
[01:12:14.440 --> 01:12:18.200]  you happen to be looking from, you can, you can define it as well as you can, given your
[01:12:18.200 --> 01:12:19.800]  tools.
[01:12:19.800 --> 01:12:23.520]  You know, I just, I just really, I just really wanted to take a moment to appreciate what
[01:12:23.520 --> 01:12:31.520]  Dasha and Carl are saying, that's just really beautifully said, yeah, that's just perfect,
[01:12:31.520 --> 01:12:37.480]  like, I just, I appreciate that perspective so much, I feel like I've figured it all out
[01:12:37.480 --> 01:12:44.720]  personally, and so for me to hear other people say that they haven't is just, it's wonderful,
[01:12:44.720 --> 01:12:49.320]  but I'll, I'll share it with everybody, if you want, so like, so like, I don't think
[01:12:49.320 --> 01:12:55.440]  that it's, that it's, it's my, my opinion, and feel free to disagree, I don't think
[01:12:55.440 --> 01:13:00.960]  that it's the electricity in the brain, and I don't think that, that, but what I think
[01:13:00.960 --> 01:13:06.360]  is like, so there's like the, there's this idea that consciousness, you know, we have
[01:13:06.360 --> 01:13:11.680]  models of reality, and the models are connected to other models, and you have, you know, your
[01:13:11.680 --> 01:13:16.000]  brain is, is making a model of the parts of your brain are making models of everything
[01:13:16.000 --> 01:13:21.680]  in your environment, you have, you have cortical columns that are recognizing, a cortical column
[01:13:21.680 --> 01:13:27.960]  is bringing together like the 3D model of every object of one object, and another cortical
[01:13:27.960 --> 01:13:31.920]  column is bringing together all the 3D properties of another object, and so if you pick up something
[01:13:31.920 --> 01:13:37.520]  like your phone, the cortical column is integrating the visual information, the sensory information,
[01:13:37.520 --> 01:13:41.880]  I mean, the touch information, like how the phone, how the phone is oriented relative
[01:13:41.880 --> 01:13:46.600]  to you, and so you have basically in the brain, you have all these neural columns working
[01:13:46.600 --> 01:13:50.760]  together to integrate all these different models together in this experience that you
[01:13:50.760 --> 01:13:56.000]  have, if you, if you take your, your, your room right now, or your, or your, if you're
[01:13:56.000 --> 01:14:01.320]  outside, if you take the space you're in right now and yourself, those are all, those are
[01:14:01.320 --> 01:14:05.800]  all like every, every object and every thought and every consideration, including just abstract
[01:14:05.800 --> 01:14:09.480]  ideas are just managed by different neural columns integrating different pieces of information
[01:14:09.480 --> 01:14:15.200]  that are constructing this, it's like this movie, this four-dimensional movie where, where,
[01:14:15.200 --> 01:14:19.600]  but it's like a multi-layer movie, but it's like a multi-scale has got all these different
[01:14:19.600 --> 01:14:25.440]  objects in it, and it's, it's got like, there's patterns of like, you know, what, what are
[01:14:25.440 --> 01:14:30.040]  your feelings, where are your feelings, what, you know, are you feelings peaking, they're
[01:14:30.040 --> 01:14:34.040]  all four-dimensional patterns, the smell, there's, there's four-dimensional like, what's
[01:14:34.040 --> 01:14:40.000]  the peak of a smell, what's the space of a, of a smell, every thought, every feeling,
[01:14:40.000 --> 01:14:46.560]  every emotion has these four-dimensional patterns, so they can be described basically as a temple
[01:14:46.560 --> 01:14:51.160]  and spatial patterns, and so everything that you can think of is a temple spatial pattern
[01:14:51.160 --> 01:14:54.040]  inside a temple spatial pattern, the room that you're in, the feelings that you're
[01:14:54.040 --> 01:15:00.600]  having, and so we have the, and they're all, they're all connected together, they're all
[01:15:00.600 --> 01:15:06.120]  interesting together, you can be aware of all of them to, to more or less extent, as
[01:15:06.120 --> 01:15:09.760]  you, as you're, as your focus shifts around, as you, you think about the room, you think
[01:15:09.760 --> 01:15:13.080]  about your feelings, your focus shifts, but you can be aware of the interconnection between
[01:15:13.080 --> 01:15:17.200]  all the different, the different feelings, they're all temple spatial patterns, the brain
[01:15:17.200 --> 01:15:23.160]  has, the brain specializes in temple spatial patterns, all the electrical firing is, is
[01:15:23.160 --> 01:15:27.360]  frequency patterns and spatial patterns, all over the place, and all over those places
[01:15:27.360 --> 01:15:32.200]  is exactly what we would need if we wanted to create a simulation of, of, of a consciousness
[01:15:32.200 --> 01:15:37.080]  that was temple or spatial patterns, but not only that, it's like, it's like, these are
[01:15:37.080 --> 01:15:42.360]  not, it's, it's not the electricity itself, it's not the, it's not really even the neurons
[01:15:42.360 --> 01:15:48.120]  themselves, it's, it's the, the patterns which are just information, and they're, and that's,
[01:15:48.120 --> 01:15:53.240]  that's an important point because it's, the information is basically non-physical, but
[01:15:53.240 --> 01:15:57.600]  it requires the physicality to move, to move between, but it can be like, you can have
[01:15:57.600 --> 01:16:03.680]  an idea anywhere in your brain, you can have the idea of a, of a cup could, could be represented
[01:16:03.680 --> 01:16:11.080]  by any neuro column in theory, it's not the way it's gonna work out, but, but it's, it's
[01:16:11.080 --> 01:16:14.480]  because, because the brain is like a general, it's like, it's like, imagine the brain is
[01:16:14.480 --> 01:16:18.880]  like a, it's like, it's not just like one piano, it's like having a million pianos in
[01:16:18.880 --> 01:16:23.120]  your head, and so like any of these pianos can play a tune that should be here, right?
[01:16:23.120 --> 01:16:27.960]  And so when neuro, when neuro, when medical imagers look at your brain and say, okay,
[01:16:27.960 --> 01:16:32.360]  okay, so which, so which, so where is the, the pattern that represents the sound that
[01:16:32.360 --> 01:16:33.360]  you just heard?
[01:16:33.360 --> 01:16:37.240]  And, and so they look for the pattern and, and they see, oh, there's a pattern, okay,
[01:16:37.240 --> 01:16:40.200]  let's look again, let's see if, let's see if it's, if we got it right.
[01:16:40.200 --> 01:16:44.840]  And then it, the pattern, it's not, it's a different pattern the second time, what,
[01:16:44.840 --> 01:16:45.840]  what's going on here?
[01:16:45.840 --> 01:16:49.200]  And then it's a different pattern the third time, I don't understand it, why is, why do
[01:16:49.200 --> 01:16:51.800]  we can't have the same pattern in the same place every single time?
[01:16:51.800 --> 01:16:56.520]  It's like different, it's like, you have all these different pianos, and when one, when
[01:16:56.520 --> 01:17:00.960]  one piano is, is done playing another, another piano decides to play that, play that tune.
[01:17:00.960 --> 01:17:05.240]  And so the information itself moves around, it's like virtual, it's not even connected
[01:17:05.240 --> 01:17:13.040]  to, to your, to, to one area or another's necessarily, it's, it's, it, it moves freely.
[01:17:13.040 --> 01:17:18.480]  And so what I'm saying is like, we have this like virtual information pattern, that's not
[01:17:18.480 --> 01:17:22.800]  the electricity of the brain, it's not even, it's not really even the, the brain itself,
[01:17:22.800 --> 01:17:25.600]  it's just, it can move around.
[01:17:25.600 --> 01:17:33.040]  And so we have, we have, but you don't, but, but it's basically, it's basically like, it's
[01:17:33.040 --> 01:17:37.600]  not like information, it's not like an information theory, it's not, I mean, it's not integrated
[01:17:37.600 --> 01:17:41.840]  information theory, it's not global workspace theory, it's, it's not, it's not integrated
[01:17:41.840 --> 01:17:47.360]  VAP theory, it's, it's, it's like virtual model theory, right, it's like virtual model
[01:17:47.360 --> 01:17:49.480]  theory, that's what I write that down.
[01:17:49.480 --> 01:17:58.560]  And so the whole thing is like, you have these virtual models, and, and, so, so the, the whole
[01:17:58.560 --> 01:18:04.720]  thing is like, but yeah, so it's like.
[01:18:04.720 --> 01:18:10.480]  I love, I love that your piano analogy and your virtual models just made you sigh.
[01:18:10.480 --> 01:18:18.920]  I love, I love how, I just, you're becoming the whirlpool yourself, you know, right now
[01:18:18.920 --> 01:18:21.040]  and I, it's, it's wonderful.
[01:18:21.040 --> 01:18:28.320]  I have a question that I would love to ask about thoughts on this, I work in, in virtual
[01:18:28.320 --> 01:18:34.280]  reality, I've been a performer in, in a VR experience for the last year and a half,
[01:18:34.280 --> 01:18:43.360]  and then also seen some new things about being able to, with like, Neuralink and Neuro, like
[01:18:43.360 --> 01:18:51.400]  being able to control games and, you know, computers or VR with your mind in the future,
[01:18:51.400 --> 01:18:59.560]  and has been already kind of proven with a small amount of hardware out there.
[01:18:59.560 --> 01:19:07.120]  So that is a topic that I would love to know if there has been more going on and anything
[01:19:07.120 --> 01:19:12.720]  available right now with a user interface that's like potentially a performer could
[01:19:12.720 --> 01:19:21.520]  have a different interface than an audience, and if there is anything, if anyone knows
[01:19:21.520 --> 01:19:24.560]  any more information about that right now.
[01:19:24.560 --> 01:19:26.560]  Did you say Neuralink?
[01:19:26.560 --> 01:19:33.840]  Well, I know Elon Musk is doing his own, like, thing with, like, controlling a phone with
[01:19:33.840 --> 01:19:41.840]  your mind, but there is a company called, like, Lucidlink that was doing, like, Neuro
[01:19:41.840 --> 01:19:46.680]  Gaming of, like, controlling VR with your mind by, there's like a device that you just
[01:19:46.680 --> 01:19:54.240]  put on the outside of the headset on the back of your head and you essentially are just,
[01:19:54.240 --> 01:19:59.360]  you know, there are areas in your brain that are firing off when you think of with your
[01:19:59.360 --> 01:20:05.400]  right hand picking up that object, and you can essentially do that already with this
[01:20:05.400 --> 01:20:13.480]  device that is just on the very outside of your head on the back.
[01:20:13.480 --> 01:20:20.040]  It's not, you know, fine-tuned yet, but I watched a demo of it on a YouTube link that
[01:20:20.040 --> 01:20:26.560]  was pretty, kind of blew my mind, and as a performer in virtual reality, as a live performer
[01:20:26.560 --> 01:20:31.160]  in virtual reality, I was like, wow, this could, like, really change the game if I didn't
[01:20:31.160 --> 01:20:38.840]  have to use controllers and just, it just seems like a whole, another level without
[01:20:38.840 --> 01:20:48.680]  having a chip implanted into my brain, but yeah, just since this was a room of people
[01:20:48.680 --> 01:20:54.520]  that know more about all of this, I was wondering whether anyone knows any further developments
[01:20:54.520 --> 01:21:03.320]  in terms of non-invasive hardware that is out there that is picking up on brainwaves
[01:21:03.320 --> 01:21:09.480]  in order to use it for storytelling or entertainment purposes.
[01:21:09.480 --> 01:21:16.160]  I in fact do have, so I was part of a room the other night and we reviewed a paper which
[01:21:16.160 --> 01:21:24.520]  I think you would totally like and it was called, oh my gosh, let me look at my LinkedIn,
[01:21:24.520 --> 01:21:33.960]  but it was called, it must be late at night because I actually started talking before
[01:21:33.960 --> 01:21:41.720]  being ready to talk, let me see, it was just a really cool paper, I'm almost there, everyone
[01:21:41.720 --> 01:21:47.720]  liked my post yesterday, okay, so it was called, and you might want to look this up, it's
[01:21:47.720 --> 01:22:01.960]  called hybrid EEG-FNERS based eight command decoding for BCI application to quadcopter
[01:22:01.960 --> 01:22:10.680]  control, so this was like really, really cool if you're into the topic of controlling things
[01:22:10.680 --> 01:22:21.080]  with BCI, by combining two different modalities, EEG and FNERS together, they were able to
[01:22:21.080 --> 01:22:29.080]  have more control over the quadcopter than they could with just one modality, because
[01:22:29.080 --> 01:22:39.040]  with one modality, like for example with just EEG, you're sort of like the more buttons
[01:22:39.040 --> 01:22:45.200]  you try to enable the worse your accuracy gets, but with two modalities you can have
[01:22:45.200 --> 01:22:50.200]  more buttons without losing your accuracy, and so this is a really cool paper to check
[01:22:50.200 --> 01:23:03.440]  out that I think you would like, does anyone else want to jump in, I have one more thought
[01:23:03.440 --> 01:23:19.840]  but I can wait, go for it, go for it, it was, I know I had, I had, I was, I wanted to connect
[01:23:19.840 --> 01:23:23.440]  something I was saying before to something other people were talking about before, but
[01:23:23.440 --> 01:23:34.440]  I can wait, oh well I wanted to refer back to something Carl said in response to you,
[01:23:34.440 --> 01:23:38.040]  but Dasha, do you feel like you got an answer to your question, oh for sure, yeah, I also
[01:23:38.040 --> 01:23:42.920]  would love to go back to what Carl was talking about and also Hyperspace, the book was what
[01:23:42.920 --> 01:23:47.760]  kind of got me thinking about like another dimension being just impossible to see, so
[01:23:47.760 --> 01:23:56.200]  that's also a book that I enjoyed that stemmed back to Carl's thought, can I say, oh sorry
[01:23:56.200 --> 01:24:07.560]  go ahead, go ahead, go ahead, sorry thanks my name's Chris and I appreciated Carl talking
[01:24:07.560 --> 01:24:12.800]  about perspective, mentioning perspective and I was having a similar question and like Carl
[01:24:12.800 --> 01:24:21.280]  I am a little out of my league here, but as an artist I've had a long time interest in
[01:24:21.280 --> 01:24:34.880]  sort of reconciling actually scientific views and primitive views and magical views, I always
[01:24:34.880 --> 01:24:40.280]  kind of feel like there's got to be, there's got to be some sort of reconciling description,
[01:24:40.280 --> 01:24:49.080]  so when Carl mentioned perspective it, I was going to ask a similar question, rather than
[01:24:49.080 --> 01:24:55.600]  aren't these all a matter of perspective, it's sort of, I also feel like it's in a way it's
[01:24:55.600 --> 01:25:03.720]  a matter of description and sometimes you know like there, for example, there's sort
[01:25:03.720 --> 01:25:15.600]  of a, might be useful to make a distinction between sensitivity, awareness, intelligence
[01:25:15.600 --> 01:25:22.360]  and consciousness and information for that matter and because I, and it's easy to sort
[01:25:22.360 --> 01:25:31.120]  of conflate different scales when you're throwing words around like this and I feel like, well
[01:25:31.120 --> 01:25:37.760]  I wonder, Micah, if you've read the work of Ian McGillchrist who's recently been writing
[01:25:37.760 --> 01:25:46.160]  about the split brain and describing how from his perspective consciousness rather than
[01:25:46.160 --> 01:25:54.720]  a state that you either have or don't have is more like a process and I think this is
[01:25:54.720 --> 01:25:59.240]  similar to what you were saying, at least I saw a similarity Micah between this idea
[01:25:59.240 --> 01:26:06.200]  and what you were talking about, about this kind of the freedom of these information pathways,
[01:26:06.200 --> 01:26:14.920]  these activity pathways and he describes in great detail sort of the relationship between
[01:26:14.920 --> 01:26:22.400]  the left and the right brain in what I take to be our most contemporary understanding
[01:26:22.400 --> 01:26:31.200]  of it and again I'm really unqualified to know that for certain but in this relationship
[01:26:31.200 --> 01:26:40.600]  between sort of inner self and outer world, inner sensitivity and outer sensitivity, this
[01:26:40.600 --> 01:26:52.320]  oscillation between relationships to the world, to everything, to reality, there is a, there
[01:26:52.320 --> 01:27:00.000]  is a necessary process there in which paradoxes are being reconciled, you know, impossible
[01:27:00.000 --> 01:27:06.600]  choices are being made, sensitivities are being understood and it's really in that
[01:27:06.600 --> 01:27:16.240]  dynamic process that meaning is sort of generated and if you describe it as a process then you
[01:27:16.240 --> 01:27:24.720]  can also map it onto a spectrum or more like a space, like a color space, and so you can
[01:27:24.720 --> 01:27:32.640]  then talk about greater or lesser degrees and so I suppose on some level you could go
[01:27:32.640 --> 01:27:41.080]  to the ultra-subtle, you know, like a rock as like, as close to zero as you can get and
[01:27:41.080 --> 01:27:48.000]  exist as a coherent, you know, mass and then you could probably go in the other direction
[01:27:48.000 --> 01:27:53.400]  up to something that we don't really understand and somewhere in the middle is our embodied
[01:27:53.400 --> 01:28:01.760]  norm, that's really all I can offer is just kind of a way of describing it that seems
[01:28:01.760 --> 01:28:10.000]  to me to, you can map a lot of different descriptions onto that, it seems to me and
[01:28:10.000 --> 01:28:16.200]  when I read Ian McGillchrist it was my first glimpse of feeling like, ah, this actually
[01:28:16.200 --> 01:28:25.640]  might be the sort of theory of everything description that all these different perspectives
[01:28:25.640 --> 01:28:35.280]  can find some sort of description, so thanks for the chance, this was a fascinating conversation,
[01:28:35.280 --> 01:28:39.320]  I really appreciate it, my thanks.
[01:28:39.320 --> 01:28:42.400]  Does anyone want to jump in after that or say something?
[01:28:42.400 --> 01:28:44.720]  I have more thoughts but I'll wait for other people to go.
[01:28:44.720 --> 01:28:51.120]  I mean I had a thought about the fact that you're saying that the rock is like the least,
[01:28:51.120 --> 01:28:56.640]  you know, in your perspective the least, if you're looking at a spectrum would be kind
[01:28:56.640 --> 01:29:02.440]  of constituting the least, having the least consciousness, if I again would bring it back
[01:29:02.440 --> 01:29:08.240]  to perspective like Carl was saying that the rock is actually just part of the entire
[01:29:08.240 --> 01:29:16.320]  earth which you could argue has more consciousness with how the earth and the rocks and the lava
[01:29:16.320 --> 01:29:24.320]  and all of the whole earth itself, earth as a matter, has a lot more changing and adapting
[01:29:24.320 --> 01:29:34.920]  to what is going on in the universe, so again I feel like bringing it back to like frame
[01:29:34.920 --> 01:29:44.240]  or lens like Carl was saying is a very valuable thing to kind of keep circling back to.
[01:29:44.240 --> 01:29:52.880]  I like to connect this actually to something that was mentioned earlier by Ben who left
[01:29:52.880 --> 01:30:02.920]  the room but there is this kind of also problem in cosmology related to fine tuning problem
[01:30:02.920 --> 01:30:09.880]  and so on and what I can see is that if you take like what I like to call an interface
[01:30:09.880 --> 01:30:19.640]  theory of everything, so we can actually better start understanding how the limited ability
[01:30:19.640 --> 01:30:31.160]  for us to integrate the experience allows us not to fully understand what consciousness
[01:30:31.160 --> 01:30:39.320]  in the cosmos actually means and that there is a very easy way to show that to people
[01:30:39.320 --> 01:30:45.640]  and that's by Joe.
[01:30:45.640 --> 01:30:51.520]  I can't hear you, I don't know if other people can hear you, you cut out right when you were
[01:30:51.520 --> 01:31:05.760]  going to tell us the essence of everything, okay try again, so what I see as the interface
[01:31:05.760 --> 01:31:12.240]  of everything, it's the idea that we are butch.
[01:31:12.240 --> 01:31:13.240]  The answer is 42.
[01:31:13.240 --> 01:31:14.240]  The universe doesn't want us to know.
[01:31:14.240 --> 01:31:23.240]  There's a conspiracy going on here, but it doesn't want Bixxel to impale the knowledge.
[01:31:23.240 --> 01:31:24.240]  Really?
[01:31:24.240 --> 01:31:28.280]  Bixxel it won't let you, the universe will not let you tell us, you figured it out I
[01:31:28.280 --> 01:31:29.280]  think.
[01:31:29.280 --> 01:31:31.280]  Yeah absolutely, you're on the right track clearly.
[01:31:31.280 --> 01:31:34.080]  Yeah I know this is happening all the time.
[01:31:34.080 --> 01:31:38.520]  You're actually waiting for three, but three is a magical number so you have to try at
[01:31:38.520 --> 01:31:40.520]  least three times.
[01:31:40.520 --> 01:31:47.400]  For me four is a magical number, and I actually discovered a bit of a mathematics that shows
[01:31:47.400 --> 01:31:51.320]  that as well, so I'm trying to figure out why.
[01:31:51.320 --> 01:31:55.640]  But at least three or four times, go ahead.
[01:31:55.640 --> 01:31:59.320]  Yeah, I like five.
[01:31:59.320 --> 01:32:04.560]  Well, I'm sticking to four.
[01:32:04.560 --> 01:32:12.800]  So what I'm seeing is that the fact that we have this way to integrate information and
[01:32:12.800 --> 01:32:21.880]  be aware of the experience we are having, it always relates to the way we can do that.
[01:32:21.880 --> 01:32:27.640]  Understanding that there is more consciousness in the cosmos that we simply cannot fully
[01:32:27.640 --> 01:32:34.480]  understand because of the way we are perceiving it all and integrating it all, you could actually
[01:32:34.480 --> 01:32:40.680]  already show it pretty easy by just looking how different people are in respect to empathy.
[01:32:40.680 --> 01:32:54.400]  So as we understand the world, but I see other people who have for example a lot of empathy
[01:32:54.400 --> 01:32:59.320]  related to the social cohesion, and I can't fully understand them, and they can't fully
[01:32:59.320 --> 01:33:08.240]  understand me, so already there's a disconnection to the things we are trying to integrate.
[01:33:08.240 --> 01:33:14.320]  One of the things I really find fascinating is that some blind people have learned to
[01:33:14.320 --> 01:33:21.360]  use echolocation to visualize the world around them.
[01:33:21.360 --> 01:33:29.440]  So they move more into the experience that also bats have with the world.
[01:33:29.440 --> 01:33:31.320]  So we don't even need to go that far.
[01:33:31.320 --> 01:33:37.280]  We can just look at each other as mirrors and understand how each of us is limited in
[01:33:37.280 --> 01:33:39.160]  the way we perceive the world.
[01:33:39.160 --> 01:33:45.760]  And if we try to extrapolate that kind of limitless already between us to limitless between me
[01:33:45.760 --> 01:33:49.360]  and a rock, that's crazy.
[01:33:49.360 --> 01:33:53.520]  So I had this talk with Don Hoffman about that, because he's working pretty well on
[01:33:53.520 --> 01:34:01.400]  that interface theory, and so between me and a cat for example, it's still doable.
[01:34:01.400 --> 01:34:07.200]  I can still have like a coordination going on with me and the cat.
[01:34:07.200 --> 01:34:13.160]  But me and an ant, that's already totally impossible, and the ant is living clearly
[01:34:13.160 --> 01:34:15.760]  some kind of conscious thing.
[01:34:15.760 --> 01:34:23.680]  So if you try to use that line of thought about consciousness, you really see how limited
[01:34:23.680 --> 01:34:27.680]  we are between, by the frame of reference we have.
[01:34:27.680 --> 01:34:29.680]  I mixed it all speaking.
[01:34:29.680 --> 01:34:32.400]  Can I quickly, oh, no, sorry.
[01:34:32.400 --> 01:34:35.000]  Go ahead, Carl, go ahead, Carl.
[01:34:35.000 --> 01:34:36.000]  Thank you.
[01:34:36.000 --> 01:34:37.000]  Sorry.
[01:34:37.000 --> 01:34:42.800]  I had a brain fart earlier while Mike was speaking, and mixed it, that's the perfect
[01:34:42.800 --> 01:34:52.200]  segue into what popped into my head, is that if, Mike, it is, Mike, I might pronounce
[01:34:52.200 --> 01:34:53.200]  that correctly.
[01:34:53.200 --> 01:34:55.200]  That's how I pronounce it, yeah.
[01:34:55.200 --> 01:34:58.520]  That's right, that's perfect.
[01:34:58.520 --> 01:35:06.400]  When you were talking about the consciousness sort of recognizing the system as in perceiving
[01:35:06.400 --> 01:35:12.240]  the system, it's not electricity, it's not this, it's not that, it's not mechanical.
[01:35:12.240 --> 01:35:18.200]  The ability to take the information and pass it, I wonder if you take that and go further
[01:35:18.200 --> 01:35:25.840]  with it, if you get to a point in the development of a civilization where you could just classify
[01:35:25.840 --> 01:35:35.000]  consciousness as the accuracy to a particular system, as in, you know, humans may not even
[01:35:35.000 --> 01:35:40.840]  register on the scale because although we're conscious, we don't have enough accuracy in
[01:35:40.840 --> 01:35:46.840]  the system, so we can perceive an environment around us, we just can't do it accurately
[01:35:46.840 --> 01:35:47.840]  enough.
[01:35:47.840 --> 01:35:52.280]  So you could literally, I wonder if, you know, it's an inevitability that you just have a
[01:35:52.280 --> 01:35:59.840]  simple scoring system, you know, to what degree can that creature, can that being, can that
[01:35:59.840 --> 01:36:09.200]  being render or describe or process this ex-environment or the environment around them to what accuracy.
[01:36:09.200 --> 01:36:14.760]  But then as Mixol was just talking there, because you brought up empathy, which hadn't
[01:36:14.760 --> 01:36:20.240]  come into my mind, I had the complete opposite thought, which is, what do you define it as
[01:36:20.240 --> 01:36:25.440]  actually the ability to completely ignore your current surroundings and instead take
[01:36:25.440 --> 01:36:32.360]  yourself out and move your ability, move your, have the ability to move your referential
[01:36:32.360 --> 01:36:38.520]  frame into, like when we're talking about empathy, into somebody else's, and that's
[01:36:38.520 --> 01:36:42.520]  a very different dividing line because then you're talking about the ability to, to take
[01:36:42.520 --> 01:36:46.600]  yourself, you know, if you say that person over there, to take yourself out and put your
[01:36:46.600 --> 01:36:51.680]  existence or your referential frame into, into theirs, which, you know, which is akin
[01:36:51.680 --> 01:36:55.760]  to imagination and, and extrapolation.
[01:36:55.760 --> 01:37:03.200]  So yeah, two completely opposite ends, you know, as far apart as they could be, really.
[01:37:03.200 --> 01:37:04.200]  Yeah.
[01:37:04.200 --> 01:37:13.080]  Well, Carl, for me, those aren't opposite ends, it's, it is, it's like the two sides
[01:37:13.080 --> 01:37:14.960]  of the same coin.
[01:37:14.960 --> 01:37:18.640]  So the, it's, it's like you need shadow to see any shape.
[01:37:18.640 --> 01:37:25.840]  So it's, it's not, it's not a contradiction, it's, it's, it's a different in perspective.
[01:37:25.840 --> 01:37:30.240]  And you, you really remind me a lot about the talk I did had with, with, with Don Hoffman
[01:37:30.240 --> 01:37:36.960]  about this, where the way he looks at it is also like we are filtering a lot of all the
[01:37:36.960 --> 01:37:42.120]  complexity out to actually make sense of the things we love.
[01:37:42.120 --> 01:37:47.720]  So, so it is a combination of, of, because if you have all the information at all the
[01:37:47.720 --> 01:37:53.480]  times, you basically get into this entropic state where it's like everything is gray,
[01:37:53.480 --> 01:37:54.480]  right?
[01:37:54.480 --> 01:38:02.080]  You, there wouldn't be any taste if you start mixing super the main dish and the dessert
[01:38:02.080 --> 01:38:06.520]  all together in a blender and then try to swallow that, that's going to be terrible.
[01:38:06.520 --> 01:38:12.520]  So you need these differentiations and separations to actually have any kind of game going on
[01:38:12.520 --> 01:38:13.520]  anyway.
[01:38:13.520 --> 01:38:18.920]  And so, but that really resonates a lot with, with, with, I'm going to, I'm going to just
[01:38:18.920 --> 01:38:28.720]  now send a tweet to a YouTube channel I have called deep digital friends and both the discussion
[01:38:28.720 --> 01:38:35.920]  I recently had with Alexander Bart about the, the, the tropopoasis and the, the, the talk
[01:38:35.920 --> 01:38:41.200]  I had with Don Hoffman really resonates a lot with the, the talks we are having here.
[01:38:41.200 --> 01:38:44.440]  So I'm mixing, I'm not speaking.
[01:38:44.440 --> 01:38:49.560]  So if folks go, if folks click on Mixl's profile and click on Twitter and they look at the
[01:38:49.560 --> 01:38:54.760]  most recent tweet, they can see what he's, the tweet that he's tweeting, right?
[01:38:54.760 --> 01:38:55.760]  Thanks.
[01:38:55.760 --> 01:38:56.760]  Yes.
[01:38:56.760 --> 01:38:57.760]  Okay.
[01:38:57.760 --> 01:39:03.800]  I, I, I realized after doing that a few times, it said that sometimes, you know, when you
[01:39:03.800 --> 01:39:06.520]  say check my Twitter, they don't exactly know they're supposed to click on your profile
[01:39:06.520 --> 01:39:07.520]  first.
[01:39:07.520 --> 01:39:13.680]  But yeah, click on the profile and then, then the Twitter.
[01:39:13.680 --> 01:39:16.320]  I, I, now I can share my idea.
[01:39:16.320 --> 01:39:24.320]  So, so, so, so the, the, the practice to the idea is that if we can define what consciousness
[01:39:24.320 --> 01:39:32.520]  is and then, then, then we no longer have to say that, then, then, then, then no longer
[01:39:32.520 --> 01:39:36.280]  do we have, what is it called, panpsychism?
[01:39:36.280 --> 01:39:39.280]  We don't have, we don't have that anymore because, because, because that, because consciousness
[01:39:39.280 --> 01:39:42.120]  has to be everything, right?
[01:39:42.120 --> 01:39:46.000]  But you know, if we can say, if we can say what it is now, now the problem is with Dasha
[01:39:46.000 --> 01:39:50.360]  and, and was Dasha and Carl or, or did names move around?
[01:39:50.360 --> 01:39:56.600]  But so, so some people were saying that, that I just remembered it was the person in the
[01:39:56.600 --> 01:40:01.480]  first column in the second, in the, like the second row and the person in the third column
[01:40:01.480 --> 01:40:06.600]  in the second row, sometimes names move around.
[01:40:06.600 --> 01:40:12.200]  So the, so, what was said was, was that, well, maybe we don't understand what consciousness
[01:40:12.200 --> 01:40:17.080]  was and, and so then I came up with another theory of consciousness, which is my theory,
[01:40:17.080 --> 01:40:18.680]  there's a bunch of theories, right?
[01:40:18.680 --> 01:40:27.040]  But what, so, what I'm saying, so, so my argument here is that, so, that, what's that, so, what,
[01:40:27.040 --> 01:40:31.120]  you know, I actually wrote it down, so I wouldn't have to be like stumbling around.
[01:40:31.120 --> 01:40:36.760]  So, yeah, so Carl, was it Carl, it was saying render, describe, and process, and to what
[01:40:36.760 --> 01:40:37.760]  accuracy?
[01:40:37.760 --> 01:40:38.760]  That was really great.
[01:40:38.760 --> 01:40:41.640]  I wrote, like, I wrote that down too, that was really great.
[01:40:41.640 --> 01:40:45.640]  No, no, I was thinking something about, is it, is it because brains are doing a very
[01:40:45.640 --> 01:40:46.640]  special process?
[01:40:46.640 --> 01:40:50.320]  Yeah, that's, that's, that, that was, that was the thing that I wanted to respond to.
[01:40:50.320 --> 01:40:52.800]  Yeah, so brains are doing something very special.
[01:40:52.800 --> 01:40:58.520]  So if you look, if you compare a rock, you say, so, the, the, so the argument from, for
[01:40:58.520 --> 01:41:03.440]  panpsychism is that there's a spectrum from, like, of consciousness, from something as
[01:41:03.440 --> 01:41:08.840]  simple as a rock, to something as, like, a human being, to something like a galaxy,
[01:41:08.840 --> 01:41:12.600]  there's a spectrum of consciousness, and they, there's different ranges, right, that, that's
[01:41:12.600 --> 01:41:18.960]  sort of like the, one of the, one of the arguments, what, so, but those arguments don't include
[01:41:18.960 --> 01:41:19.960]  computers.
[01:41:19.960 --> 01:41:23.800]  No one says, okay, there's a spectrum of consciousness from, from a rock, to a human
[01:41:23.800 --> 01:41:26.040]  being, to a computer, and then to a galaxy.
[01:41:26.040 --> 01:41:27.040]  They never mentioned computers.
[01:41:27.040 --> 01:41:32.200]  And the thing is, like, a computer, like, the complexity of it can be vastly greater
[01:41:32.200 --> 01:41:36.440]  than the complexity of an insect, but the insect can do things a computer can't do.
[01:41:36.440 --> 01:41:41.440]  The computer can be vastly more complex than, than a, than a, than a mouse, in terms of what
[01:41:41.440 --> 01:41:44.280]  it can do, but, versus the mouse, but it's not conscious.
[01:41:44.280 --> 01:41:48.040]  You know, we know it says, the computer's conscious, but why not?
[01:41:48.040 --> 01:41:52.840]  Because there's something very specific about what, what brains are doing that, that computers
[01:41:52.840 --> 01:41:53.840]  are not doing yet.
[01:41:53.840 --> 01:42:00.360]  So, the computer can, can render, it can describe, it can process to what accuracy, though.
[01:42:00.360 --> 01:42:04.800]  I mean, greater accuracy to the human beings, right, but, so why isn't the computer conscious
[01:42:04.800 --> 01:42:05.800]  yet?
[01:42:05.800 --> 01:42:12.040]  And, and that's where I think this, this, you know, this idea that we, we are, that
[01:42:12.040 --> 01:42:17.120]  our, our brains are doing something very special that rocks are not doing, is that we're creating
[01:42:17.120 --> 01:42:23.800]  models of reality, but we're, but we're, we're, we're creating models of reality that
[01:42:23.800 --> 01:42:30.920]  are, are able to, because of the, the, the, because of how the brain works, they're able
[01:42:30.920 --> 01:42:36.440]  to, to slide around and move around and connect with other models of reality and all the models
[01:42:36.440 --> 01:42:41.080]  of reality are able to interact with each other so you, so you can have, you can have
[01:42:41.080 --> 01:42:44.560]  a, a cup, right, and maybe that's represented by part of your brain.
[01:42:44.560 --> 01:42:48.800]  And then when you throw it from your, from your right hand to your left hand, guess what
[01:42:48.800 --> 01:42:49.800]  has to change?
[01:42:49.800 --> 01:42:56.360]  See, see, when it was in your right hand, you had all of your, you had, you had a neural
[01:42:56.360 --> 01:43:00.920]  column that was representing your hand and the cup together, the right, but it was representing
[01:43:00.920 --> 01:43:02.120]  the right hand and the cup together.
[01:43:02.120 --> 01:43:05.960]  Well, the right hand is not located where the left hand is located in your brain.
[01:43:05.960 --> 01:43:09.400]  So you toss the cup over to your left hand.
[01:43:09.400 --> 01:43:13.120]  Now you have to have a neural column that's, that's, that's talking to your left hand that's
[01:43:13.120 --> 01:43:15.960]  integrated in the bottle that's in your left hand, even though it is the same bottle that
[01:43:15.960 --> 01:43:17.600]  was in your right hand.
[01:43:17.600 --> 01:43:19.880]  So the brain activity has to change.
[01:43:19.880 --> 01:43:22.560]  It's like, it's like, it has to move around.
[01:43:22.560 --> 01:43:27.080]  So as stuff moves around in your environment, it has to move around in your brain.
[01:43:27.080 --> 01:43:34.600]  And, and there's, a rock is not doing that, but your mind is, minds are doing that.
[01:43:34.600 --> 01:43:39.200]  And so that's a special thing that minds are doing, that rocks are not doing, that computers
[01:43:39.200 --> 01:43:45.440]  are not doing, and, and, and so, and so if you know that, then you can, then you know
[01:43:45.440 --> 01:43:50.720]  to yourself, you can prove to yourself that panpsychism is, is not it.
[01:43:50.720 --> 01:43:51.720]  That's all.
[01:43:51.720 --> 01:43:52.720]  That's it.
[01:43:52.720 --> 01:43:53.720]  I'm mic'ing.
[01:43:53.720 --> 01:43:54.720]  That's it.
[01:43:54.720 --> 01:44:00.960]  I wonder if, because you were just talking about computers, they're having the accuracy
[01:44:00.960 --> 01:44:05.560]  and the perception, but still it's, it would be difficult to argue that there's consciousness
[01:44:05.560 --> 01:44:06.560]  there.
[01:44:06.560 --> 01:44:12.400]  This is getting a little bit into AI and the, and the techniques that a lot of AI companies
[01:44:12.400 --> 01:44:16.080]  are working on, but I wonder if it's just because there's too much accuracy, there's
[01:44:16.080 --> 01:44:24.200]  not enough chaos, there's not enough randomization in the system for it to, for it to generate
[01:44:24.200 --> 01:44:29.280]  something which we would cause chaos, it's too linear, it's too ordered.
[01:44:29.280 --> 01:44:31.600]  Yeah, maybe that's it.
[01:44:31.600 --> 01:44:36.240]  And that leads on, like I said, that leads on to some places now discovering for AI learning
[01:44:36.240 --> 01:44:45.000]  that it's much better to give a very loose set of, a loose set of orders or limitations
[01:44:45.000 --> 01:44:50.320]  and just let the system work itself out in a chaotic sense.
[01:44:50.320 --> 01:45:01.720]  Well, that's kind of funny that there is a, a session by, wait a minute, it's called
[01:45:01.720 --> 01:45:09.520]  criticality and complexity and it's exactly describing this difference between, on the
[01:45:09.520 --> 01:45:16.480]  one hand you have a lot of entropy, on the other hand you have no entropy, so, and, but
[01:45:16.480 --> 01:45:18.480]  complexity is in between it.
[01:45:18.480 --> 01:45:24.080]  So I'll, I'll try to give the link with my Twitter, I should just give the previous link
[01:45:24.080 --> 01:45:30.360]  with the Twitter, where you actually go into the mathematics of cosmology on how you can
[01:45:30.360 --> 01:45:37.720]  understand the emerging of complexity as the interface between low entropy and high entropy.
[01:45:37.720 --> 01:45:43.280]  I'll pinpoint the exact moment, you start talking about that and showing the, the, the
[01:45:43.280 --> 01:45:45.560]  science, the, the mathematics of it.
[01:45:45.560 --> 01:45:50.680]  So check my tweets in a few seconds, via my profile.
[01:45:50.680 --> 01:45:55.160]  I do like the theory that there just hasn't been enough allowed to the computer, like
[01:45:55.160 --> 01:45:59.600]  we haven't feted the chaos of the, of the universe enough.
[01:45:59.600 --> 01:46:08.360]  Isn't there some something that happened with two computers that they were, they were allowed
[01:46:08.360 --> 01:46:13.120]  to kind of talk back and forth and make their own language and they started doing something
[01:46:13.120 --> 01:46:17.360]  so outlandish that they had to like shut the whole thing down, because it was like getting
[01:46:17.360 --> 01:46:18.360]  out of control.
[01:46:18.360 --> 01:46:23.840]  Oh man, I wish I could remember what it was, but it was, it was some sort of experiment
[01:46:23.840 --> 01:46:28.480]  with, I don't know whether it was like a translation system or just two computers were, that were
[01:46:28.480 --> 01:46:33.080]  allowed to communicate back and forth and they started making up their own language
[01:46:33.080 --> 01:46:38.400]  that became so separate from what the organizers of the research were doing that they got nervous
[01:46:38.400 --> 01:46:45.920]  and just shut the whole thing down, but that's what it's reminding me of.
[01:46:45.920 --> 01:46:53.160]  Anyone want to respond to that?
[01:46:53.160 --> 01:46:57.640]  I think Dasha was just describing when my Alexa accidentally picks up my Siri and they
[01:46:57.640 --> 01:47:00.520]  can't repeat back what you just said.
[01:47:00.520 --> 01:47:06.720]  No, this was something that was a, that was a study that they were doing that then they
[01:47:06.720 --> 01:47:10.520]  were, you know, just getting, I guess, nervous that some, they were getting too close to
[01:47:10.520 --> 01:47:15.360]  a singularity and just, just shut it down, but.
[01:47:15.360 --> 01:47:24.360]  I think I remember that story and I think these two computers were started to sort of
[01:47:24.360 --> 01:47:27.840]  just would appear to be malevolent language of some kind.
[01:47:27.840 --> 01:47:33.560]  Yes, yes, that's, yes, that's what I'm thinking of this exact story, yes.
[01:47:33.560 --> 01:47:38.480]  I don't remember the details either, but it was quite remarkable.
[01:47:38.480 --> 01:47:44.680]  But I guess while I'm on, if it's okay, I'll just, I just want to, Micah, I really, I
[01:47:44.680 --> 01:47:46.280]  really enjoy listening to you.
[01:47:46.280 --> 01:47:55.760]  I mean, I, and I, I just feel like, I still feel like there's some sort of language.
[01:47:55.760 --> 01:48:04.200]  There's some, the language is sort of a hall of mirrors and you're, you, I understand,
[01:48:04.200 --> 01:48:09.720]  if I understand what you're saying that, you know, that if you understand this mechanism,
[01:48:09.720 --> 01:48:15.360]  there's no more panpsychism and I just keep thinking, well, it just depends how you want
[01:48:15.360 --> 01:48:23.600]  to describe it really, like I'm not so sure why you can't talk about, you can't, in any
[01:48:23.600 --> 01:48:27.120]  given scenario, can't you, can't you talk about extreme degrees?
[01:48:27.120 --> 01:48:34.840]  I suppose I would agree that there is a point of absurdity where placing something on a,
[01:48:34.840 --> 01:48:41.920]  on a graph is just, is just meaningless because it's in a whole other, but the, but it still,
[01:48:41.920 --> 01:48:49.360]  it still can be, I don't know, I think there's a poetic, there's a poetic value, I find
[01:48:49.360 --> 01:48:55.760]  a poetic value, you know, but I know that's not, you guys are, this is not a poet's room,
[01:48:55.760 --> 01:48:59.360]  so I'm just going to listen, thank you, I really appreciate this.
[01:48:59.360 --> 01:49:00.360]  Oh yes it is.
[01:49:00.360 --> 01:49:04.520]  Yeah, we're just, we're just joking around, I'm just happy at all, but I, I, you can,
[01:49:04.520 --> 01:49:09.480]  you know, you can, you can tell you, if you want to share a poem, you can, I, I, I think,
[01:49:09.480 --> 01:49:19.160]  I don't know who, I don't know who's in charge, maybe Mixl's in charge, but I, I would like
[01:49:19.160 --> 01:49:34.600]  to respond, if I can, and I will, I'll go, okay, okay, so I, so the, so I like the idea
[01:49:34.600 --> 01:49:41.120]  that there's a spectrum between the smallest brain is possible and the largest brain is
[01:49:41.120 --> 01:49:51.200]  possible, that between an insect's brain and, or between just like a basic nervous system
[01:49:51.200 --> 01:49:58.800]  or basic, or basic brainstem, you know, all the way up to the, the really interesting
[01:49:58.800 --> 01:50:04.920]  complexities of the human, human mind, and so I think there's, there's a spectrum there
[01:50:04.920 --> 01:50:14.920]  with, with hardware that matches, that is capable of running a certain kind of process,
[01:50:14.920 --> 01:50:20.200]  and, and, and that's the key, there, there's hardware that's running a certain kind of
[01:50:20.200 --> 01:50:24.920]  process, and that would be like most, most of the brains, and so there, there would be
[01:50:24.920 --> 01:50:33.760]  a spectrum of consciousness from conscious to unconscious, and it would, it would, and,
[01:50:33.760 --> 01:50:38.240]  and I don't know exactly how that spectrum works, but, but, but I imagine the small of
[01:50:38.240 --> 01:50:46.680]  the brain, the, the, I don't want to, I don't want to even finish that thought, but, but
[01:50:46.680 --> 01:50:50.680]  I just, like I imagine there's some sort of spectrum there between brains, but, but not,
[01:50:50.680 --> 01:50:55.000]  but a rock wouldn't be included, because a rock has a different process, that's, that's
[01:50:55.000 --> 01:50:56.000]  what I'm thinking.
[01:50:56.000 --> 01:51:05.160]  I hear you, as the son of a librarian, I appreciate your categorical neatness, I totally hear you
[01:51:05.160 --> 01:51:07.160]  loud and clear.
[01:51:07.160 --> 01:51:09.160]  Thanks.
[01:51:09.160 --> 01:51:24.880]  I wonder if maybe proper, in inverted commas, consciousness and awareness will be achieved
[01:51:24.880 --> 01:51:31.920]  once we actually merge and meld in some form or another with computers, in some sort of
[01:51:31.920 --> 01:51:39.400]  android or cyborg instance, because then you have the accuracy to potentially perceive,
[01:51:39.400 --> 01:51:44.840]  maybe, you know, if you're talking about hardware, adding hardware to the human body,
[01:51:44.840 --> 01:51:52.040]  maybe potentially perceiving infinitely, but then the chaos of the human mind underneath
[01:51:52.040 --> 01:51:55.960]  and the ability to dream and imagine and be empathic.
[01:51:55.960 --> 01:51:59.920]  So I've started to, I've started to vote in my life to doing three things.
[01:51:59.920 --> 01:52:03.920]  One is to solving neural lace or nerve gear, basically the ultimate brain-computer interface
[01:52:03.920 --> 01:52:08.640]  that plugs you into the matrix or allows you to experience AR and VR just via direct brain
[01:52:08.640 --> 01:52:11.200]  stimulation and closed loop therapy.
[01:52:11.200 --> 01:52:15.760]  The second thing I am planning to do is to create artificial cortex, so if you are missing
[01:52:15.760 --> 01:52:20.640]  part of your brain, then you could just replace it with artificial cortex, or you could extend
[01:52:20.640 --> 01:52:25.600]  your brain into a larger brain with this artificial cortex, which interfaces with your brain naturally
[01:52:25.600 --> 01:52:29.400]  and processes basically information in the exact same way.
[01:52:29.400 --> 01:52:36.560]  And it could be not necessarily with the same hardware, but basically with the same functions.
[01:52:36.560 --> 01:52:43.280]  And that would, artificial cortex would be capable of consciousness, of being a component
[01:52:43.280 --> 01:52:44.280]  of consciousness.
[01:52:44.280 --> 01:52:48.320]  And then the third product of course is artificial brains that would be entirely separate, but
[01:52:48.320 --> 01:52:50.840]  also capable of consciousness.
[01:52:50.840 --> 01:52:58.120]  And so what I am saying, getting to the point is that I believe that we could create the
[01:52:58.120 --> 01:53:06.120]  process that brains have that enables what I am defining as consciousness, what I think
[01:53:06.120 --> 01:53:10.600]  we are thinking of as consciousness, but it is difficult to describe.
[01:53:10.600 --> 01:53:17.000]  I think we could take that process, that experience, and I think we could bring that process into
[01:53:17.000 --> 01:53:20.080]  a computer and make a computer conscious.
[01:53:20.080 --> 01:53:29.640]  brains computers don't have the right architecture, software architecture, and they have too many
[01:53:29.640 --> 01:53:38.320]  hardware limitations at the moment, but future computers will have vastly improved hardware,
[01:53:38.320 --> 01:53:43.640]  computational hardware, and vastly improved software.
[01:53:43.640 --> 01:53:55.040]  And that software and neural networking architectures will enable true consciousness, the way human
[01:53:55.040 --> 01:53:58.720]  brains have human level consciousness and beyond in computers.
[01:53:58.720 --> 01:53:59.720]  That's what I believe.
[01:53:59.720 --> 01:54:05.920]  But I don't think that chatbots will accidentally become conscious ever, because it's not the
[01:54:05.920 --> 01:54:06.920]  right structure.
[01:54:06.920 --> 01:54:16.800]  Can I ask you a question about that, your statement, again, I'm interested if you are
[01:54:16.800 --> 01:54:24.440]  familiar with Ian McGillchrist, he's actually a psychiatrist, but writes about the neuro
[01:54:24.440 --> 01:54:33.120]  biology of consciousness, really, really interesting to me, but one of the things that he mentions
[01:54:33.120 --> 01:54:44.040]  and I've also heard mentioned in other discussions of the nature of human consciousness is the
[01:54:44.040 --> 01:54:48.600]  vitality of embodiment in the process.
[01:54:48.600 --> 01:54:56.880]  And so, you know, because we're getting all this sense data and we're interpreting it
[01:54:56.880 --> 01:55:05.320]  as you beautifully described with all of this, I forget the exact terminology you used, but
[01:55:05.320 --> 01:55:10.960]  I was sort of visualizing this kind of almost projection mechanism where we're applying
[01:55:10.960 --> 01:55:16.280]  tremendous amount of information that we already have, associations, some of it is correct
[01:55:16.280 --> 01:55:20.760]  and some of it is in error, probably.
[01:55:20.760 --> 01:55:30.040]  So we have this kind of very complex reality that we're both experiencing and interpreting
[01:55:30.040 --> 01:55:32.560]  and affecting.
[01:55:32.560 --> 01:55:40.280]  So it's this kind of real time complexity that involves embodiment as a crucial component.
[01:55:40.280 --> 01:55:55.120]  So how does that fit into your proposed model of hardware complexity consciousness?
[01:55:55.120 --> 01:55:56.120]  That's my question.
[01:55:56.120 --> 01:55:57.120]  Yeah.
[01:55:57.120 --> 01:55:58.120]  Yeah.
[01:55:58.120 --> 01:56:02.960]  So I have thoughts, but I'm a little slow right now because it's late, so let me leave
[01:56:02.960 --> 01:56:07.120]  it open for a minute and I get lead out of the people to maybe either answer that or
[01:56:07.120 --> 01:56:11.040]  share some thoughts and then I'll come back to that.
[01:56:11.040 --> 01:56:16.640]  I think it's late everywhere.
[01:56:16.640 --> 01:56:21.640]  Yeah.
[01:56:21.640 --> 01:56:23.160]  It's way late here.
[01:56:23.160 --> 01:56:26.440]  I really should be asleep, but this is too interesting.
[01:56:26.440 --> 01:56:27.600]  I know, same.
[01:56:27.600 --> 01:56:35.280]  I was about to go to sleep when I just popped in here and this is impossible to leave.
[01:56:35.280 --> 01:56:40.960]  It's noon in Brussels now, so when I jumped in it was morning.
[01:56:40.960 --> 01:56:49.320]  I mean, I have a separate kind of question about neural healing, which is completely unrelated.
[01:56:49.320 --> 01:56:53.280]  So after, I don't want to, you know, bring in a whole other topic, though, if we were
[01:56:53.280 --> 01:56:56.080]  in the process of wrapping other things up.
[01:56:56.080 --> 01:56:57.080]  Oh, throw it out.
[01:56:57.080 --> 01:56:59.560]  We could just vamp with it for a while.
[01:56:59.560 --> 01:57:00.560]  Follow me.
[01:57:00.560 --> 01:57:01.560]  Um, well.
[01:57:01.560 --> 01:57:02.560]  This is Dick here.
[01:57:02.560 --> 01:57:03.560]  Sorry, Dasha.
[01:57:03.560 --> 01:57:13.080]  I have one question for Mika just now about the consciousness or the machine itself.
[01:57:13.080 --> 01:57:21.040]  When there is a situation when a machine needs to make a decision, how did the mechanism
[01:57:21.040 --> 01:57:28.480]  or method machine can make a decision based on a certain value?
[01:57:28.480 --> 01:57:38.760]  So the value system of the machine, how you teach the machine to value a certain situation
[01:57:38.760 --> 01:57:41.960]  like humans do.
[01:57:41.960 --> 01:57:43.960]  Okay.
[01:57:43.960 --> 01:57:55.320]  So, um, so there's a lot of work that's been done by other people that I can sort of talk
[01:57:55.320 --> 01:57:58.920]  about that that that could be relevant.
[01:57:58.920 --> 01:58:03.520]  The answers, the answers are on different scales of magnitude, depending on your level
[01:58:03.520 --> 01:58:05.680]  of focus.
[01:58:05.680 --> 01:58:09.400]  There's many different scales of answers related to this.
[01:58:09.400 --> 01:58:19.000]  So I could talk about, you know, like there's that like, you know, how, how like at an architectural
[01:58:19.000 --> 01:58:26.640]  level, how the future neural networks will make decisions in a granular way, uh, based
[01:58:26.640 --> 01:58:33.560]  on some analysis of how the brains make decisions in an extremely granular way at the neuron
[01:58:33.560 --> 01:58:35.440]  and neural circuit level.
[01:58:35.440 --> 01:58:37.920]  But that's one level of focus.
[01:58:37.920 --> 01:58:48.000]  Another level of focus is to, you know, basically how will machines learn all the relevant information?
[01:58:48.000 --> 01:58:52.760]  And, um, and so there's a work of, um, you know, many neuroscientists, and that's right,
[01:58:52.760 --> 01:58:59.880]  there are many deep learning engineers who, who want to basically create, uh, who want
[01:58:59.880 --> 01:59:06.360]  to train artificial general intelligences, um, to not only do the, to not only like be
[01:59:06.360 --> 01:59:12.520]  able to at least do the same things that children can do, but, um, but sort of like raise artificial
[01:59:12.520 --> 01:59:18.480]  intelligence as, as, uh, as our, the way you would raise a children first and then turn,
[01:59:18.480 --> 01:59:24.640]  and then turn that child into an adult, but walk them through basically a lifespan of,
[01:59:24.640 --> 01:59:31.280]  of training in, in sort of like the same sort of like, you know, along with you as, um,
[01:59:31.280 --> 01:59:35.880]  as maybe it'll be a virtual being or a little robot, but it will learn along with you over,
[01:59:35.880 --> 01:59:40.160]  over the course of a lifespan and, and maybe we'll have many people have little robots
[01:59:40.160 --> 01:59:45.240]  with them and the, the, the robots will learn sort of like knowledge at the temporal pace
[01:59:45.240 --> 01:59:49.480]  that human beings are in, so they learn to understand that, you know, because it takes
[01:59:49.480 --> 01:59:56.960]  a human being, uh, you know, 20 plus years, uh, to, to learn some things, you know, right?
[01:59:56.960 --> 02:00:00.440]  Sometimes it takes a human being 30, 30 or 40 plus years to learn some things.
[02:00:00.440 --> 02:00:01.440]  Yeah.
[02:00:01.440 --> 02:00:05.440]  I'm, I'm only 40, so I mean, I imagine there are things that I won't learn until I'm 50,
[02:00:05.440 --> 02:00:06.440]  right?
[02:00:06.440 --> 02:00:14.760]  Because it takes 50 years to learn those things, um, and, um, and, uh, so, so, uh, but, but,
[02:00:14.760 --> 02:00:20.440]  you know, but, but maybe, um, uh, once we have one machine that has gone through that
[02:00:20.440 --> 02:00:29.160]  journey with us as human beings and, and understands, um, the information that matters to us the
[02:00:29.160 --> 02:00:36.800]  way, um, the way, um, we understand it, then, then perhaps we could, um, sort of just copy
[02:00:36.800 --> 02:00:43.800]  that machine and copy their knowledge and, and, uh, the way you copy a hard drive and
[02:00:43.800 --> 02:00:48.120]  maybe at some point we'll be able to read, of course, read and write to the human brain
[02:00:48.120 --> 02:00:52.920]  as if it was a special kind of hard drive and, and we could, we could pass along advanced
[02:00:52.920 --> 02:00:58.720]  information, uh, to other human beings digitally, um, just by plugging their minds into the
[02:00:58.720 --> 02:00:59.720]  computer.
[02:00:59.720 --> 02:01:00.720]  Here you go.
[02:01:00.720 --> 02:01:03.680]  Here's, there's decades of knowledge and maybe, maybe at some point future human beings
[02:01:03.680 --> 02:01:11.120]  will, will have the equivalent of, of, um, of millions of years of knowledge, uh, and,
[02:01:11.120 --> 02:01:15.080]  and, or maybe trillions of years of knowledge because it, because, because, because at some
[02:01:15.080 --> 02:01:19.680]  point you could have these, uh, these computers that have been trained to understand everything
[02:01:19.680 --> 02:01:25.400]  that human beings have, uh, sort of like, um, you know, go through like some sort of hyperspeed
[02:01:25.400 --> 02:01:31.960]  hyperspace virtual program in which they, they can, uh, do, uh, they can live in, in
[02:01:31.960 --> 02:01:38.920]  virtual civilizations, um, at, uh, at light speed compared to the pace of life on, on
[02:01:38.920 --> 02:01:41.520]  outside virtual reality.
[02:01:41.520 --> 02:01:46.120]  And, uh, and when they come out of that, um, they've, they've, uh, lived a vast, uh, you
[02:01:46.120 --> 02:01:49.920]  know, a vastly longer amount of time than, than any human being has ever lived.
[02:01:49.920 --> 02:01:56.280]  And, and then, and then that knowledge could be, um, uh, pulled out of them and, and, and
[02:01:56.280 --> 02:01:58.360]  passed on into human beings.
[02:01:58.360 --> 02:02:03.120]  So human beings in the future could, could have, um, trillions of years worth of knowledge
[02:02:03.120 --> 02:02:08.920]  that was acquired virtually, um, through, um, uh, robots that learned to, to be humans
[02:02:08.920 --> 02:02:10.920]  by living with us.
[02:02:10.920 --> 02:02:13.720]  I love that concept.
[02:02:13.720 --> 02:02:20.000]  It's very reminiscent of, um, The Egg by Andy Weir, which is a, a wonderful sort of
[02:02:20.000 --> 02:02:21.000]  short story.
[02:02:21.000 --> 02:02:24.720]  Um, that's, that's like, like my mind, what you just said, just said.
[02:02:24.720 --> 02:02:30.120]  So, um, those are examples of, of robots that are embracing the, the knowledge of embodiment,
[02:02:30.120 --> 02:02:33.480]  which so, so many people feel so essential and so critical to consciousness.
[02:02:33.480 --> 02:02:40.080]  Um, I am of the mind that, uh, you could have an alternative form of consciousness based
[02:02:40.080 --> 02:02:46.400]  upon understanding, my understanding of what the conscious, the conscious process is, as
[02:02:46.400 --> 02:02:53.560]  a process that is a process that's independent of the, the, of the, the data that's coming
[02:02:53.560 --> 02:02:54.560]  into it.
[02:02:54.560 --> 02:02:55.560]  That's my understanding.
[02:02:55.560 --> 02:03:00.280]  So that, so that, so you could have a virtual environment or you could have just like abstract
[02:03:00.280 --> 02:03:01.360]  stock data.
[02:03:01.360 --> 02:03:06.040]  You could have, um, I mean, what, what, what is, what would be important to the brain
[02:03:06.040 --> 02:03:14.080]  is the ability to make, uh, connections between different, uh, types of, of, of data so that
[02:03:14.080 --> 02:03:20.080]  there's a multidimensional representation, um, that happening, but, but there could be
[02:03:20.080 --> 02:03:25.920]  a conscious consciousness of a multidimensional representation that is unlike anything that
[02:03:25.920 --> 02:03:31.160]  we, we experience as, as bodies, as embodied consciousness and that could also, that other
[02:03:31.160 --> 02:03:36.360]  alternative consciousness could also be conscious, but in, but in a different sort of dimension
[02:03:36.360 --> 02:03:37.360]  of reality.
[02:03:37.360 --> 02:03:46.240]  So do you think that consciousness, as you understand it, requires a physical substrate
[02:03:46.240 --> 02:03:51.440]  of some kind, like a, a, does it require hardware?
[02:03:51.440 --> 02:03:52.440]  Yes.
[02:03:52.440 --> 02:04:00.200]  Um, yes it does, but only because there's no way to separate information from physics.
[02:04:00.200 --> 02:04:04.880]  Information is non-physical, but it's like a pattern, but it's never not embedded in
[02:04:04.880 --> 02:04:05.880]  physics.
[02:04:05.880 --> 02:04:06.880]  It's never not.
[02:04:06.880 --> 02:04:07.880]  Right.
[02:04:07.880 --> 02:04:11.280]  But what about like a radio, radio waves, what about a carrier wave?
[02:04:11.280 --> 02:04:17.840]  What about some sort of, um, I mean, there's still physicality there, I, I, I understand
[02:04:17.840 --> 02:04:28.840]  where that could go, but, uh, but, um, disembodied as we might think in the solid sense.
[02:04:28.840 --> 02:04:31.640]  So with it, so with the radio wave, there's two parts, right?
[02:04:31.640 --> 02:04:36.000]  There's one, one is the wave and the other is the information.
[02:04:36.000 --> 02:04:40.320]  And even in, even in the radio waves, like, you know, it's like the radio wave itself
[02:04:40.320 --> 02:04:46.560]  is just like a CD-ROM and, um, and the information is just the pattern in the CD-ROM or in the
[02:04:46.560 --> 02:04:47.560]  radio wave.
[02:04:47.560 --> 02:04:52.160]  And, and so it's the same sort of, it's so, it's, it's just, um, it's just not, it's
[02:04:52.160 --> 02:04:55.160]  not, it's not a solid CD-ROM.
[02:04:55.160 --> 02:05:01.280]  The problem I have, Michel, with that, uh, view is that it's too concrete.
[02:05:01.280 --> 02:05:06.040]  Of course, we know CD-ROMs and computers and how we store information over there, but
[02:05:06.040 --> 02:05:13.880]  we at the moment still have really no clue how we, uh, touch into information with the
[02:05:13.880 --> 02:05:14.880]  brain.
[02:05:14.880 --> 02:05:19.880]  And, uh, I was following Chris, I don't know, Chris, right?
[02:05:19.880 --> 02:05:20.880]  Chris.
[02:05:20.880 --> 02:05:21.880]  Yeah.
[02:05:21.880 --> 02:05:22.880]  Chris.
[02:05:22.880 --> 02:05:25.320]  So what about radio waves?
[02:05:25.320 --> 02:05:31.480]  So the way we approach it, uh, from, from, from our research field, which is systems
[02:05:31.480 --> 02:05:37.880]  and cybernetics, is that, um, it can actually, first of all, be described as a system and
[02:05:37.880 --> 02:05:38.880]  a cybernetic.
[02:05:38.880 --> 02:05:46.440]  And that, so systems and cybernetics is basically something virtual, but it's about, um, creating
[02:05:46.440 --> 02:05:50.400]  somehow a recursion into a system you can create.
[02:05:50.400 --> 02:05:56.800]  So I, I, I mentioned earlier, um, you can actually create a virtual machine as a scaffold
[02:05:56.800 --> 02:06:01.720]  to create something in that virtual machine that can build an actual machine.
[02:06:01.720 --> 02:06:04.640]  Stuff like this is happening at the moment as well.
[02:06:04.640 --> 02:06:09.080]  So, um, so I want to disagree with something that was said, that Michel said.
[02:06:09.080 --> 02:06:14.840]  So, um, I, and purpose, like, whenever, like, whenever I hear anybody say we don't know,
[02:06:14.840 --> 02:06:19.760]  I always react internally, like, I should tell them to just say I don't know and to
[02:06:19.760 --> 02:06:23.560]  never say we don't know, because if they don't know, they, they can't speak for everyone
[02:06:23.560 --> 02:06:24.560]  else.
[02:06:24.560 --> 02:06:25.560]  They don't know what everyone else knows.
[02:06:25.560 --> 02:06:26.560]  No one took a survey of everybody.
[02:06:26.560 --> 02:06:32.720]  Um, but, but what I want to say is that we, is it, is that, um, it is known by at least
[02:06:32.720 --> 02:06:36.800]  some people how, how the information, how the brain detects information.
[02:06:36.800 --> 02:06:43.480]  Um, and, and, uh, so you've heard of Hibian learning, you know, there's, there's, um,
[02:06:43.480 --> 02:06:47.320]  uh, neurons detected, uh, coincidence patterns.
[02:06:47.320 --> 02:06:50.200]  Neurons detect when two neurons downstream fire.
[02:06:50.200 --> 02:06:54.000]  That is the detection of information that, that is in a book called the neuro basis of
[02:06:54.000 --> 02:06:55.000]  free will.
[02:06:55.000 --> 02:06:58.400]  And, uh, so that's known, not unknown.
[02:06:58.400 --> 02:07:07.160]  So what is known there is that, uh, something can create a pattern that something can recreate.
[02:07:07.160 --> 02:07:08.160]  That's the thing.
[02:07:08.160 --> 02:07:10.040]  That's the only thing we know.
[02:07:10.040 --> 02:07:16.280]  No, no, it's, no, it's, it's that the, that information can be detected as coincidence
[02:07:16.280 --> 02:07:20.800]  patterns and then considered by, by neural circuits and then so on.
[02:07:20.800 --> 02:07:22.120]  That's coincidence patterns.
[02:07:22.120 --> 02:07:24.280]  That's the essence as a coincidence pattern.
[02:07:24.280 --> 02:07:31.920]  So that's cybernetics.
[02:07:31.920 --> 02:07:34.040]  I can't tell if you're agreeing or disagreeing.
[02:07:34.040 --> 02:07:35.040]  I'm not sure.
[02:07:35.040 --> 02:07:36.600]  Do you want to hear a poem?
[02:07:36.600 --> 02:07:39.080]  I wrote about green base.
[02:07:39.080 --> 02:07:43.720]  But that's the thing that, that's been described in the 90s, 20s, and that's what we mean with
[02:07:43.720 --> 02:07:49.000]  the, with the, we, if I was referring to the, we, I was referring to the group that actually
[02:07:49.000 --> 02:07:54.000]  started school of thought related to systems and cybernetics and of, I mean, they made
[02:07:54.000 --> 02:08:00.320]  many misconceptions like every new school of thought, uh, it, it all has evolved.
[02:08:00.320 --> 02:08:06.600]  And today we talk more about complex adaptive systems and evolutionary cybernetics.
[02:08:06.600 --> 02:08:09.760]  So, so, but it's still basically the same.
[02:08:09.760 --> 02:08:15.800]  It's the same that there are patterns you can see across domains, across disciplines,
[02:08:15.800 --> 02:08:20.120]  which you kind of start understanding as a kind of laws of nature.
[02:08:20.120 --> 02:08:28.040]  But in the 90s, 20s, 30s, so that's the found, founding moment of, um, the, the, the, the
[02:08:28.040 --> 02:08:34.560]  cybernetics, they, they started understanding that those patterns are, uh, you can describe
[02:08:34.560 --> 02:08:39.840]  them in a virtual way and they still say something about reality.
[02:08:39.840 --> 02:08:43.640]  So if you create a recursion between those two, you can bootstrap it.
[02:08:43.640 --> 02:08:48.220]  And that's what I've been trying to say earlier, like what the things we're doing with something
[02:08:48.220 --> 02:08:53.640]  like a bootstrapping compiler or, or what we're doing with scaffolds when we build,
[02:08:53.640 --> 02:09:01.040]  uh, uh, physical, um, uh, buildings, it's, it's, we create scaffolds as a temporal week
[02:09:01.040 --> 02:09:07.880]  structure to create something more stable that can exist once, uh, we created the, the
[02:09:07.880 --> 02:09:12.800]  feedback and once we created feedback, we can take off the, the scaffold again and then
[02:09:12.800 --> 02:09:14.880]  the building can stand on its own.
[02:09:14.880 --> 02:09:18.760]  So, so that's kind of, uh, bootstrapping.
[02:09:18.760 --> 02:09:25.280]  That's the thing that, that can allow you to start creating from a virtual world, like
[02:09:25.280 --> 02:09:31.000]  just an idea and then put that idea by action in the world and you create scaffolds first
[02:09:31.000 --> 02:09:34.680]  and then the thing becomes more autonomous.
[02:09:34.680 --> 02:09:36.080]  All right.
[02:09:36.080 --> 02:09:42.360]  Um, so I want to go around the room and we'll, we'll wrap up soon and give everyone a chance
[02:09:42.360 --> 02:09:49.000]  to, to say, to say something and, uh, and before we wrap up and anyone who's in the audience
[02:09:49.000 --> 02:09:52.800]  who wants to come up and say something before we go, just, um, raise your hand and I will
[02:09:52.800 --> 02:09:54.480]  also send an invite.
[02:09:54.480 --> 02:09:58.640]  Um, and then, uh, after everyone has said something, we'll, we'll go ahead and, um,
[02:09:58.640 --> 02:10:03.880]  because I just, I feel a responsibility to, uh, we've had such a great conversation, but
[02:10:03.880 --> 02:10:10.440]  I'm keeping everyone awake except for, except for Mexico's afternoon over in, uh, and, and
[02:10:10.440 --> 02:10:14.440]  I don't know what time it is in Manila, Manila, uh, let's see what time it is.
[02:10:14.440 --> 02:10:18.960]  But anyway, so, but for some of us, it's, we're up, we're up too, too late and our sleep
[02:10:18.960 --> 02:10:26.920]  is super important and, um, so I'm, I'm going to like, uh, so I'll go ahead and pause and
[02:10:26.920 --> 02:10:31.840]  pause for myself. It's been wonderful and, uh, and to seal, do you want to say something
[02:10:31.840 --> 02:10:37.720]  next and then, then we'll just go in order than Dasha, Jesse, Carl, Chris, Mixwell, Shane,
[02:10:37.720 --> 02:10:39.720]  Dave, etc.
[02:10:39.720 --> 02:10:46.720]  Can you reset, refresh the room so that the order is, uh, for everyone the same?
[02:10:46.720 --> 02:10:53.720]  Oh, okay. Yeah. Everyone. Okay. So I guess, I guess, uh, Dave is, is, is, uh, is in charge
[02:10:53.720 --> 02:10:54.720]  of the room now.
[02:10:54.720 --> 02:10:59.680]  Um, so yeah, Dave, if you want to go first and then Mixwell and then Seal and then Shane,
[02:10:59.680 --> 02:11:03.920]  Dasha, Jesse, Carl, and Chris, and then I'll invite the rest of the folks if they want
[02:11:03.920 --> 02:11:08.720]  to come up. And, uh, so thank you, everybody. That's, uh, the last for me and, um, I'll go
[02:11:08.720 --> 02:11:12.720]  ahead and, and let's you all finish it up.
[02:11:12.720 --> 02:11:24.720]  So I guess Dave is, uh, not there. So I'll take a sleep, perhaps. Yeah, maybe I can understand
[02:11:24.720 --> 02:11:30.720]  you guys. I mean, yeah, for me, it's, it's, it's been a great, uh, morning, right? It's,
[02:11:30.720 --> 02:11:35.720]  it's noon now. I need to go eating also. So do a lot of other stuff, but it was a way to
[02:11:35.720 --> 02:11:42.720]  fun to, to, to, to leave. And so thanks a lot, everyone. I've learned a lot. It was fun to
[02:11:42.720 --> 02:11:48.720]  interact with all of you. I hope to see you guys again and have continued the discussion.
[02:11:48.720 --> 02:11:57.720]  So until the next time, I've enjoyed it a lot and I'm very happy to be here. Thanks.
[02:11:57.720 --> 02:12:02.720]  It was a great conversation and very thankful to have listened to all of your ideas and
[02:12:02.720 --> 02:12:07.720]  have met you today. It's too bad. I had a very bad signal. I would have had a lot of things
[02:12:07.720 --> 02:12:13.720]  to say, perhaps in the near future, we could do a new discussion again to continue and
[02:12:13.720 --> 02:12:19.720]  explore what we're thinking about and researchers that has been shared in terms of, uh, the
[02:12:19.720 --> 02:12:31.720]  connectivity of different disciplines and the complex.
[02:12:31.720 --> 02:12:36.720]  I think you cut out. Yeah. So thank you.
[02:12:36.720 --> 02:12:50.720]  Okay, Shane, you're going to next.
[02:12:50.720 --> 02:12:59.720]  Moving on to Dash. Yes, we're dropping like flies here in our brains or falling asleep, perhaps.
[02:12:59.720 --> 02:13:07.720]  Yes, I also just thank you so much, Micah and Excel for, for having such a, just a thorough
[02:13:07.720 --> 02:13:12.720]  back and forth when I joined this room that it was impossible to leave. And I really appreciate
[02:13:12.720 --> 02:13:19.720]  you having a conversation where you disagree but are challenging each other in respectful
[02:13:19.720 --> 02:13:25.720]  ways. I in the future would love to bring up the topic that I can get to tonight about
[02:13:25.720 --> 02:13:31.720]  the olfactory nerve and nerve healing with COVID and the brain with what's going on with
[02:13:31.720 --> 02:13:36.720]  COVID. That's an interest of mine and I have some very good friends that are suffering greatly.
[02:13:36.720 --> 02:13:41.720]  So I have a lot of questions and thoughts about that and I'd love to have a conversation
[02:13:41.720 --> 02:13:47.720]  done online someday with anyone that's interested in that.
[02:13:47.720 --> 02:13:53.720]  I want to quickly respond to that, Dash, because like eight years ago, we, we actually tried
[02:13:53.720 --> 02:14:00.720]  to create a project on food security. You try to guess where and try to guess what we
[02:14:00.720 --> 02:14:07.720]  try to prevent. So I would love to, to get in there. We want to build a living city to
[02:14:07.720 --> 02:14:13.720]  improve food security for the province of Habay. And the top diplomats of the province, so
[02:14:13.720 --> 02:14:18.720]  that's the province where Wuhan is located. The top diplomats really wanted us to, to
[02:14:18.720 --> 02:14:26.720]  work on that, but we got kind of canceled out by the, the, our own, well, diplomatic
[02:14:26.720 --> 02:14:32.720]  frame from, from Europe. So I'd love to tell you a lot more about that and I would love
[02:14:32.720 --> 02:14:46.720]  to go into those directions. Thanks. Yes, you want to say something?
[02:14:46.720 --> 02:14:50.720]  Falling like flies going over to Carl?
[02:14:50.720 --> 02:14:56.720]  Thankfully it's, it's only 11 in the, in the morning here. So I'm relatively awake.
[02:14:56.720 --> 02:15:01.720]  This has been a wonderful conversation. I intended to come in for about five minutes
[02:15:01.720 --> 02:15:07.720]  because I was a little bit bored and ended up staying for an hour and a half.
[02:15:07.720 --> 02:15:12.720]  I think something about that. It's been absolutely wonderful.
[02:15:12.720 --> 02:15:18.720]  And, and, and Mixle, you've been fantastic. And then, you know, Dasha and Jesse and Chris
[02:15:18.720 --> 02:15:25.720]  have been absolutely wonderful as well. You definitely helped this ape understand a little
[02:15:25.720 --> 02:15:33.720]  more of the world outside of his cave. Yeah. And my takeaway from this is maybe consciousness
[02:15:33.720 --> 02:15:41.720]  is defined at the point where you can ask if I'm conscious.
[02:15:41.720 --> 02:15:49.720]  I'm a little conscious. Thank you, Micah and everyone for indulging my amateur dabbling
[02:15:49.720 --> 02:15:55.720]  and really, really fascinating. And a lot of, a lot of things that were said this evening
[02:15:55.720 --> 02:16:03.720]  are kind of fuel resonating, which is all I can ask. And I am actually going to leave
[02:16:03.720 --> 02:16:12.720]  you with an eight line poem about brainwaves. It's called, it's called Team Brainwave Action
[02:16:12.720 --> 02:16:18.720]  Calibration Chant. And you have to imagine it in a call and response, but I'm just going
[02:16:18.720 --> 02:16:24.720]  to read it in my sleepy monotone here. So it goes like this. Delta theta, alpha, beta,
[02:16:24.720 --> 02:16:30.720]  sequence, switcher, loop or theta, gamma, gamma, gamma, hey, flash of insight for today.
[02:16:30.720 --> 02:16:37.720]  Theta, delta, beta, alpha, blossom money from alpha, gamma, gamma, gamma, gosh, stage dive
[02:16:37.720 --> 02:16:43.720]  into mental mush. Alpha, beta, theta, delta, need a nap. I'll see you later. Gamma, gamma,
[02:16:43.720 --> 02:16:49.720]  gamma, hey, remember what you came to say. Beta, alpha, theta, delta, give me sleep or
[02:16:49.720 --> 02:16:54.720]  give me shelter. Gamma, gamma, gamma, gosh, surf the curl of wishy-wash.
[02:16:54.720 --> 02:17:03.720]  Ah, Chris, you just made my day. Thanks. That was amazing. Glad you liked it. Thank you.
[02:17:03.720 --> 02:17:14.720]  That was cool. Anyone else? Is that it? That's it, I guess. Thanks a lot, guys. And girls.
[02:17:14.720 --> 02:17:24.720]  Thank you, thank you. It was great. Good night.