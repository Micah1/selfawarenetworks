a0320z Artificial Neurology: Robotic State & Stages

To the reader: Think of yourself as an organic robot to sort of anchor your focus abstractly in the discussion of this book. Of course by some implied definition a robot is something man made, and by implied definition a human being is not man made, but shaped by evolution, a product of nature and nurture. Yet this is useful in the same way that Dennet's conjecture is that we are not seeing redness, we are just experiencing the illusion that we are seeing redness. Imagining that you are an organic robot is useful for thinking about cognition as a computational process. Neurons and other cells are doing complex non-linear spatial & temporal computations, so it's not a straightforward computation like what your computer in 2022 is doing, but even so there is a computational process that can be understood, and the comparison of consciousness to computational rendering is valid, useful, and correct.

If a human being can have developmental states, experiences and transformative experiences, and developmental stages, where your perspective on reality shifts in a big way after a long time, then our goal is to imagine what it will mean for sentient robots to have experiential states and developmental stages that they can pass through. We want mature helpful robots, not crazy unhelpful robots right? So we have to think about developing robots' minds and what we can learn from the development of human minds.

Imagine that Synaptic Frequency is a neural correlate, or the information configuration of a cell in terms of its embodied frequencies is a neural correlate for state, or the state of your experience of reality at a given moment.

Imagine that Synaptic Connections via Growth LTP & Decay LTD, over a long term, result in eventual threshold changes that we can identify as Stages, or major changes in life perspective that only happen after a very long time, or only happen after a very big life event.

The concept of Cognitive States & Stages comes from either AQAL  (All Quadrants All Lines) or Spiral Dynamics, I forget which, and it doesn't matter. 

It's a concept that means we, as robots, can have computed short term state experiences, with synaptic phase changes, and long term stage changes, with (real or simulated) synaptic growth or decay which takes longer to happen.

I think of defining the brain into a hardware synaptic growth configuration layer, and a software synaptic frequency layer, at least as a working analogy.

I speculate about building a Sentient Self Aware Neural Network on an FPGA, a GPU, a TPU, with a memory buffer, or with a 3D program, as a part of a BCI Brain Computer Interface.

An FPGA can change its circuit mapping, but it isn't meant to change fast, and it isn't meant to change many times, so it's not the right substrate to mimic the brain. I think however that a GPU could be the right tool for now, given that we want to simulate the physics of both short term fast phase changes and long term slow connection changes. The essential physics process could be simulated in just matrix calculations on a TPU, with each dendrite, each neuron, each synapse, and each cortical column being unique or random in its original configuration, and in a sense what is needed is a space for an array for the outputs to be rendered in human scale time, so that they can be perceived by a sequence of other arrays, also in human scale spacetime, in a loop of array signaling that lasts for a duration comparable to human experience. Each neuron also specializes in non-linear spatial computation, meaning that where, when, and how the neuron is interacted with helps determine the type of response that it will have, including variations in the frequency & magnitude (duration & amplitude) of it's response, the same sentence I just wrote is applicable to neural arrays, edge communities, cortical columns, and any synchronously oscillating group of cells. The GPU as a graphical process could be the right tool, because we could have the neural network render it's output inside a 3D graphical program, it doesn't have to print to the screen, it could print to active memory, but if it prints 3D graphics we could have a visualization of the artificial mind's development, and interestingly this could lead to brain computer interfaces through virtual reality or augmented reality applications where the human subject is interacting with the artificial intelligence via a head mounted display, or glasses, and the AI is similarly observing the human subject, possibly with non-invasive medical imaging sensors, and possibly with just the sensors that are in existing VR headsets today such as eye tracking, microphones, hand tracking, head tracking, and position tracking sensors. The idea is that as the head mounted AI thinks about you, you see its thoughts, and then its thoughts affect your behavior in some small way, and perhaps eventually you two will eventually be in sync together. You with your partner's brain. Your eXtenda brain. (Disclaimer: eXtenda brain is my trademark.)

Will neuromorphic chips offer something that beats the GPU? I think so. Every year I search for news on neuromorphic chips, and I look for updates on Stanford's long running Neurogrid project, and I search for other projects attempting to build computers that can simulate the brain faster & more efficiently than previous efforts. 

