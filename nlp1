Neural Lace Podcast 1 Transcript

Apr 12, 2017

Original Audio here https://youtu.be/h2OLBUIQouI

00:03
So welcome to the Neural Ace podcast but then realized podcasts is all about narrow physics which is the science and technology behind their release neural. Physics is an umbrella topic that includes spatial computing, which is the technology behind virtual reality, augmented reality and mixed reality. It's also an umbrella topic that includes deep learning.
00:31
Artificial intelligence. Some of the topics that confidence podcasts will be back in computer. Visions specifically about applying deep learning to medical imaging, it's the first podcast will attempt to unite. The topics of web. You are with narrowlays and quantum mechanics. It's the podcast that there's asked how information in the networks, the brand are organized and how we might query that information.
00:57
And any region of the brain with a brain port, that understands a transmission, particle of the brain. I'm gonna propose that transmission protocol on this podcast and let's talk about hacking into brains view, our system, add things here reality that are not really there and and I actually attempt to explain how we can do that in this first episode.
01:21
And then we also chat about in the course of this podcast, talking about the kind of things. We can download information from the brain like holograms of your experiences as if your eyes became cameras. So we can capture what you saw, what you heard, and you can share those experiences of others.
01:40
You can share those experiences in court. We also talk about defending our brains against remote hacks and we both like we dared to boldly. Go where no one has gone before. So welcome very much to the new release podcast. Your host is Michael Bloomberg. The podcast is edited by Adam Alonzi and today's guest on the episode one, democratizing neural lace is Shannon around.
02:06
Who is an air beer? Enthusiast, Shannon is also part of the chronos group, developing open XR. And I wanted to talk to Shannon in part because he's connected to web, PR and web show. But also, because I'm interested in creating an open standard for normalize, so the world can have it.
02:30
And that's why I'm giving it away on this podcast and future episodes of this podcast, we talking to neuroscientists and we'll be talking to computer scientists and we'll be talking to the executives. That major tech companies that are making EG products product products, or AI products or new kinds of web browsers or just new kinds of technology period.
02:57
People who are working on converging technologies, like applying deep learning to, to all sorts of imaging, not just medical imaging. And so there's so much technology that's converging. And it's going to result in amazing for progress in terms of technology and science. And so the social group that can be achieved so welcome to the narrow lace podcast.
03:26
I hope you enjoy it. So let's go ahead and get started. So Shannon. Let's explain people, maybe went in early says, and at the root of it, you could say, let's describe that. It's a advanced form of just brain computer interface. Yeah, thanks for having me. All right. So yeah, my name is Shannon Neurel and I'm a passionate I guess VR AR evangelist type person kind of been here for the ride the last five years or so and just bringing it to the next evolutionary step which is passed AR which is going to be a neural lace and well you get to almost say neural laces is going to become factored into VRA are as we know it neural lays being a branded computer interface.
04:24
So we were talking about we have there's a lot of great ideas on how to achieve an release and a lot of people want to keep their ideas a secret. And so we're talking about that in terms of if we give away how to make neural lace on this podcast, what is our object event and shouldn't release be democratized.
04:43
So every company can know how to do it. And, and my thought there is that we should democratize because we really want this technology to exist in the world. And we do not necessarily know because it's a very complex thing that to try to solve. And so we want to encourage as many people to get on board with helping us to create an earliest possible, so that we can habit early in our lives.
05:05
Times. Absolutely agree and they're everyone in the space and so far as I've seen, it are very protective of their algorithms, their techniques. I mean, talking about it, openly, it seems to be like, you know, protecting their IP, they're squatting on domains already. You know, it's kind of a, it's kind of ridiculous because it is a difficult problem.
05:30
It's it's a problem that when Saul will affect all of humanity. So why I shouldn't we all share it. And I think having an open source will be useful and required really, to make it happen because we have secret algorithms that are operating, we don't know how what's going on in the background.
05:50
It's kind of slowly adoption rate of neural lace. People will be suspicious of it, things like that. So I think you know, if we create some kind of a open source standard or groups that will work together to make this thing happen I think it's a good thing it's a good move.
06:08
So someone remarked me recently that in order to achieve the kind of an earliest we're talking about a brain computer interface. We're talking about you'd have to have a number of like nobel prize winning discoveries, that would happen. It's pretty much simultaneously and the so one of the positive outcomes that of norizes is we'll be able to do it's on the way to discovering it.
06:30
We're going to have to revolutionize medical neuroscience and that, I mean, for example, we're going to have to figure out the transmission. Let's let's say that the brain is his organized, like brain of the nurses from organized, like the internet. For example, as many neuroscientists believe we're going to have to figure out what the transmission particle of the brain has and if we do that, you know, from neurontin on if we do that, it's going to enable the kinds of products like like artificial.
07:02
Li that can connect to your nervous system or reconnecting the spine. It's not necessarily as glamorous as the science fiction concept of neural lays. But there's a lot of practical medical applications to being able to connect a limb, or reconnect the spine in an artificial limit here, nervous systems.
07:20
So you can really even create new kinds of wins. So these are really positive outcomes that can come from pursuing a direction of research towards Well, let me just back up and say kind of some points I wanted to cover. So solving quote neural lace brain, computer interface. Yeah, it is a difficult task but yes it is something that needs to not be overthought too much.
07:49
There are certain babies steps we can take along the way that are low-hanging fruit. That are easily soluble now. Okay, the the first path. So I'm sitting here, I'm looking at any motive headset because aren't familiar with that it has I think it has six EEGs on it. There have one model that has six.
08:11
Another one has 14 and what that does is it pulls in brainwaves from different regions of your brain. Okay, now there's a there's a section called a homunculus. It's kind of an idealized section of the brains aren't really a specific region but it's an idealized version region of the brain that will indicate things like facial expression.
08:37
It'll indicate like eye blinks it'll indicate. I think body posture hand position if we're able to accurately read these areas of the brain. So these emotive devices are really quite primitive now. I mean there the the sixth sense of one, two, three, four, five, six sensor, one effectively reads skin tension and and small motors up in the forehead and can infer things like like, eyeblink, stuff like that.
09:15
So my first, my sort of my first baby step I want to accomplish is to use a an easy headset, neural lace alpha point one, to solve the problem of not having a face and VR. So, we're in, when you're in virtual reality, you don't really have, you know, your eyes are covered.
09:37
You can't, when you're looking at someone else in another virtual world, you can't tell if their eyes are blinking. You can't see if they're mouth is moving. I think that these that particular problem is solvable in the, in the short run, okay? So sort of reading facial expressions, I'd like to solve that first, the next step would be to get more involved and get actual body position and arm position finger position by reading these sections of the brain.
10:06
Now, how do we read these sections of brain of the brain? When, you know, everyone's brain is different and the placement of the EEGs may be different each time. So, these are definitely problems we have to solve. We have to solve. So, so at present what we have, and I've seen this in action, is is a way of sort of recording, brainwave states and inferring.
10:32
Meaning from that. So for instance, let's say you put on a headset. You're in a blank completely blank room, and I put an apple in the middle of the floor. So all you can see is an apple and then I record your brain wave state as you focus on this apple.
10:49
Okay, okay. Next. Stop record. So we have this capture of data of what an apple is to you say, I give the headset to mica, he puts it on. Looks at the same apple, we record his brain wave state. Okay. So they should in theory, be quite similar. We don't know for sure at this point if they are.
11:14
But presumably, there will be an amount of machine learning and pattern recognition that we can feed these data sets into to arrive at what is the perception of an apple? It's kind of, I kind of like it to the the old days of of, Dragon naturally speaking. Remember, that app where you had to like, train, the you had to train the interface to recognize your voice.
11:40
Yeah. Dragon naturally. Speaking led to Siri, yeah, it's no serious now. You can pick up any phone and talk to it and it knows your voice. You don't have to train it. It's not it's not a thing anymore. Like I say, the sort of the first step is gathering up these patterns building up a large database of brainwave patterns.
12:03
Applying it to genetic algorithms to other machine learning techniques that will in effect be able to record what our perception is of certain objects, so that you can put it on another person, and they can think Apple, and you will know that it's Apple. It's kind of like this, this read only extracting of data from the brain.
12:30
So step one is to do the facial recognition stuff. Step two is to do the pattern recognition for known objects step. We got step three of kind of a larger overriding step that we can be working on simultaneously is how do we do a right into the brain. So we're working on read, right?
12:52
Is another thing that's really far out in the horizon but for now I just be happy with read but you know presumably if we know exactly what the pattern the brain wave pattern does is of you viewing. An apple looks like, and we knew how to transmit that wave brainwave pattern into your brain.
13:09
You would visualize an apple without being an actual apple there. That's kind of the long and short of neural lace. I think it's fascinating. There's a lot of different people with a lot of different ideas about new ways to send data into the brain. For example, David Eagleman talks about there are strips you can put like on your tongue or on your back that the strip is a great of electrodes and it's connect to a camera and the camera is watching the world.
13:39
And takes that image and conversion into the electrical grid of signals on your back or on your tongue, and eventually your brain figures out how to seen image from that, that is, is a way of inputting data back into the brain. In fact, just using your regular eyes is actually putting data back into your brain.
13:58
So that's that's another input channel to think about really fascinated with, you know, with all the different ways, what was Shannon shared is is one way of attacking the problem of solving her lays. But there's are people who are setting medical imaging, you know, with with MRI machines besides ET.
14:19
So Shannon was, let's go to the EG approach and there are people who are putting chips inside the cutting into the brain and putting tips inside the, the patients of who have epilepsy who need their brains open up. Anyways and then so, this has been sort of like a long dream of scientists who study the brain of what kinds of sensors can we attach?
14:39
And once we have the sensor data, how can we analyze that data with with new techniques, like deep learning with computer vision? We have, we can create, you know, different kinds of biosensors. We can apply deep learning to use sets of data in new ways and potentially revolutionize medical neuroscience by combining state-of-the-art AI with medical imaging of all kinds not just EG.
15:06
But so that's my thought there. I'm, we can totally start with some easy products and and come up with. In the result of that action will certainly yield some really awesome new neural lays products but it's not the status, really the only direction the industry can go. So this is definitely a broad topic and hope we can cover a lot of those different ways in this podcast.
15:28
Yeah. I mean certainly a wet interface is more efficient. I mean you know drill a hole in the back of someone's skull, mount an electrode, you know, it's in a known location. You can you can calibrate it and train it to, you know, based on the exact brain and it never shifts and it removes, you know, but will will society adapt that adopt that?
15:51
I don't know, jury's out on that when I think not but, you know, I probably be willing to try it but I don't know that other people would. So I just wanted to say that I'm really interested in creating a sort of installed, wetware sensor for the purpose of, just a research but I was at CES 2017.
16:19
And I saw this really awesome new wireless EHA system. So that's one of the things that companies are talking about right now is, you know, we actually could do sort of a nonavase of neural lace and get to the core of a brain by using a variety of of tools, such as some, you know, like, like they use ultrasonic sound for surgery, you know?
16:43
So there are ways to stimulate the brain and read the brain, like really deep without actually, cutting into the brain. And so that's definitely well, I mean, it's certainly possible to embed an electrode under the skin. So let's say you have a tiny incision behind the scalp and you just insert the electrode under the so directly on the skull.
17:04
So that's, you know, quote unquote wetware but it's it's not exactly as invasive as you know, punching a hole in the inner skull. I don't know how pleasant that be. That would be, I think the that at least my goal for a manifestation of it of the version, one of neural waste using EG is it's going to look like a baseball cap.
17:25
It's a baseball cap, you pop on, and it's got all the EGs sensors in there. Yes, there's gonna be a ton of noise that comes out of those readings. Yes, it's gonna be difficult to sort of sift out the meaning from all that data, but I think we've been making a lot of improvements and advances in in computer vision lately with the autonomous driving vehicles and whatnot, and the light are data that comes out of that is just so full of noise.
17:53
It's, it's incredible. How much noise is in that data. But, you know, our guys with doctorates and things have been working out, great algorithms, just through that data and pull out the meeting. So, I think some of that learning liberal stories can be applied to EG Norway's. It definitely, you know, if you look at the supercomputer, the Nvidia is doing for self-driving, cars.
18:16
That's a really mean and serious machine and they're still figuring out how to do it. So, basically, they're computer has all these sensors, light are like, eight cameras besides that and a whole bunch of other things and it's figuring out, not only this space is around the car, but also it's it's it's figuring out what the objects are like, there is a, it's saying, well, this is, this is a cat and this is a dog.
18:38
And this is a card door in the stars. He, this car door is gonna open or this car door is going to close, and there's a car across the street. That's, it's moving at such a speed and it's going to intersect the path in front of you. These are things that they're having the require a supercomputer, that's going to be installed in front of your car for for driving.
18:59
But if we can take for one minute, if we could take those self-driving cars, and apply them to the task of of narrow science, we could probably create a revolution neuroscience. So those are the kinds of things that keep me up late. Yes, I've seen a samples. I was at a talk recently at Stamford and they it was, it was on, light are single, photon, light art stuff.
19:26
And an image that it that it pulls out, was just literally, a cloud of looking like, a cloud of dust it was just like, smeared, you couldn't make anything out of it. And then they did a single past filter of it using their advanced algorithms and voila embedded in that was, you know, a picture of a dog or whatever.
19:46
It was, I forget what the three but a three-dimensional picture of the saying, but on the first passive it just look like garbage. That's what I anticipate the version of neural lace using EG will just have this cloud of garbage that comes out that we need to sift through, to have, you know, true meaning up.
20:06
So it's, it's doable. I think it's doable. I think we just don't want to overthink it too much, so some final thoughts in terms of you know, recently in the news we had Elon Musk announced that he is a new knurling initiative and so suddenly since a popular topic again so but big companies that are thinking about jumping into the nearly scheme, they what are some things that they might need to think about in terms of, you know, making products that have the can scale and have broad commercial value.
20:43
And do you have some thoughts on that? Well, at this, at this phase, we're such early days. I mean, we're just thinking about figuring out how to do it, you know? And if it can be done, then we have to worry about, you know, ethics and and, and, you know, appealing to a broad range of people, not offending people, not letting them think we're using their, their thoughts to market things like that.
21:10
I mean, so so ultimately, there will need to come into play. Some kind of standards. I'm actually involved with the chronos group. Doing some stuff with open, XR helps to do with VR and AR and also, the IEEE. There's a standards dealing with virtual reality augmented reality. So in particular with augmented reality, let's say you have eye tracking and you can see where people are looking.
21:39
You can see if they look at the coke can versus the Pepsi can now do we allow that data to be transmitted to marketers with personally identifiable information? So that we know, you know, to always put a coke at in front of Shannon, you know, probably not. So we're willing to develop standards having to do with brain, to computer interface.
22:01
So, do we do, we let personally identifyable information be sent with thought patterns to servers to be processed. No, definitely not. So, we'll need to implement standards.
22:27
Hi everyone.
22:35
So, welcome to
