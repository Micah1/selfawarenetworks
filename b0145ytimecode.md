b0145y Time Code (This version was auto transcribed by OpenAI's Whisper with time codes)

Title: Thoughts on Michael Graziano's Description of the TPJ as a Consciousness Module, and his Attention Schema Theory contrasted with my thinking on a distributed Observer made up of many smaller observers each of which is an oscillator.

Quick note: Some of my thoughts on Michael Graziano's work, his TPJ module vs my comparison of the brain as a tv monitor rendering one line of pixels at a time, my thoughts start off rambling but then I draw a contrast between theories of a local of central focus, or a center of consciousness, to a distributed conscious observer consisting of smaller observers, audio is cleared by me to share

Listen to the Original Audio here https://recorder.google.com/share/b05673ce-0f9d-4d54-918e-1d054b8be409

Audio transcription begins below this line

---------------------------------------------------------------------------------------------------------------

[00:00.000 --> 00:09.640]  Yeah, so Michael Grasio thinks that consciousness might be like a module that you add it and

[00:09.640 --> 00:17.000]  then suddenly it can have the concept of self.

[00:17.000 --> 00:20.120]  I think that's incorrect.

[00:20.120 --> 00:33.560]  I think that the self-concept can be learned like any visual concept or audio concept and

[00:33.560 --> 00:40.200]  to be able to have language and speak about yourself is something that a machine could

[00:40.200 --> 00:43.280]  do unconsciously.

[00:43.280 --> 00:57.680]  The machine could construct a language dialogue that describes its own, that describes, you

[00:57.680 --> 01:05.160]  know, because the machine, because the human being you have, you're not born with this

[01:05.160 --> 01:11.240]  representation of yourself, you know, at some point you don't have language, you have to

[01:11.240 --> 01:17.360]  learn language, but you see, but you have to figure out, you know, your eyes have to

[01:17.360 --> 01:18.360]  learn.

[01:18.360 --> 01:23.440]  If you look at the book Action to Perception, you know, the story is that some people who

[01:23.440 --> 01:29.880]  lost their eyesight regained it, but they had no eyesight for such a long time that

[01:29.880 --> 01:33.440]  when they regained their eyesight they didn't immediately understand what they were looking

[01:33.440 --> 01:34.440]  at.

[01:34.440 --> 01:42.760]  In some cases what they thought were black holes would look like, I'm sorry, would look

[01:42.760 --> 01:50.880]  like crazy black holes, later on their mind figured out that those crazy black holes were

[01:50.880 --> 02:00.360]  actually windows in another building across the courtyard or something like that.

[02:00.360 --> 02:06.880]  So there was a sort of like relearning, visual learning that over time through the action

[02:06.880 --> 02:18.120]  and curiosity of their own intention and attentional processes they were curious about their incoming

[02:18.120 --> 02:23.720]  sensory inputs to their eyes and they were, you know, continuing to look at those crazy

[02:23.720 --> 02:28.400]  black holes until they figured out that they were actually windows and they had rectangular

[02:28.400 --> 02:34.680]  shapes and that they had borders and that they were in the context of being parts of

[02:34.680 --> 02:41.400]  buildings and this was a sort of like visual conceptual buildup and I suppose that the

[02:41.400 --> 02:51.280]  same thing happens when someone's newborn that they may look around as if they understand

[02:51.280 --> 02:59.800]  what they're looking at but there's no reason to think that they do that, you know, that

[02:59.800 --> 03:07.120]  the restoration of sight to someone who hasn't had sight for a long time, sort of like paints

[03:07.120 --> 03:12.280]  a picture of what could be happening to newborns when they first come out of their womb and

[03:12.280 --> 03:26.640]  they look around in the world, they are not able to make the kinds of distinctions that

[03:26.640 --> 03:27.640]  adults can make.

[03:27.640 --> 03:38.080]  You know, their eyes can look and you might project that they understand what they're

[03:38.080 --> 03:46.960]  looking at but of course they can't, at that point there's no language they can't say anything.

[03:46.960 --> 03:57.280]  It's hard to say what's going on in the mind of the newborns but let's just stick with

[03:57.280 --> 04:06.240]  the idea that your eyes have to create visual models, your ears have to create auditory

[04:06.240 --> 04:11.520]  models, your audio, I'm sorry, your visual cortex and your audio cortex.

[04:11.520 --> 04:17.960]  Your brain has to create models of each of your sensory representations and at the same

[04:17.960 --> 04:22.640]  time receiving the sensory inputs is also receiving motor inputs and so it's creating

[04:22.640 --> 04:35.920]  a model of your own motor activity that includes the sounds that you make and the audio cortex

[04:35.920 --> 04:40.160]  helping to build a model of the sounds that you make, not just tracking your muscle movements

[04:40.160 --> 04:43.760]  but correlating the sounds that you make with your muscle movements and your eyes are tracking

[04:43.760 --> 04:51.160]  how your body moves in the visual scene so you can feel your arm moving, you're moving

[04:51.160 --> 04:57.720]  your arm, you're seeing your arm, you're hearing your arm when your arm collides with something

[04:57.720 --> 05:02.920]  or claps, you make a clapping sound with one hand.

[05:02.920 --> 05:05.920]  Have you ever done that?

[05:05.920 --> 05:07.920]  What exactly does that sound like?

[05:07.920 --> 05:20.360]  Okay, just listen for a second to your one hand clapping.

[05:20.360 --> 05:26.240]  Just listen to it.

[05:26.240 --> 05:39.360]  And yeah, so we have these models of the self and there's no, in principle there is no reason

[05:39.360 --> 05:45.840]  why a robot can't make a model of itself in addition to making models of the rest of the

[05:45.840 --> 05:51.520]  world and I think that's what Michael Graziano is missing is like it's not going to be like

[05:51.520 --> 05:59.400]  the consciousness module, you add it, so you have these sensory modules and you have to

[05:59.400 --> 06:07.680]  add a consciousness module so that part of your brain is just modeling, it's just now

[06:07.680 --> 06:13.440]  capable of modeling consciousness because it has this additional module.

[06:13.440 --> 06:24.920]  I'm going to veto that idea, I'm just kidding, it's not that it's definitely wrong, it's

[06:24.920 --> 06:43.360]  not an idea that I am in favor of because I think of, yeah, it's not, I think I have

[06:43.360 --> 06:52.240]  to go back to the idea of you have observer mechanisms, you don't have a central observer

[06:52.240 --> 07:10.760]  but you have observer mechanisms that correlate and so that's the OSC and yes, OSC is separate

[07:10.760 --> 07:22.760]  from the attention mechanisms are going to be separate from the observer effect because

[07:22.760 --> 07:40.680]  you have incoming frames of patterns but it might be that it could be that the TPJ represents

[07:40.680 --> 07:58.520]  some sort of peak in the hierarchy of both observers and attention, it's like when observers

[07:58.520 --> 08:05.040]  and attention collide, maybe that's the peak of the hierarchy of the brain in a sense and

[08:05.040 --> 08:10.920]  I know it sounds weird so you say okay well obviously the incoming senses, the primary

[08:10.920 --> 08:18.480]  sensory cortices which include the location of the TPJ, those are lower in the hierarchy

[08:18.480 --> 08:36.680]  and the prefrontal cortex is above it and argument is okay but maybe the signals from

[08:36.680 --> 08:46.800]  the prefrontal, because we want to say they are a lot more distant from the incoming senses

[08:46.800 --> 08:56.280]  but again going back to graph networks as applied to the networks of the brain, it could

[08:56.280 --> 09:21.280]  be that it could be that there's so much interchange and traffic that the real peak of the brain

[09:21.280 --> 09:29.400]  isn't the prefrontal parietal lobes but it's actually closer to the temporal parietal function

[09:29.400 --> 09:43.920]  in terms of information flowing from the prefrontal parietal back to the TPJ, again I don't know

[09:43.920 --> 09:59.240]  but we have some activity coordination across the corpus callosum and we have a lot of connected

[09:59.240 --> 10:14.760]  various, the phylamic bridges and Broca's and Werner's area, there's a lot of spots

[10:14.760 --> 10:34.760]  where the distally separated information seems to converge in a coordinated way, some areas

[10:34.760 --> 10:44.000]  seem to be more, there's a lot of silent cortical columns and the cortical columns are

[10:44.000 --> 10:53.480]  more, most of the connections are local and so therefore they're kind of in a silo but

[10:53.480 --> 11:01.280]  not completely and most of the connections that branch out from the cortical columns

[11:01.280 --> 11:08.600]  we're talking about, those are just in the upper layers, it's a fifth or sixth layer

[11:08.600 --> 11:22.400]  where the pyramidal cells are now reaching from the peaks of the siloed cortical columns

[11:22.400 --> 11:32.280]  to reach out and connect or network different cortical columns together, the network with

[11:32.280 --> 11:41.280]  long interneurons, they have different regions of the brain together, they form rich clubs

[11:41.280 --> 11:57.600]  and they form the default network and there's other anatomically described networks and

[11:57.600 --> 12:18.000]  so it seems that, it probably is the fact that the TPJ is at least one of the major

[12:18.000 --> 12:30.880]  crossroads for the convergence of attentional patterns and the higher more macroscopic levels

[12:30.880 --> 12:41.240]  of observation that are tracking patterns at a higher level, so my hypothesis was that

[12:41.240 --> 12:46.880]  because we have basically this, with anesthesia we have the impaired function of pyramidal

[12:46.880 --> 12:55.200]  cells and the pyramidal cells are linking together brain areas and so you have this lack of consciousness

[12:55.200 --> 12:59.960]  when brain areas are not linked together because the pyramidal cells are being inhibited, not

[12:59.960 --> 13:08.960]  inhibited but disrupted from functioning, so the brain is not linking together and there's

[13:08.960 --> 13:14.840]  a loss of consciousness function and a patient under anesthesia could have some of their brain

[13:14.840 --> 13:22.800]  lighting up, it's not global brain activity and so that's what I think of and so the idea

[13:22.800 --> 13:30.440]  that somehow it's just a module in the temporal pyramidal function that we could copy and put

[13:30.440 --> 13:34.200]  into a robot to the rest of the robot and now it has a conscious model so it moves from

[13:34.200 --> 13:39.360]  its representation of what's seeing, representation of what's hearing to its conscious module

[13:39.360 --> 13:52.040]  and now it can think of consciousness and that is just not really possible, it's not

[13:52.040 --> 13:55.520]  impossible but it's not possible, I mean it's like the reason I say that is because I think

[13:55.520 --> 14:25.440]  yeah you have to have this, you have to have large sequences of multi-scale interrelation

[14:25.440 --> 14:35.640]  related data across many different modalities in many different areas of the brain collaborating

[14:35.640 --> 14:43.280]  in huge sequences to achieve something like what we would sense as a human level or animal

[14:43.280 --> 14:54.840]  level being this and this is going to be something that the computers will be able

[14:54.840 --> 15:12.840]  to do because it is, but it's not going to be like a module, it's going to be like how

[15:12.840 --> 15:23.880]  because one of the key things is you have basically like this bubble with a lot of bubbles

[15:23.880 --> 15:34.520]  inside it just to oversimplify what it is, you have cells or they're at the bubble that

[15:34.520 --> 15:45.440]  is a 3D, that contains a 3D learning machine that learns and it's not just a learning machine,

[15:45.440 --> 15:52.040]  it's also a speaker, it's a learner and a speaker or it's a listener and a drawer so

[15:52.040 --> 15:57.120]  it draws and it listens, it's a observer and a transmitter, it's a receiver and a transmitter

[15:57.120 --> 16:04.960]  but it feels, it feels because they can create basically a feeling, let's just say a feeling

[16:04.960 --> 16:15.280]  is a multimodal association, so multimodal tracking and multi-modal correlation, so if

[16:15.280 --> 16:28.960]  you could take different kinds of data and you can track it, you can feel it, you can

[16:28.960 --> 16:33.920]  detect a pattern, you're basically like a feeling is a pattern that's being detected

[16:33.920 --> 16:39.560]  in a multi, it's a multi-dimensional pattern that's being detected, it has, it has feelings

[16:39.560 --> 16:48.080]  have characteristics, patterns, it's a temple of spatial characteristics, if you rub your

[16:48.080 --> 16:55.000]  finger on some cloth, it could be your jeans or your socks or maybe a flannel shirt, rub

[16:55.000 --> 16:59.880]  your finger on different materials, maybe the carpet or just the flat surface of the

[16:59.880 --> 17:09.560]  feelings have, you know, the feeling of a texture, of a textile has, maybe it has bumps,

[17:09.560 --> 17:14.240]  maybe it doesn't, maybe it's rough, maybe it's smooth, but those are temple of spatial

[17:14.240 --> 17:23.480]  characteristics that a grid of cells is modeling, it's modeling what it is, it's taking lots

[17:23.480 --> 17:33.760]  of moments of feeling and putting them together and compiling them so that, it's compiling

[17:33.760 --> 17:41.120]  them so that your experiences of that feeling develop the more you sort of tune in and pay

[17:41.120 --> 17:55.000]  attention to and continue to experience that feeling and they, you know, at some point,

[17:55.000 --> 18:02.480]  you know, when you're really, when you're paying attention to something, the multimodal

[18:02.480 --> 18:08.440]  nature of what, of paying attention to something, it's going to be engaging multiple areas

[18:08.440 --> 18:14.440]  of your brain simultaneously, like the visual cortex, the occipital cortex, I'm sorry, the

[18:14.440 --> 18:21.280]  occipital cortex is the visual cortex, but I mean the occipital, the parietal, the temporal,

[18:21.280 --> 18:26.000]  the prefrontal, different areas of the brain are going to be engaged when you're paying

[18:26.000 --> 18:31.920]  attention to something and, you know, so that's why, that's why I made the joke about being,

[18:31.920 --> 18:37.840]  the temporal parietal not being the module we can add on to make machines conscious,

[18:37.840 --> 18:53.640]  but instead being the LED of the consciousness in terms of an indicator that the, that enough

[18:53.640 --> 19:02.640]  of the global workspace across multiple regions has been, you know, has been activated in

[19:02.640 --> 19:09.640]  is, is interlacing and is communicating. It could be an indicator of crosstalk between

[19:09.640 --> 19:16.640]  the senses, which actually makes sense. That actually makes sense because when you're paying

[19:16.640 --> 19:24.880]  attention to something, there is a convergence of your sensory activities. And so if you're,

[19:24.880 --> 19:29.240]  if you're really paying attention to someone, that means you're bringing your, your visual

[19:29.240 --> 19:35.560]  representations and alignment with your auditory representations and alignment with your somatosensory

[19:35.560 --> 19:42.760]  representations. And that probably resulting, would result in some peak activity in your

[19:42.760 --> 19:49.520]  temporal parietal junction or TPJ because it's right at the center of your incoming

[19:49.520 --> 19:58.800]  of, of the primary sensory cortices. And so in that, in that sense by itself, like, you

[19:58.800 --> 20:11.120]  could say that thinking that of the TPJ as the, as the, the consciousness module sort

[20:11.120 --> 20:22.080]  of obscures an alternative idea in which you're just thinking of it as a symbol that your,

[20:22.080 --> 20:27.360]  that your primary sensory cortices are working together at the highest level, which is, which

[20:27.360 --> 20:41.080]  is going to be a good correlation of multi-sensory awareness or multi-sensory attention. And

[20:41.080 --> 20:45.440]  so you could, I guess you could argue that, yeah, we have visual, we have attention at

[20:45.440 --> 20:58.160]  every level of, of neuroanatomy. And, and so it could, it could, it could just be that

[20:58.160 --> 21:04.440]  because you notice like attention can change, like you use smoke pot, some people become

[21:04.440 --> 21:10.120]  very detail focused. They start focusing on details obsessively, like little stuff. And

[21:10.120 --> 21:17.600]  so you start to wonder if there's other kinds of ways to alter the mind and what you're

[21:17.600 --> 21:25.720]  focusing on. So, you know, some, some people have had experiences where they'll take a

[21:25.720 --> 21:31.600]  psychedelic medication and they'll, and the, their focus will get very broad. Now start

[21:31.600 --> 21:42.320]  thinking of things in them on a longer timeline. Let's start thinking of, of, you know, of,

[21:42.320 --> 21:47.520]  like maybe look down at your hand and you can see a hundred years of history in your

[21:47.520 --> 21:58.480]  hand or in the clouds. You see speakers going back generations, ancestors, great minds.

[21:58.480 --> 22:06.480]  And this is like, this is a total opposite of, you know, your mind is, your mind's perspective

[22:06.480 --> 22:18.520]  on time is expanding versus your, your attention contracting. And, um, so anyway, going back

[22:18.520 --> 22:26.560]  to like, yeah, I think maybe the TVJ represents the convergence of the primary sensory courtesies

[22:26.560 --> 22:34.840]  in terms of like attention exists at all levels. And, and, but therefore like, um, the reason

[22:34.840 --> 22:42.120]  the top of the hierarchy sort of dominates your conscious attention and you are not necessarily

[22:42.120 --> 22:48.520]  uh, you know, your consciousness may not, it's not that, it's not like we want to make

[22:48.520 --> 22:53.880]  an argument like, yeah, well, well maybe it's only the animals with bigger brains that have

[22:53.880 --> 22:57.680]  consciousness, but like, how do you explain how a roach operates? How do you explain

[22:57.680 --> 23:05.520]  how a mouse operates or a dolphin, right? And all these different, uh, animals or like

[23:05.520 --> 23:15.240]  or, you know, um, insects that have, um, some sort of, uh, operation and I, and I get that,

[23:15.240 --> 23:22.160]  I get that, you know, we want to, we want to believe that I want to believe that fish

[23:22.160 --> 23:32.360]  are not conscious, but, but it's like, okay, well, what, what, why is, um, why is the consciousness

[23:32.360 --> 23:40.680]  at a high level? Why is it happening at a high level? And it could be that sort of like,

[23:40.680 --> 23:46.200]  going back to basically the attention schema theory, it could just be that the stuff that

[23:46.200 --> 23:52.360]  is inside consciousness at a high level, like I'm consciously aware of stuff, it's stuff

[23:52.360 --> 23:59.440]  that is just, um, I mean, I, I guess this is covered already in the theory, like it's

[23:59.440 --> 24:05.240]  like, it's the stuff that's dominating other stuff. It's, it's the patterns that outcompeted

[24:05.240 --> 24:13.400]  other patterns in a, in a way that moved up to the higher levels. But then, and you have,

[24:13.400 --> 24:19.120]  you know, you have this sort of competition that patterns go through to become the pattern

[24:19.120 --> 24:25.760]  that is, is dominant in the architecture of the mind. They're competing against each

[24:25.760 --> 24:34.840]  other. Um, they're competing to take up space in your mind. I mean, it's one way to look

[24:34.840 --> 24:46.480]  at it. Um, trying to say these patterns have their own agency is a little funny. But, uh,

[24:46.480 --> 24:55.040]  you know, I would say maybe it's the moment, it's the momentum of past patterns that, uh,

[24:55.040 --> 25:01.480]  and perhaps also the momentum of, of, um, the memory of what, of your past that you've

[25:01.480 --> 25:13.280]  learned in terms of what your ancestors did and that, that drives you and, um, drives

[25:13.280 --> 25:23.640]  your neural patterns, right? Your memories, uh, are drivers. And because that, because

[25:23.640 --> 25:37.320]  memories have their own, like, momentum, right? And, and, um, well, yeah, I guess I have a

[25:37.320 --> 25:43.000]  big disagreement, Michael, across the honor, because I don't think of neural consciousness

[25:43.000 --> 25:50.000]  as just like some module that we can just slap onto the back of a, of a robot and then suddenly

[25:50.000 --> 25:56.880]  it's going to be able to talk about it. So if that's, I don't want to say that's ridiculous,

[25:56.880 --> 26:10.120]  but that's what I'm thinking. Um, I think that, uh, that the, if you, if you look, I

[26:10.120 --> 26:18.280]  think that the brain has, is taking turns rendering on a large scale, the brain is taking turns,

[26:18.280 --> 26:24.240]  um, just like, just like, okay, so like when you think of how a computer monitor works,

[26:24.240 --> 26:36.240]  right? A computer monitor, a computer is, is sending, uh, an image to the screen, uh,

[26:36.240 --> 26:41.720]  one pixel at a time, one line at a time, like when, when, when a screen, when a television

[26:41.720 --> 26:50.880]  screen renders, uh, an image, it renders one line at a time, but it does it so fast that

[26:50.880 --> 26:58.400]  you see, when you, when you look at a computer screen, you, you see, um, an image in the

[26:58.400 --> 27:05.200]  computer screen, maybe you see a moving image, maybe it's a bit, maybe it's a video or a

[27:05.200 --> 27:12.600]  video, and, um, but that, but that is made up of like one line being drawn at a time across,

[27:12.600 --> 27:15.980]  one line goes across the top, and then another line underneath it, and then another line

[27:15.980 --> 27:20.640]  underneath it, like if you slow down a computer screen, a computer screen is just doing one

[27:20.640 --> 27:26.800]  line at a time, and then when it gets to the bottom, it goes back up to the top and starts

[27:26.800 --> 27:35.560]  drawing the next frame, and it has to do, like if you, if you're watching a, um, a film

[27:35.560 --> 27:43.160]  that is, uh, 40 frames per second or playing a video game, that's 120 frames per second,

[27:43.160 --> 27:52.720]  um, that means that, that's, uh, for each of those frames that happen in, in each second,

[27:52.720 --> 27:57.520]  there, there, what, there's still like, you know, one line is being drawn at a time, that

[27:57.520 --> 28:03.800]  line is being drawn so fast, that you don't perceive what, that you don't, you, you typically

[28:03.800 --> 28:10.600]  don't perceive that one line is being drawn at a time, you just see the movie, you just

[28:10.600 --> 28:17.200]  see the whole image transforming, and, um, and that's what I'm suggesting that the whole

[28:17.200 --> 28:24.000]  brain is doing in a distributed way, is that the, um, you know, the, what, what Michael

[28:24.000 --> 28:31.840]  Bryson and I might refer to as the attention schema, is basically, um, the neurons that

[28:31.840 --> 28:39.880]  are activated, versus the neurons that are inactivated, but in a global way, each part

[28:39.880 --> 28:50.360]  of the brain is, is going to sort of, um, be writing a different line of the pattern

[28:50.360 --> 28:55.080]  that represents the, the sequence of things that we consider when we're conscious, and

[28:55.080 --> 29:02.600]  so of course the visual stuff, the screen, uh, uh, uh, that we see is going to start

[29:02.600 --> 29:10.520]  in the, in the B1 area, and like, like just on a visual level, that's just all it needs

[29:10.520 --> 29:11.520]  to be.

[29:11.520 --> 29:15.480]  It is, you know, basically creating basic models, but what those models mean, the meaning

[29:15.480 --> 29:22.720]  of those models, um, that is, that's, that's another thought that takes up another line

[29:22.720 --> 29:32.560]  on the monitor, and so that's how you, you see, the brain is like a, it's like a distributed

[29:32.560 --> 29:37.800]  rendering, a distributed drawing, think of the consciousness of distributed drawing,

[29:37.800 --> 29:45.400]  where each part of the brain gets to render a different, uh, pixel, and each oscillator

[29:45.400 --> 29:51.280]  gets to, gets to, um, and the different pixels are like dark, it's like one line, like you

[29:51.280 --> 29:57.080]  have a whole bunch of, so you have a quart of comb, and each quart of comb in the, in

[29:57.080 --> 30:10.400]  the neocortex is representing one line of the screen of the mind, and, um, and then

[30:10.400 --> 30:15.400]  what happens is the oscillator, where's all the neurons that didn't fire, that are basically

[30:15.400 --> 30:19.520]  listening and predicting when they're gonna fire, because they're collecting little electric

[30:19.520 --> 30:26.920]  surges, actually, I mean, they're collecting, um, phase differentials, right, they're,

[30:26.920 --> 30:34.080]  they're, uh, like, you know, either they're receiving neurotransmitters in a very regular

[30:34.080 --> 30:43.960]  way, um, or they're tracking basically novel, uh, patterns, and predicting when, when they're

[30:43.960 --> 30:50.520]  going to fire, which is going to allow them to activate in a sequence, in a playback sequence,

[30:50.520 --> 31:01.400]  and so that observation, that observation, um, is something that the entire, um, the

[31:01.400 --> 31:06.400]  observation that a single neuron is doing in order to predict when it's going to fire,

[31:06.400 --> 31:13.400]  um, is something that's also being done by the network, or the oscillator, or the, what

[31:13.400 --> 31:19.000]  they're calling, so yes, so we have, so we have predictive coding at the individual neuron

[31:19.000 --> 31:25.920]  level, but we also have predictive coding at the oscillator, uh, level, at the cortical

[31:25.920 --> 31:38.640]  column level, and, uh, we have predictive coding at the, um, larger scale of, of different

[31:38.640 --> 31:45.640]  brain regions, and so the, so the interneurons are participating in predictive coding of,

[31:45.640 --> 31:54.320]  um, you know, firing at, at the greatest level, and that, and so what I'm saying is, like,

[31:54.320 --> 32:01.480]  at all levels, we're, you know, at the micro level, we're deciding on individual pixels,

[32:01.480 --> 32:08.840]  and, uh, at the cortical column level, which is also the same as the oscillator level,

[32:08.840 --> 32:17.480]  you know, the meso level, we have, uh, we're doing lines, so pixels, lines that we're rendering,

[32:17.480 --> 32:25.400]  uh, and then at the global level, that's like, okay, the global level is like, um, the cortical

[32:25.400 --> 32:36.240]  columns are going to play back in sequences, so that it's like one line gets filled in,

[32:36.240 --> 32:40.560]  and the next line, and the next line, and the next line, and then you have the computer

[32:40.560 --> 32:49.160]  has now rendered an entire frame for your eyeball, but it's not for your, it's not for

[32:49.160 --> 32:57.240]  your eyeball, it's a set, it's you have, um, not a singular observer, but you have, um,

[32:57.240 --> 33:07.880]  the network itself being an observer, you have a distributed observer, you have many

[33:07.880 --> 33:15.280]  small observers that work together to create the illusion of the singular observer, because

[33:15.280 --> 33:23.480]  the many small observers are, uh, producing, uh, together, collectively, they're producing

[33:23.480 --> 33:52.760]  a common, a common pattern.

Colab paid products - Cancel contracts here

145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241

Yeah, so Michael Grasio thinks that consciousness might be like a module that you add it and

then suddenly it can have the concept of self.

I think that's incorrect.

I think that the self-concept can be learned like any visual concept or audio concept and

to be able to have language and speak about yourself is something that a machine could

do unconsciously.

The machine could construct a language dialogue that describes its own, that describes, you

know, because the machine, because the human being you have, you're not born with this

representation of yourself, you know, at some point you don't have language, you have to

learn language, but you see, but you have to figure out, you know, your eyes have to