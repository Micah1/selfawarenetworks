b0327y (time codes included)

Audio transcription is by OpenAI's Whisper, 

Audio Recorded on Mar 25, 2021

Title: The flow of information in the brain

Original Recording https://recorder.google.com/b612bace-0639-419d-990a-0f16fd0552f5 

[00:00.000 --> 00:21.520]  It like literally is like some on-off signal that just like moves away like it's the you know like the

[00:21.520 --> 00:30.800]  robotic leg anyways it was just a completely complete disaster but so so yeah so obviously I

[00:30.800 --> 00:35.280]  don't think EEG is the way to go I mean with EEG I think the best you're gonna get is like a click

[00:35.280 --> 00:40.200]  and and that's I mean I think that's you can make something now with it like if you have an eye

[00:40.200 --> 00:45.200]  tracker and eye trackers are amazing you can have an eye tracker be like the cursor on your

[00:45.200 --> 00:50.920]  goop on your lens that's like on your glasses for instance and then just have EEG on like just a

[00:50.920 --> 00:56.520]  two probe EEG just clicking on or off and maybe even control the contrast to see how like

[00:56.520 --> 01:01.880]  transparent your screen is gonna be or something but like I think the real way to go is I think

[01:01.880 --> 01:10.360]  intercranial PCIs are going to like be like a thing that people are all like all on and I think

[01:10.360 --> 01:15.920]  integrating that with the AR VR world is just gonna be like it's gonna get rid of our iPhones

[01:15.920 --> 01:20.360]  and it's just gonna be the new thing but that's just my perspective and I had this perspective

[01:20.360 --> 01:25.200]  before I went back to school and I and so I've been studying neuroscience with the strong intent

[01:25.200 --> 01:31.960]  of like finding out how the thalamus communicates to the cortex because the best way to sort of

[01:31.960 --> 01:36.680]  evoke some sort of artificial perception would be the real way that perception happens so I

[01:36.680 --> 01:43.280]  wanted to learn like how what's the real way the thalamus sort of sends the perceptive signals to

[01:43.280 --> 01:49.960]  the cortex so I could emulate that since like electronically or with biosynthetics which I've

[01:49.960 --> 01:55.680]  been like keeping my feet in the in the world pretty strongly in the same time because I don't

[01:55.680 --> 02:00.760]  think at the end of the day years from now it's not gonna be metal electrodes that are handling

[02:00.760 --> 02:04.440]  this sort of resolution that we need it's gonna be like bioelectronics anyways do you have any

[02:04.440 --> 02:14.960]  thoughts on that stuff absolutely I've tons of thoughts and also questions for you so so one

[02:14.960 --> 02:22.160]  of the thoughts is okay so you so you know that gosh I'm still just starting for a full memory

[02:22.160 --> 02:28.880]  recall I should have written this down while I was thinking about it but so one of the thoughts

[02:28.880 --> 02:36.080]  hang on let's see sorry my memory god damn today is today is one of those days when my memory is

[02:36.080 --> 02:41.720]  just just not super great okay so we're talking about let's try to trigger my memory a little bit

[02:41.720 --> 02:46.960]  we were talking about brain-computer interfaces and EEG and the limitations you're asking about

[02:46.960 --> 02:50.320]  the limitations of EEG that's one of the things what is it what is the ultimate

[02:50.320 --> 02:59.000]  league that is good that you would let me break it into two pieces yeah so the future of BCI let's

[02:59.000 --> 03:04.680]  say is one piece and that's like the word you have all the ideals we figured out how the

[03:04.680 --> 03:09.040]  phalamus is communicating in the cortex we're able to emulate it we're able to use bioelectrodes

[03:09.040 --> 03:13.520]  to just get full result there are tons of resolution and get your four degrees of freedom

[03:13.520 --> 03:19.000]  or whatever and then on the flip side for right now because if you're gonna start a business you

[03:19.000 --> 03:23.680]  gotta you gotta have some have it do something and so what can you make work now and on that side

[03:23.680 --> 03:30.720]  you can do stuff with eye trackers and EEG with like an on-off signal you could say it's a click

[03:30.720 --> 03:35.240]  hold down on the mouse not hold down on the mouse kind of signal and then the mouse movement you

[03:35.240 --> 03:39.760]  could just use an eye tracker and track the pupils but like you know so those were like two

[03:39.760 --> 03:43.560]  areas you have the simplistic like what can we do right now and then I was also on track with

[03:43.560 --> 03:49.160]  it like where is it gonna be in 12 years okay so so here so I remembered now the interesting

[03:49.160 --> 03:53.440]  points that I wanted to make and I and thankfully while you were talking I wrote them down

[03:53.440 --> 04:03.520]  all right so okay so there was this interesting study that just came out where in video they

[04:03.520 --> 04:10.520]  developed a radically different way to compress video for video chats and this just this is

[04:10.520 --> 04:20.200]  November 19th 2020 and the story is that they need about they need a couple seconds of real

[04:20.200 --> 04:29.720]  video and then they're going to use the neural network to render an artificial video that

[04:29.720 --> 04:37.680]  can that's so it's so good that it can predict all your facial expressions after that first

[04:37.680 --> 04:42.640]  few seconds and based upon you know just your voice coming through and and also your head

[04:42.640 --> 04:48.600]  rotating it can it can you know sort of generate your head rotation and it can be used you can

[04:48.600 --> 04:53.280]  substitute your face for someone else's face and keep talking and you know just like you've seen

[04:53.280 --> 05:00.720]  in the sort of online the deep fake stuff it does all that in one simple algorithm and and but

[05:00.720 --> 05:07.960]  importantly that they basically have in effect created a video compression that works with in

[05:07.960 --> 05:15.240]  10 in 10 at 10x improvement so it's like that means that it's that you need 10 times less

[05:15.240 --> 05:21.080]  bandwidth to do a video call and and so that means that older phone technology that wasn't

[05:21.080 --> 05:26.600]  capable of video calls before because it had the bandwidth of the network was too slow is now

[05:26.600 --> 05:38.960]  capable of a video calling and that's profound that's profound just by itself but then you know

[05:38.960 --> 05:49.160]  so what I know about EEG is that just just from talking to a very particular neuroscientist who's

[05:49.160 --> 05:57.880]  you know for a long time who you know he's been a neuroimager for like 30 years and he is you

[05:57.880 --> 06:02.320]  know like he's looking at a world where the universities want to fund MRI research because

[06:02.320 --> 06:09.120]  you know MRI machines print money and they don't want to fund EEG and so you have to argue really

[06:09.120 --> 06:16.480]  hard to do EEG stuff in the research world and and you know or to get funding for a major

[06:16.480 --> 06:21.200]  EEG project but he but he's you know seeing and has pointed out to me that there's a whole

[06:21.200 --> 06:29.320]  bunch of stuff that's never been done before with EEG that if there you know for one example was

[06:29.320 --> 06:38.480]  that you know the the the Chodmite Institute data is this massive collection of data that combines

[06:38.480 --> 06:44.600]  basically you know eye tracking with EEG with voice tracking with behavior tracking but it does

[06:44.600 --> 06:52.520]  so in a in a way that they've created a sort of standardized data collection and anyone who can

[06:52.520 --> 06:58.560]  get access to that data who who can potentially extend that data by making sure that they adhere

[06:58.560 --> 07:05.200]  to the same data collection standards and so if you had a massive global project where you asked

[07:05.200 --> 07:12.440]  every EEG research laboratory in the world to start following this exact same data collection

[07:12.440 --> 07:17.160]  standard for EEG then you'd have something that doesn't exist today where today you can't combine

[07:17.160 --> 07:22.480]  all the EEG research that's ever been done into a single study because the method that the methods

[07:22.480 --> 07:32.120]  of of and of doing the EEG capture varied wildly but also in some cases they're poorly documented

[07:32.120 --> 07:40.640]  and absolutely yeah I've been in a EEG lab before or actually I've been in a lab a computational

[07:40.640 --> 07:48.160]  neuroscience lab that relied on trying to get EEG data from other labs and it was just a disaster

[07:48.160 --> 07:56.680]  so my thinking is that you know we were just talking we're just talking about there is these

[07:56.680 --> 08:01.840]  some there's old like you know there's been these sort of I'm starting to think of them as like

[08:01.840 --> 08:08.080]  added bad attitudes where you you'll have for decades or more the entire neuroscience community

[08:08.080 --> 08:14.240]  or psychiatrist neuropsychiatry community or psychology community well we'll have these like

[08:14.240 --> 08:25.080]  ideas that they're all attached to that turn out to be you know wrong and and but then but then

[08:25.080 --> 08:29.640]  like the precursors for what was right was sort of like was sort of partially there the whole time

[08:29.640 --> 08:34.840]  and so then they'll be like these massive shifts anyway what I'm what I'm what in terms of like

[08:34.840 --> 08:40.160]  what is the the ideas that are dominant you know like with AI I guess it could be an example of how

[08:40.160 --> 08:44.800]  I don't know if we need an example I mean you know this stuff there's there's been AI winters

[08:44.800 --> 08:49.920]  where there was a ton of hype and enthusiasm for certain you know for certain kind of AI for

[08:49.920 --> 08:54.800]  eat for neural networks and then all of it died and then there was a ton of hype and enthusiasm

[08:54.800 --> 09:00.880]  for symbolic neural net for symbolic AI and then that died and then you know hot and then now we're

[09:00.880 --> 09:09.120]  still in the deep learning era but there's been like three or four AI winters I'm not sure depending

[09:09.120 --> 09:19.360]  on but but and then there's okay let me just pause for a second because I remember somebody

[09:19.360 --> 09:24.920]  in my program the person in my program is like that more of the expert on AI he does like artificial vision he

[09:24.920 --> 09:30.120]  he's like you know like this I mean neuro or like bio-neuro so he's like the only one he's like on

[09:30.120 --> 09:35.560]  the AI side and he was telling me all the differences between symbolic and people were all

[09:35.560 --> 09:40.120]  about that but now deep learning just kind of just massively kills everything else and how good it is

[09:40.120 --> 09:45.840]  but it's not as it's getting further and further from biological like the most biological would

[09:45.840 --> 09:50.360]  have been recursive neural networks but like for language processing at least transformal neural

[09:50.360 --> 09:55.880]  networks and I watched the three brown blood one blue on that so I roughly understand it like you

[09:55.880 --> 10:01.640]  know what I mean like it you don't need recursive neural networks and highly analogous to the

[10:01.640 --> 10:11.080]  biological system for it to be good at what it does it was last time so you don't need you don't

[10:11.080 --> 10:18.840]  need the AI to be close to the actual brain for it to be good at what it does like the closest

[10:18.840 --> 10:22.880]  would have been like recursive neural networks but those aren't as great as some of the other stuff

[10:22.880 --> 10:30.880]  around oh gosh yeah yeah I mean I mean all this stuff is in my view is pretty far from the brain

[10:30.880 --> 10:37.680]  I mean I guess you know like I like I wrote so I wrote this article on my website about a new

[10:37.680 --> 10:43.560]  study that came out about multi vesicular release being a lot more common throughout the neocortex

[10:43.560 --> 10:49.640]  with it turns out there's like they people thought this is something that was like multi

[10:49.640 --> 10:56.080]  vesicular release something that was constrained doing MVPs with multi with multiple vesicles in

[10:56.080 --> 11:08.240]  them so again are you talking about MVPs multi vesicular bodies I yes so yeah we're talking

[11:08.240 --> 11:14.760]  in the in the in the axon terminal right well the axon terminal has has vesicles and they get

[11:14.760 --> 11:20.080]  released and you know with us with a spike with an action potential they release you know one or

[11:20.080 --> 11:24.160]  two or five or whatever at a time but there's also these things called multi vesicular bodies

[11:24.160 --> 11:29.760]  which is kind of a new thing that people know about huh okay I didn't know I didn't know this one

[11:29.760 --> 11:37.360]  go ahead and tell me more oh so multi vesicular bodies hold multiple vesicles and they'll also

[11:37.360 --> 11:43.400]  take some of the other internal like from the inside the cell the from the cytoplasm and they'll

[11:43.400 --> 11:47.720]  you know because there's mRNA floating around there's a bunch of stuff floating around so it'll

[11:47.720 --> 11:53.440]  encapsulate multiple vesicles which have the neurotransmitters in them but it'll also encapsulate

[11:53.440 --> 11:59.600]  some other stuff and then those get released and they're thought to like transfer stuff from one

[11:59.600 --> 12:06.920]  cell to stuff to another cell so like a cell that never had any of like should never have RNA from

[12:06.920 --> 12:12.560]  like you know like because this other cell type makes the RNA for it but not the cell but it can

[12:12.560 --> 12:19.240]  somehow transfer across cells that's the multi vesicular bodies stuff that's it came out a few

[12:19.240 --> 12:30.400]  years ago that's cool that looks like that there's um okay yeah that's cool so so no so I was talking

[12:30.400 --> 12:38.280]  about how so going back to AI so what I was pointing out was that the the core at the core of you

[12:38.280 --> 12:43.520]  know basically whether you're doing like computational neuroscience or spike modeling or you're

[12:43.520 --> 12:49.560]  talking about you know you know deep learning which is which is um you know you have you have the the

[12:49.560 --> 12:56.280]  basic let me see if I can find my article I'm gonna go to my website real quick so SVGN I'm

[12:56.280 --> 13:03.000]  actually gonna go to vrma.io that's my other page with my featured articles on it and it's the

[13:03.000 --> 13:08.120]  article that says it's called synaptic unreliability what is it called what's the recipe so if you

[13:08.120 --> 13:15.760]  go to vrma.io and look at look at the the article that's called so under the featured article

[13:15.760 --> 13:26.680]  section like the third article down says synaptic unreliability hold on I'm just gonna leave my

[13:26.680 --> 13:37.760]  mic on while I do this yeah go ahead okay third at WebAR digital fashion synaptic unreliability so um so

[13:37.760 --> 13:45.800]  this this basically like what the widespread idea was that in in machine learning for a long time I'm

[13:45.800 --> 13:54.640]  sorry in neural networks was was an idea that um that a neuron would be summed up by by a spike

[13:54.640 --> 14:02.560]  like an on or off signal and then and then later on they had and that was called um this this um

[14:02.560 --> 14:08.680]  this idea was fundamental um wait this isn't it so now we're talking about an AI or a note right

[14:08.680 --> 14:22.720]  yeah we're talking about specifically it's called yeah well we're talking about both

[14:22.720 --> 14:28.600]  neuroscience and machine this article is about both neuroscience and machine learning um it's in

[14:28.600 --> 14:35.040]  the beginning it talks about um machine learning but the rest of the article is neuroscience okay so

[14:35.040 --> 14:42.280]  it's only machine learning in the beginning but but so there's um there's a concept in called the

[14:42.280 --> 14:51.680]  um why does my in the basically that the model of a neuron I think is called like a positron or

[14:51.680 --> 14:59.600]  something I'm trying to remember what the what it's called not positron it's like a basic

[14:59.600 --> 15:15.800]  the computation unit in an artificial neural network it is a let me type in the word positron

[15:15.800 --> 15:22.240]  I think it's just neuron but that just gets confusing when we're gonna switch back and forth

[15:22.240 --> 15:31.480]  so um well I think the artificial I think it is yes no people can put no positron or something

[15:31.480 --> 15:39.680]  else there's um okay so let's just call it the artificial neuron then for the sake of brevity

[15:39.680 --> 15:46.400]  so in any case the artificial neuron was like saying that the action potential was summed up

[15:46.400 --> 15:58.440]  and and that created this sort of like you know the probability of they're trying to like you

[15:58.440 --> 16:02.600]  know basically like simplify they're saying okay we need to put this into machine and we just

[16:02.600 --> 16:06.160]  need to simplify it because there's too much complexity here so we're gonna that the word

[16:06.160 --> 16:10.440]  we're gonna like you know cut off the the complexities just we're just gonna say okay

[16:10.440 --> 16:16.960]  everything they get summed up and so you imagine that no matter what the the dendrite is is thought

[16:16.960 --> 16:24.000]  to have learned if you have to reduce it to a single you know on or off then then potentially

[16:24.000 --> 16:28.720]  you're losing a lot of information and later on they made an adaption to where you know it's

[16:28.720 --> 16:36.240]  um like it it can there's like um if it crosses it if it's it's on but it's above a certain threshold

[16:36.240 --> 16:42.800]  then you can you can have like a vector that represents the the magnitude above that threshold

[16:43.760 --> 16:48.640]  so they expanded the the depth of of what a single neuron could do what a single artificial

[16:48.640 --> 16:56.320]  neuron could could transmit so its output isn't is no longer uh one or zero it's like it can be

[16:56.320 --> 17:04.240]  like uh something over time it's um well so it's it's it's it's it's it's only like there's a one

[17:04.240 --> 17:09.040]  or zero in terms of whether it's activated but then beyond that artificial neuron has a is represented

[17:09.040 --> 17:16.400]  by a vector which is just um greater or or less than the um it's either closer to to zero or closer

[17:16.400 --> 17:26.640]  to one uh in in terms of um so so it sort of gives some sort of magnitude to the um to the neural

[17:26.640 --> 17:36.480]  network but so the single artificial neuron uh output is what i was asking yeah so that so that

[17:37.760 --> 17:43.200]  the um well the output has to has to stem from from that uh you know whether it's it's but

[17:43.200 --> 17:48.560]  basically it's between zero and one it's it's so it's either it's it's not just zero or one it's

[17:48.560 --> 17:54.800]  like this vector between zero and one and uh so um so it's a little bit more complex but a multi

[17:54.800 --> 18:00.480]  a multi vascular release so the rest of it so beyond that is um the idea is that a multi vascular

[18:00.480 --> 18:11.520]  release um could uh would make neuron make neurons a lot more uh computationally um a lot

[18:11.520 --> 18:17.680]  more capable of passing along information than than the um then idea of neurons that are sort of

[18:17.680 --> 18:23.920]  summing everything up in in the in the action potential in a simplified way because because

[18:23.920 --> 18:29.440]  you have i mean the idea is that so i mean the article sort of covers how it works but and i'm

[18:29.440 --> 18:38.160]  not sure 100 if um if this article is correct because i sort of um it's i'm sort of putting

[18:38.160 --> 18:44.720]  together a lot of different pieces of information but basically um i put together that uh based on

[18:44.720 --> 18:49.920]  others others research that it looks like the the changes in the um there's there's research to say

[18:49.920 --> 18:57.840]  the changes in the uh for example in in k or potassium are going to change the um they're

[18:57.840 --> 19:04.960]  going to affect the amount of uh um the pre-snaptic k potassium modifies the action potential amplitude

[19:04.960 --> 19:12.400]  uh or ap sin which determines the strength of the synaptic signal including the amplitude of the

[19:12.400 --> 19:19.040]  post-synaptic response and then that leads to how many uh vesicles can be released per action

[19:19.040 --> 19:23.520]  potential which is between usually between zero and three i guess i could go to five as you said

[19:23.520 --> 19:29.120]  but but the average uh number i was just making numbers up but yeah that's like so so here's

[19:29.120 --> 19:36.320]  what i think you're talking about so um so an action potential is so uh cell is that right

[19:36.320 --> 19:42.240]  so i get minus 65 millivolts uh you get it up to a threshold which let's say it's for the

[19:42.240 --> 19:47.440]  our example cell we'll say minus 40 which is roughly what they usually are uh then you get

[19:47.440 --> 19:53.280]  to but then the spike happens where it just shoots way up and this is happening because

[19:53.280 --> 19:57.680]  there's channels that are just letting sodium in and sodium being positive it's it's gonna

[19:57.680 --> 20:02.800]  depolarize the cell shoot the action am i speaking like shit that you already know

[20:02.800 --> 20:08.560]  pretty well no keep going i'm okay okay so like it's letting a bunch of sodium in

[20:08.560 --> 20:13.120]  and all this positive charge is coming in and it's staying near the membrane which is like

[20:13.120 --> 20:18.400]  basically shooting the voltage up then you have the voltage sensitive channels so that's like the

[20:18.400 --> 20:25.360]  positive feedback loop it gets positive so it opens to let these are so more sodium channels

[20:25.360 --> 20:30.400]  their voltage sensitive sodium channels so like as it gets through negative 40 it's like oh okay

[20:30.400 --> 20:34.560]  now we can open and so it's like positive feedback loop now there's tons of sodium coming in

[20:34.560 --> 20:40.080]  and it's just shooting up up up so it's going closer to zero and then 10 20 it goes up to like

[20:40.080 --> 20:45.920]  positive 40 millivolts but what's happens there is you have this these potassium channels these

[20:45.920 --> 20:51.920]  k channels that open and start doing the opposite and the reason why you're thinking wait this is

[20:51.920 --> 20:57.600]  positive wait wouldn't that be more positive current yes except there's a a gradient and so

[20:58.160 --> 21:03.280]  while there's more sodium outside the neuron than there is inside the neuron there's way more

[21:03.280 --> 21:07.920]  potassium inside the neuron than outside the neuron so if you're going to open a potassium

[21:07.920 --> 21:13.040]  channel the positive charge is going to go the other way and and basically end up with a net

[21:13.040 --> 21:18.400]  negative charge it would be like a negative charge is entering the cell yeah although it's

[21:18.400 --> 21:25.360]  positive leaving anyways so so that potassium that k current is hyperpolarizing the cell back to where

[21:25.360 --> 21:29.440]  it was and it even over overshoots it a little bit and that's called the after hyperpolarization

[21:29.440 --> 21:36.800]  but but the the timing that these things come on are so synchronized and so uh there there's

[21:36.800 --> 21:42.400]  there there's so fast it results in a spike that lasts less than a millisecond so these

[21:42.400 --> 21:47.360]  potassium currents start shooting up and before they're even done shooting up the i'm sorry i said

[21:47.360 --> 21:52.240]  that backwards the sodium currents start shooting the voltage of the cell up and then before it's

[21:52.240 --> 21:57.200]  even done shooting up the potassium currents have turned on and start shooting it back down

[21:57.200 --> 22:02.720]  to the point where you just get this in super quick spike and then it goes back down now the result

[22:02.720 --> 22:08.960]  of the spike the spike transmits down the axon which at the very tip is where the vesicles are

[22:08.960 --> 22:15.440]  and at the very tip of the axon you have these calcium channels and calcium is something that

[22:15.440 --> 22:20.800]  needs to come in so calcium is one of the things where there's like a gazillion times more outside

[22:20.800 --> 22:25.920]  the cell than inside the cell so if a calcium channel opens the the force that's shoving the

[22:25.920 --> 22:31.760]  calcium in the cell is just soup is like the strongest you're going to get but but uh these

[22:31.760 --> 22:37.120]  calcium channels on the very tip of the axon they're voltage gated and they're not not going to open

[22:37.120 --> 22:44.720]  unless unless it's super positive voltage so basically they only open during a spike so that's

[22:44.720 --> 22:50.960]  split second that one millisecond of a spike that finally reaches the tip of the axon gets this

[22:50.960 --> 22:55.920]  calcium to come in and when that calcium comes in it binds to something called synaptopagmin

[22:55.920 --> 23:02.160]  synaptopagmin shut basically shoves the vesicle into the membrane and allows the neurotransmitter

[23:02.160 --> 23:07.840]  to spit out into the synapse and that's that's the formation of neural communication so that

[23:07.840 --> 23:16.320]  split one millisecond split positive uh mill voltage that reaches the uh that reaches the end

[23:16.320 --> 23:22.000]  of the axon allows that calcium to come in now if you fuck with the potassium and you don't

[23:22.560 --> 23:28.000]  let it come on as fast as it would normally what you end up doing is you end up spreading that

[23:28.000 --> 23:32.960]  action potential instead of being just a split one millisecond event you by fucking with the

[23:32.960 --> 23:39.440]  potassium you just made that action potential a two millisecond or three millisecond event i don't

[23:39.440 --> 23:43.360]  know i'm exaggerating but something super long right compared to what it should be so i guess it

[23:43.360 --> 23:49.040]  can last as a maximum of like 10 milliseconds right no i think it's way less i think i think the

[23:49.040 --> 23:54.000]  difference of like half a millisecond to like point seven of a millisecond is like a huge

[23:54.000 --> 23:59.600]  difference in the terms of what we're talking about but uh but yeah let's just say for sake of

[23:59.600 --> 24:04.320]  example it's like going from one millisecond to two milliseconds or five milliseconds something

[24:05.360 --> 24:09.680]  what you're talking about with the 10 milliseconds is the whole process after it went back down and

[24:09.680 --> 24:14.640]  after hyper polarized and stuff that's not i'm just talking about the the amount of time that it

[24:14.640 --> 24:21.840]  stays very positive which is the when the calcium will be let in so if you double that or even make

[24:21.840 --> 24:28.560]  it 1.5 what the normal amount that calcium will come in so much more calcium will come in that's

[24:28.560 --> 24:33.040]  so much more calcium will bind to the synaptic tagmin which will allow so many more vesicles to

[24:33.040 --> 24:40.640]  be released so i think that's what you're talking about with the uh by messing with the k the k

[24:40.640 --> 24:46.560]  currents the k channels the potassium channels you're you're changing the shape of the spike

[24:46.560 --> 24:52.240]  which you didn't mention but that's basically the result of that and by making the shape of the

[24:52.240 --> 24:57.600]  spike more spread out it allows enough time for it's not just one vesicle to get released but

[24:57.600 --> 25:03.600]  like a bunch of them get released uh over that much time because more calcium can come in in

[25:03.600 --> 25:11.600]  more time because there's the calcium channels open for longer yeah for longer and that's what

[25:11.600 --> 25:22.080]  causes more of the um of the uh um what is it the um the vesicles more more vesicles released

[25:22.080 --> 25:29.200]  yeah so the there's there's sort of like so before the action potential let's zoom in to just the

[25:29.200 --> 25:33.520]  very tip of the axon terminal yeah before the action potential it's just sitting there in

[25:33.520 --> 25:39.840]  what's called a docked state and so you have this vesicle inside the membrane and it's like

[25:39.840 --> 25:46.080]  right up against the membrane but you know like think of two oil droplets you know like if the

[25:46.080 --> 25:50.880]  two oil droplets sort of touch they're gonna they're gonna like come together and make one bigger

[25:50.880 --> 25:55.760]  droplet right so they're sitting there and you have the membrane this and then this one oil

[25:55.760 --> 26:00.000]  droplet kind of right next to it but you know it's like not touching and it needs a little bit of

[26:00.000 --> 26:04.800]  force to get them to touch and sort of like bind and then let the because the droplet holds the

[26:04.800 --> 26:10.960]  neurotransmitter right so so that the thing that allows it to sort of get enough force to push

[26:10.960 --> 26:17.840]  into the membrane is the synaptotagmin molecule and that's synaptotagmin I mean it's the synaptotagmin

[26:17.840 --> 26:22.640]  like linker don't think of it as think of it like a linker um there's other ones there's other names

[26:22.640 --> 26:26.720]  involved there that I'm just skipping but I'm trying to make this like somewhat comprehensible

[26:27.280 --> 26:33.520]  and that synaptotagmin needs doesn't do its job until calcium reaches it and the calcium has to

[26:33.520 --> 26:40.960]  come from outside the cell okay there's very little calcium inside the cell so that calcium

[26:40.960 --> 26:46.560]  channel has to open so that calcium comes from outside the cell goes inside the cell and reaches

[26:46.560 --> 26:53.280]  that synaptotagmin but but that use that duration is usually like a split you know like a millisecond

[26:53.280 --> 27:00.720]  or less which only allows enough to come in to sort of just release like one vesicle or two you

[27:00.720 --> 27:05.280]  know what I mean it just yeah it's just enough to release one from the docking site but if you

[27:05.280 --> 27:09.200]  left that channel open long enough it would just keep it would keep doing it and it would release

[27:09.200 --> 27:16.560]  a bunch from the same site okay so the um so so what so there was um so peter c he suggested that um

[27:17.200 --> 27:23.840]  the the amount of the ndma receptors um I forget exactly was many years ago I read his book ndma

[27:23.840 --> 27:31.760]  receptors um could uh be regulated by the neuron or by um by neurons regulating on the other neurons

[27:31.760 --> 27:41.360]  so to change um basically to change the um okay so so part of this was this idea that the output of

[27:41.360 --> 27:47.920]  the neuron would have would have a would be able to change its volume like it could it um it would

[27:47.920 --> 27:53.760]  become you know neurons can be louder or quieter that's what that's one analogy um is so can I

[27:53.760 --> 27:59.120]  clear acid acrylic filtration so yeah if you're releasing multiple vesicles so if so every vesicle

[27:59.120 --> 28:05.520]  I think let's just I forget if it's 2000 or 3000 but let's say 2000 every vesicle has like 2000

[28:05.520 --> 28:13.040]  roughly uh neurotransmitters so those things float into the synapse and then they bind to things uh

[28:13.040 --> 28:17.680]  you know predominantly the the receptor on the other on the other cell right but if you release

[28:17.680 --> 28:23.920]  five times that amount so instead of 2000 you're getting 10 000 the lateral spread is going to

[28:23.920 --> 28:29.440]  increase to the point where receptors very far lateral which it would have never diffused that

[28:29.440 --> 28:34.720]  far before but because now there's 10 000 of them instead of 2000 now they're diffusing

[28:35.680 --> 28:40.880]  further and further laterally uh you're activating receptors that probably weren't

[28:40.880 --> 28:45.040]  getting activated in the normal case is that what you're talking about or yeah yeah maybe

[28:45.040 --> 28:48.800]  so that's not that's part of what I'm talking about so it's not just it's not so and now

[28:48.800 --> 28:54.800]  that the neuron is not just passing along frequency information and frequency information

[28:54.800 --> 29:00.400]  of slightly larger scales is determined by a vector it's also passing along spatial information

[29:00.400 --> 29:06.240]  because it can reach more neurons right yeah are we going to be relating this back to machine

[29:06.240 --> 29:11.040]  learnings would would it be like because I feel like in machine learning all the neurons are

[29:11.040 --> 29:18.560]  connected every neuron has a connection to every neuron in the other layer uh already so you don't

[29:18.560 --> 29:25.920]  need this well in the current in the current architecture but yeah that's um but but but

[29:25.920 --> 29:32.480]  I mean if you were if you were to add a spatial dimension to uh to neural networks and you we

[29:32.480 --> 29:38.240]  would yeah you'd have to give them like a sort of they'd have to the neural net the the neuron

[29:38.240 --> 29:44.640]  would have a sort of like local a local community and then it would only reach out to other other

[29:44.640 --> 29:50.240]  other neurons in the upstream and in the so it would so a neuron would only talk to some

[29:51.440 --> 29:58.720]  neurons in in the in the layer above it but then if if it uh if it could alternate to talking to

[29:58.720 --> 30:05.520]  all of them to or to just to a few of them you could alternate in between okay yeah yeah that

[30:05.520 --> 30:11.200]  so okay that's where this concept ties in okay yeah so if if you made a neural network that

[30:11.200 --> 30:17.440]  that like each neuron only talked to the 10 closest neurons from the next layer then you could sort of

[30:18.560 --> 30:24.320]  make this event where you have this wide action potential and multi-vesticular release to where

[30:24.320 --> 30:29.680]  instead of just talking to the 10 closest to it it actually spreads out to the like 30 closest or

[30:29.680 --> 30:35.920]  the 20 you know what I mean yeah so so then um so then it spreads out and then um you know so

[30:35.920 --> 30:43.600]  the the um a lot of people have focused on the the ampa receptors but but um but peter um

[30:44.240 --> 30:50.080]  you know tsc he was saying that the the his focus is on the ndma receptors because they can respond

[30:50.080 --> 30:55.760]  with um they can respond faster they can react faster like within and the and the way they can

[30:55.760 --> 30:59.920]  respond you know within like they could vary from like you know somewhere between like one and

[30:59.920 --> 31:05.520]  in four milliseconds or something like that so they can they can receive a a neurotransmitter

[31:05.520 --> 31:11.760]  and then send a signal or something do you know more about that so both ampa and nmda receptors

[31:12.800 --> 31:19.120]  can be fast both of them are very fast uh both of them on the scale of the few milliseconds um

[31:20.640 --> 31:29.520]  ampa most ampa uh is just so mostly sodium potato it's so calcium is a kind of a weird

[31:29.520 --> 31:36.400]  molecule in that it's not just making its influence through its charge calcium has this sort of

[31:36.400 --> 31:41.600]  second influence because it it can act as a messenger so if because the cell is so good at

[31:41.600 --> 31:49.040]  keeping calcium out outside of it away from it that whenever any calcium gets in it it's it binds

[31:49.040 --> 31:55.600]  to things that causes secondary messaging signals and they like changes like uh phosphorylation

[31:55.600 --> 32:01.840]  and d and what dna gets coded so calcium is like the sort of unique thing and so anyways this the

[32:01.840 --> 32:08.320]  nmda channels let calcium in whereas the ampa channels everything i'm saying is like a most of

[32:08.320 --> 32:12.560]  the time thing by the way because there's always this like weird thing anyways that if in general

[32:12.560 --> 32:17.680]  the ampa channels don't let calcium in and the nmda channels do let calcium in

[32:19.600 --> 32:23.920]  uh but they they're very similar otherwise so yeah so i mean so so one of the questions i

[32:23.920 --> 32:27.360]  but because i did this um podcast called the neural ace podcast was it was how does a neural

[32:27.360 --> 32:33.760]  circuit repeat itself so i mean if you could imagine i mean so so so one of the early descriptions

[32:33.760 --> 32:37.360]  that i heard of neural circuits and and i guess there's a lot of different kinds of neural circuits

[32:37.360 --> 32:43.120]  are different scales of neural circuits just neurons talking to other neurons um if you have

[32:43.120 --> 32:50.160]  if you have but i was madding if you had a neural circuit that that consisted of um high of several

[32:50.160 --> 32:54.400]  neurons that were not directly connected to each other but you could see somehow with medical imaging

[32:54.400 --> 32:59.120]  that that one neuron over here was firing and then the next neuron over over here is firing

[32:59.120 --> 33:03.120]  there's no direct connection but then the next neuron was firing and then you saw maybe maybe

[33:03.120 --> 33:10.400]  six or each firing in a loop so my question was how are they able to keep how is that you know

[33:10.400 --> 33:16.800]  frequency pattern able to um how could that continue to happen over time how could you have

[33:16.800 --> 33:26.240]  basically if a neuron might be connected to 200 upstream neurons um and then you know or maybe

[33:26.240 --> 33:30.080]  10 000 upstream neurons that i don't know what the you know what what the number is going to be

[33:30.080 --> 33:38.080]  right i like to let's say stick with 200 for this one 200 200 uh or 200 000 um let's say 200 so you

[33:38.080 --> 33:44.400]  have a neuron and it's connected to 200 other uh it makes synapses on the 200 other neurons yeah

[33:44.400 --> 33:48.400]  and how does it continue how does how would it like so my question was how would it continue to

[33:49.600 --> 33:56.640]  connect to um lead to us to a causality sequence so where that where signals from from the first

[33:56.640 --> 34:00.960]  first neuron that's in the circuit would eventually reach the second neuron on the circuit that's not

[34:00.960 --> 34:06.160]  directly connected and then those signals would somehow reach the the next neuron on the circuit

[34:06.160 --> 34:12.400]  which is also not directly connected and so on because it's almost it almost implies that the

[34:12.400 --> 34:18.960]  signals are traveling along the same path and and so so i so i asked people said is it possible that

[34:19.600 --> 34:25.200]  you know that there's somehow the neuron in the post synaptic uh the post synaptic neuron

[34:25.200 --> 34:32.000]  was somehow selecting the signal um and selecting you know you know deciding what it was going to

[34:32.000 --> 34:37.760]  it was the one it was the one that was going to receive uh the certain signal type and the other

[34:37.760 --> 34:44.400]  neurons were deciding that they were not and and and another idea was was was was a selection

[34:44.400 --> 34:49.680]  related to some sort of like electromagnetic property where that that neuron was had the

[34:50.320 --> 34:55.600]  lowest possible negative and therefore it it attracted the the the signals that it wanted

[34:55.600 --> 34:59.760]  because i was thinking about you know there's positive and negative charges right and i i was

[34:59.760 --> 35:05.040]  talking about this in the earliest podcast and then someone else was like well no uh you know

[35:05.040 --> 35:11.120]  in a later conversation i said basically it's the same distribution every single time a neuron fires

[35:11.120 --> 35:18.720]  and and all the neurons upstream get exactly the same signal every single time and um and then and

[35:18.720 --> 35:28.400]  then i i thought well and then they said um yeah so it says no like there's no selection and but

[35:28.400 --> 35:36.320]  but oh gosh what was the rest of it um yeah so maybe so maybe at that point it was like um

[35:37.840 --> 35:41.200]  no okay so maybe so if if there's no like

[35:43.920 --> 35:48.320]  what i'm saying is if um like like how does it how does the signal travel from

[35:49.600 --> 35:55.040]  how would a signal travel along a consistent path from you know across you know regions of the brain

[35:55.040 --> 36:02.880]  to to like a consistently repeating neural circuit and is is it yeah so i don't know if you if you

[36:02.880 --> 36:07.840]  can answer that let me see what i let's see what i followed and then you can kind of tell me if i

[36:07.840 --> 36:18.080]  followed it okay so from what i understand likes uh you have a an area with some neurons and let's

[36:18.080 --> 36:23.520]  say uh you're looking at one of them and it fires and then in another area where you're looking at

[36:23.520 --> 36:29.200]  some neurons you're seeing some activation of some neurons there at that that happened like

[36:29.200 --> 36:35.840]  at the exact uh time difference that it would take for the signal to progress to that area and

[36:35.840 --> 36:41.440]  then maybe a third area so and so you're seeing a pattern you're seeing like a neuron from or

[36:41.440 --> 36:45.440]  these neurons from area one lead to these neurons from area two firing lead to these

[36:45.440 --> 36:54.160]  neurons from area three firing and and so as far as how exact it is over like so let's say

[36:54.720 --> 37:00.720]  in in for stuff i've seen so like you give a mouse the same stimulus you give him like a picture of

[37:00.720 --> 37:05.920]  uh whatever who cares you give him the same stimulus you wiggle the whisker the exact same way whatever

[37:05.920 --> 37:14.160]  um that's kind the exact same neurons kind of just doesn't happen but the um but but the same

[37:14.160 --> 37:19.760]  areas do happen so you do see like like 50 neurons an area one fired and then 50 and

[37:19.760 --> 37:24.960]  neurons an area two in this other second area fired and then 50 in this third area fired and so

[37:24.960 --> 37:29.040]  that's what people usually report they're like so we're what we get is we get bam a bunch of them

[37:29.040 --> 37:34.480]  fire here then a bunch of them here so what we're seeing is area a and then area b and area c are

[37:34.480 --> 37:41.520]  a direct result of wiggling the whisker in this exact way but uh i i'm sure there's like exceptions

[37:41.520 --> 37:47.200]  but in general you i think it's fair to think of most cases i mean you could definitely think in

[37:47.200 --> 37:53.840]  most cases it's not like the same neurons firing every time definitely not so uh what what's interesting

[37:53.840 --> 38:00.560]  though is okay so another thing maybe that's helpful and especially for a lot of competition

[38:00.560 --> 38:06.160]  the computational people that i talked to even in my program uh is they always think like things

[38:06.160 --> 38:10.800]  are very strongly connected probably because in machine learning that they are but in in the

[38:10.800 --> 38:17.360]  brain i think they did some models and i think typically for one cell if you'd like counted

[38:17.360 --> 38:24.880]  all 500 of their inputs i think it would take like 60 of the inputs or 30 of the inputs all

[38:24.880 --> 38:32.400]  firing at the same time to be like enough to get that that's to be depolarizing enough to get that

[38:32.400 --> 38:38.880]  cell to fire um and that has to be relatively within a very you know within a couple millisecond

[38:38.880 --> 38:46.560]  window um so it's very rare almost never the you know maybe there's a couple strong also there's a

[38:46.560 --> 38:51.680]  high degree very uh high degree of variability in the strength of connections so with let's say a

[38:51.680 --> 38:56.160]  500 neurons you're gonna have a maybe just a couple super strong ones that maybe can almost

[38:56.160 --> 39:01.440]  drive a spike on their own but probably not more than like you know one percent of those ones

[39:01.440 --> 39:05.760]  but for the vast majority they're all going to be like on the average of one thirtieth or one

[39:05.760 --> 39:11.920]  fiftieth or something of the size it needs to be do we build a spike in the in the next neuron so

[39:12.480 --> 39:19.840]  so it's very rare that like one neuron spikes and then the next neuron spikes because that neuron

[39:19.840 --> 39:27.680]  spiked um uh what's more a more helpful way to think about it is all these neurons are just

[39:27.680 --> 39:32.000]  like even when you're not receiving any stimulus the brand's just sort of firing in its default

[39:32.000 --> 39:36.880]  mode and there's just so much activity just already going and so the outside world almost

[39:36.880 --> 39:41.920]  doesn't cause activity it just sort of shifts the activity that's already going on into something

[39:41.920 --> 39:48.960]  else so and so the way I think it's helpful on a computational standpoint is think of all these

[39:48.960 --> 39:54.640]  neurons are already sort of just hanging around near threshold so it doesn't take a ton to get

[39:54.640 --> 40:03.040]  them past threshold um but they're sort of uh you know in vector calc where there's like saddle

[40:03.040 --> 40:10.000]  points and attractor states and repeller states it's almost like the brain is normally like on

[40:10.000 --> 40:15.680]  that saddle or on in the repeller state like not where it shouldn't be stable just waiting

[40:16.320 --> 40:21.760]  to go to a stable place through any input and then you know and so the the the network is always on

[40:21.760 --> 40:29.440]  the edge uh yeah yeah I know the I know this concept um so okay so the the next question is the um

[40:29.440 --> 40:37.760]  is the uh so the dendrite uh so the idea here is that the the dendrite is um um you know in

[40:37.760 --> 40:42.480]  Jeff Hawking's in Jeff Hawking's explanation in on intelligence the dendrite the hairs in

[40:42.480 --> 40:48.720]  the dendrite could potentially um store temporal temporal temporal memories right so the memory

[40:48.720 --> 40:55.520]  that is a that is a uh like a proposition currently like uh a good like uh what's it called like a

[40:55.520 --> 40:59.600]  standing like like there's not like a ton of research on it but this is kind of what everybody

[40:59.600 --> 41:04.160]  thinks is the spines of the dendrite which are these little like like bubbles on on the dendrites

[41:04.160 --> 41:10.880]  sort of like hold like the memory of where the input came so that it knows so that way the neuron

[41:10.880 --> 41:16.160]  knows what to strengthen for next time okay so then so then so that's how the neuron knows

[41:16.160 --> 41:21.920]  what to strengthen so so that that leads to the next question is is in and so um so the dendrite

[41:21.920 --> 41:30.400]  might um you know inform the neuron somehow of of what criteria to look for like like the idea is

[41:30.400 --> 41:35.680]  it's gonna um you know change the information it collects is somehow gonna go back to the

[41:36.320 --> 41:42.400]  NDMA receptors and and it's going to you know change the interval of what of what they'll accept

[41:42.400 --> 41:48.000]  between uh from three milliseconds to only two milliseconds with only one millisecond and and

[41:48.000 --> 41:54.960]  so it it can so in the idea is that it could change what the um what it's willing to listen to

[41:54.960 --> 42:00.800]  whether it's going to listen to one neuron or 30 neurons before it does anything um well well

[42:00.800 --> 42:10.000]  it's always list so the way that it's listening to all the neurons is um you you know so all those

[42:10.000 --> 42:16.400]  neurons have connections and receptors uh okay so if you look across all the dendrites and you say

[42:16.400 --> 42:23.360]  there's like 500 areas where axons from other neurons are like right across the synapse from it

[42:23.360 --> 42:32.000]  all along those dendrites um if any of those 500 axons release neurotransmitter you know those

[42:32.000 --> 42:36.000]  that that transmitter is going to bind to the receptors and that's going to open channels and

[42:36.000 --> 42:41.840]  those channels are going to get depolarized so it's it can't really selectively ignore anything

[42:41.840 --> 42:47.280]  because if it if there's charge coming in the dendrites that charge is going to go towards

[42:47.280 --> 42:54.640]  the cell body and that charge is going to uh going to you know like add up in towards this

[42:54.640 --> 43:00.480]  you know and and perhaps cause a spike it's uh the way things are it can be like ignored is this

[43:00.480 --> 43:09.040]  kind of it they can happen but but it's uh like you can sort of like remove receptors but that

[43:09.040 --> 43:15.280]  that type of control isn't coming from the the cell bot like the cell body isn't like there isn't

[43:15.280 --> 43:21.600]  like some center control like saying what to pay attention to what not to so um i mean that that

[43:21.600 --> 43:30.000]  is kind of the idea of peter tse's book the neural basis of of uh free will is that a neuron can change

[43:30.000 --> 43:39.120]  uh the the uh sort of like the um what what it whether it what it's ndma receptors are receptive

[43:39.120 --> 43:46.480]  to but you're saying that that that that's that's not uh an idea that you did you uh okay well not

[43:46.480 --> 43:55.760]  from the so so the let's talk about the things that can change uh the receptivity of uh nmdh

[43:55.760 --> 44:06.160]  receptors uh so they can be phosphorylated uh phosphorylation occurs from from uh you know

[44:06.160 --> 44:11.120]  like there there can be signaling that happens through other receptors near it's other like gpcr

[44:11.120 --> 44:19.360]  receptors can uh get activated that change the phosphorylation in a specific area but that that

[44:19.360 --> 44:27.520]  signal is not um coming from the center of the cell you can also like express more kinases

[44:27.520 --> 44:32.640]  and that would come from the cell but the the kinases are not going to be directed they're

[44:32.640 --> 44:37.360]  going to be evenly distributed across the so if you up regulate your kinases that's going to just

[44:37.920 --> 44:42.560]  go everywhere and so that's what the cell itself can control but if there's going to be some sort

[44:42.560 --> 44:48.480]  of local signaling to an nmda receptors to say phosphorylate these to make them weaker or like

[44:48.480 --> 44:53.520]  a ret you know have a rest in mind to get them removed from the membrane that type of signaling

[44:54.080 --> 45:01.680]  i don't know of any way that the cell body that the the nucleus of the cell can really control

[45:01.680 --> 45:09.040]  it in a space spatial spatial way uh that kind of controls going to come probably from signaling

[45:09.040 --> 45:12.720]  happening from other cells so i mean the stuff could be right but i just don't think it would be

[45:12.720 --> 45:18.080]  correct to think about it in terms of like the cell is deciding that it would be like the cells in

[45:18.080 --> 45:24.560]  the network are contributing to that results where the nmda so um how would the cells in the network

[45:24.560 --> 45:32.240]  can could modify the ndma receptors of another cell okay so say uh along those are in our example

[45:32.240 --> 45:39.360]  500 contacts onto the all the dendrites of this one cell let's say like uh 10 of those contacts

[45:39.360 --> 45:45.120]  are get are coming from a certain cell like an acetylcholine cell where instead of sending

[45:45.120 --> 45:49.840]  glutamate like most cells do this particular cell is sending acetylcholine and let's say one of the

[45:49.840 --> 45:57.360]  receptors in your cell in in your dendrite is a acetylcholine receptor that's a gpcr that is a

[45:57.360 --> 46:04.640]  gi coupled receptor that's going to decrease the cyclic amp in that local region and because it's

[46:04.640 --> 46:09.360]  going to have that decreased cyclic amp it's going to lower the activity of pka which is a kinase

[46:09.360 --> 46:14.400]  which phosphorylates and that phosphorylating kinase is now no longer going to phosphorylate

[46:14.400 --> 46:21.120]  this nmda channel and so then all the nmda channels in that rough region are now going to like get

[46:21.120 --> 46:25.280]  turned down or turned up or so okay so we're talking about inhibitory inhibitory action

[46:25.840 --> 46:34.160]  the inhibitory cells right no no no no we're uh this is signaling this isn't inhibitory like in

[46:34.160 --> 46:38.480]  the GABA sense where you're getting a negative charge okay this is like a signaling molecule

[46:38.480 --> 46:47.280]  that sort of sends a signal inside the dendrite and tells the receptors to you know uh sort of

[46:47.280 --> 46:50.880]  it sort of either blocks them a little bit or gets them a little more activated or it even

[46:50.880 --> 46:54.960]  signals to them to like hey remove from the membrane you don't even need to be there anymore

[46:54.960 --> 46:59.840]  which would decrease the you know so the net result is you're right an inhibitory result

[46:59.840 --> 47:06.880]  but i just don't want that to be confused with uh in an inhibitory uh you know response which is a

[47:06.880 --> 47:12.960]  GABA response which directly just puts negative charge in it like it's a chloride channel basically

[47:12.960 --> 47:17.200]  where chloride comes in and makes it more negative but that's that's not what i'm talking about in

[47:17.200 --> 47:23.040]  this case okay but that happens too both of all of those things happen um and so another question

[47:23.040 --> 47:31.680]  is the um how does the um uh so the dendrite i guess can store um with its spikes it might store

[47:31.680 --> 47:38.720]  like um temporal information um and is that also is it also storing spatial information like you

[47:38.720 --> 47:46.960]  know basically like able to set the cell up so that it responds to um not just 30 downstream neurons

[47:46.960 --> 47:55.360]  but a specific set of downstream neurons like you know like neurons over over in in certain

[47:55.360 --> 48:00.560]  regions i mean i'm just thinking about like how you know there are certain um you know

[48:00.560 --> 48:08.080]  uh cells or cones uh in the eye that that might be the only responds to uh information moving

[48:08.080 --> 48:14.960]  from left to right for example right so yeah yeah so how does that work yeah so i'm just i'm just

[48:14.960 --> 48:20.400]  thinking like yeah so if you can so so yeah if you could respond to that and i'll remember the

[48:20.400 --> 48:26.240]  rest of it while you're talking yeah so i'm trying to remember so there's amicron amicron cells that

[48:26.240 --> 48:32.000]  do that that left to right thing and the way they work is like there's this inside to outside um

[48:32.000 --> 48:38.960]  kind of thing going on basically it's like it has to do with temporal summation so

[48:39.920 --> 48:46.320]  so uh you only get so okay first i think it's best to clarify that the action potential

[48:47.840 --> 48:53.440]  you know only it starts at the cell body and it goes down the axon uh the in the simplest

[48:53.440 --> 48:58.080]  way to think about it it it sort of doesn't the action potential doesn't go up the dendrites at

[48:58.080 --> 49:03.440]  all it's just like you add up all so charges come in from the in these little skinny dendrites

[49:03.440 --> 49:07.920]  and then they sort of travel down and it's you think of these dendrites as a leaky hose right

[49:07.920 --> 49:14.160]  so if you shove a lot of water in it which is voltage uh you know if you shove enough in it

[49:14.160 --> 49:18.800]  you know enough will get through the other side and not leak out the hose right you know if you

[49:18.800 --> 49:24.000]  stabbed a fork with the you know stabbed a hose with the fork a bunch of time but uh so whatever

[49:24.000 --> 49:28.080]  makes it so now you have all these hoses and they're all sort of plugged into the same thing at the

[49:28.080 --> 49:32.480]  end and what you're trying to do is get enough water in at the end at the same time and if you

[49:32.480 --> 49:38.720]  get enough then it like has a burst right like and that burst is the action potential so the

[49:38.720 --> 49:44.160]  action potential is only happening on the opposite side of all the dendrite leaky hose dendrites um

[49:44.160 --> 49:50.160]  um which you know in reality there's some that that you for the sake of thinking about it just

[49:50.160 --> 50:00.000]  consider this to be like roughly true um so so the uh sorry i lost my train of thought well so if

[50:00.000 --> 50:06.000]  the dend if the dendrite hairs are storing a temporal uh memory um then they would have to

[50:06.000 --> 50:12.480]  somehow that memory has to um be triggered and sometimes when voltage is being received but

[50:12.480 --> 50:18.000]  not every time when voltage is being received otherwise it wouldn't have any meaning yeah but

[50:18.000 --> 50:26.480]  the what what the difference is every is the if you're only looking at 10 of the inputs and sometimes

[50:26.480 --> 50:31.760]  there it's causing it and sometimes it's not the difference 10 usually is the other 390 inputs

[50:31.760 --> 50:36.960]  that you're not looking at you know um so so let me finish it i remember where i was going you

[50:36.960 --> 50:44.560]  you asked about left to right so so the way that that works is imagine so let's say there were

[50:44.560 --> 50:50.560]  10 dendrites so 10 leaky hoses and you can shove in water at the very tips of all 10 of these leaky

[50:50.560 --> 50:57.120]  hoses um and and if you get enough water to the center where they all meet at the same time it

[50:57.120 --> 51:03.600]  would cause a spike now this is how you could get left to right sort of uh firing if the hoses

[51:03.600 --> 51:11.360]  on on one side were gradually longer than the so imagine the rightmost hose is 50 feet long then

[51:11.360 --> 51:16.640]  the then the second one is 45 feet long and the third one is 40 feet long and the fourth one is

[51:16.640 --> 51:21.360]  35 and then 30 feet and so so all these dendrites are different sizes all these hoses are different

[51:21.360 --> 51:29.360]  sizes if you start putting water in the longest hose first and then a split second later you put

[51:29.360 --> 51:33.920]  it in the second longest and then a split second later you put in the third longest and so forth

[51:33.920 --> 51:39.760]  then at the end of the day all of those water will reach the center point at the exact same time

[51:39.760 --> 51:45.440]  which will be enough to get it over the threshold and to communicate hey fire now if you do it in

[51:45.440 --> 51:50.080]  the opposite order and you put water in the shortest hose first and then gradually put water

[51:50.080 --> 51:55.680]  towards the you know get go towards the longest hose what'll happen is water will reach it and

[51:55.680 --> 52:00.960]  leak out reach the center point and then reach out leak out and then the next the water from the

[52:00.960 --> 52:05.280]  other hose the longer hose will reach it next but it is basically they don't add up all at the same

[52:05.280 --> 52:10.640]  time they don't reach it basically the voltage all has to kind of get there because all of the

[52:10.640 --> 52:16.480]  stuff is super leaky so so it has to all get there at the same time to cause a spike so if the

[52:16.480 --> 52:23.440]  organization of of the dendrites are such that you know you're going they're longer on one side

[52:23.440 --> 52:28.640]  and temp tapering to shorter on the other side it's only going to work in the left to right

[52:28.640 --> 52:34.560]  direction but not the right to left direction if you know if does that make sense it'd be easier

[52:34.560 --> 52:39.120]  to draw a picture I guess no I know I got it a hundred percent but um but so so then I wanted

[52:39.120 --> 52:47.200]  to ask you how how then dendritic spikes affect this this equation so dendritic spikes are actually

[52:47.200 --> 52:53.200]  the thing I was telling you to ignore so so in the in the neuroscience 101 where you don't learn

[52:53.200 --> 52:59.840]  about dendritic spikes it's just axons that spike the spike starts at the soma uh which is the center

[52:59.840 --> 53:05.680]  point and then it propagates down the axon to the axon tip and then vessel release but in reality

[53:05.680 --> 53:14.320]  if there's a spike that's going to start at the soma even though the vast majority of of the energy

[53:14.320 --> 53:18.960]  is going down the axon there's it's so powerful there's still a little bit getting pushed the other

[53:18.960 --> 53:28.240]  way too right like when you you know when when uh when two rockets are attached in space and one's

[53:28.240 --> 53:33.280]  super heavy and one super light the light one's gonna shoot off super far but the heavy one's

[53:33.280 --> 53:37.840]  still gonna like have a little bit you know what I mean so like the the spike is sort of going in

[53:37.840 --> 53:44.560]  the opposite direction too and that causes what's called a back action potential BAP uh it's and

[53:44.560 --> 53:51.600]  back propagating yeah back propagating action potential and uh that that is that is one component

[53:51.600 --> 53:58.960]  of a dendritic spike there's this other component of it and then so because there's these voltage

[53:58.960 --> 54:04.000]  gated calcium channels which are very similar to the ones on the tip of the axon that let the

[54:04.000 --> 54:09.200]  calcium in that let the vessel release but there's very similar channels also on the dendrite um

[54:09.200 --> 54:16.880]  and and they can sort of add up and sort of propagate in a in a you think about of an op amp

[54:16.880 --> 54:22.800]  with a with a feet a foot feet forward you know what I mean with a positive feedback gain kind of

[54:22.800 --> 54:29.040]  thing and so so and actually NMDA channels can contribute to this so there can be but but not

[54:29.040 --> 54:34.000]  every cell has these really it's mostly layer five pyramidal cells that have these but uh yeah so there

[54:34.000 --> 54:42.000]  can be um dendritic spikes which are very different than than normal than the classic spike that were

[54:42.000 --> 54:47.280]  the classic axonal spike that we always talk about these dendritic spikes can happen actually both

[54:48.560 --> 54:53.360]  uh through calcium channels in the forward direction in those layer five pyramidal cells

[54:53.360 --> 54:58.160]  but in every cell you have those back propagating uh action potentials where it's happening in the

[54:58.160 --> 55:04.560]  cell and going backwards and so I think a lot of what we were talking about is trying to define

[55:04.560 --> 55:11.840]  this uh thing called uh spike timing dependent plasticity um and like how it assigns credit

[55:11.840 --> 55:17.600]  because in hebean learning you have this idea where you know fire together wire together um

[55:18.320 --> 55:26.240]  where if one neuron caught like sort of caused the spike of the next neuron then that strength

[55:26.240 --> 55:32.000]  that synapse should be strengthened whereas if it didn't it should be weakened and so uh this the

[55:32.000 --> 55:38.000]  idea is like if it goes neuron one then two then that neuron one probably caused it and thus it

[55:38.000 --> 55:43.120]  should be strengthened but if if the postsynaptic neuron fires first and then the presynaptic

[55:43.120 --> 55:48.880]  neuron fires then you know that presynaptic neuron didn't contribute crafted the first one firing

[55:48.880 --> 55:54.400]  because it's the first one sorry to neuron two to firing because that it it didn't you know it

[55:54.400 --> 55:58.400]  didn't fire before it so it couldn't contribute to it so in that sense it should be weakened and

[55:58.400 --> 56:05.440]  they've even showed uh moving coupons lab has showed like that you know the closer one fires

[56:05.440 --> 56:09.120]  before the other the more the synapse gets strengthened and then if it happens in the

[56:09.120 --> 56:16.400]  other direction the more it gets weakened and the way they've explained it is is uh through

[56:16.400 --> 56:22.400]  we were just talking about like with assigning sort of the credit to the to the dendritic spine

[56:22.400 --> 56:29.600]  through this sort of calcium signaling and back propagating action potentials uh and uh i think

[56:29.600 --> 56:35.840]  rafael used probably has some good uh explanations of it in his book called spines

[56:38.160 --> 56:43.760]  but but uh but yeah sorry i probably went too far all over the place there no no no you

[56:43.760 --> 56:48.960]  no you didn't um that was really great that was um so it was interesting okay so so jeff

[56:48.960 --> 56:55.360]  hawkings was in his book on the new book uh a thousand brains he was saying that um he thinks

[56:55.360 --> 57:02.320]  the the dendritic um spike could be how the neuron is predicting that it's going to fire

[57:02.320 --> 57:09.360]  and in addition to everything else he said do you do you agree with that yeah like a dendritic spike

[57:09.360 --> 57:17.760]  spike uh well the well first off the back propagating action potential that dendritic spike

[57:17.760 --> 57:23.280]  only happens as a result of the neuron already firing but as far as what you're saying if it

[57:23.280 --> 57:29.840]  can uh be a large factor in causing the axonal spike then you're talking about it forward

[57:29.840 --> 57:36.320]  dendritic spike and those yes in the cells that have those in the layer five pyramidal cells

[57:36.320 --> 57:43.520]  yeah that those are huge huge contributing factors usually yeah usually you need usually you need

[57:43.520 --> 57:48.560]  that plus like a little bit of something else to get a spike you're you're probably not going to get

[57:49.280 --> 57:53.760]  it's probably going to be difficult to evoke spikes without that that dendritic spike so so

[57:53.760 --> 57:57.600]  that was that was that's sort of like jeff hawkings basis for how a neuron is predicting that it's

[57:57.600 --> 58:04.400]  going to fire without actually firing completely um and uh thank you for like and then so i also

[58:04.400 --> 58:08.560]  wanted to ask you so i so when i asked about dendritic spikes you you answered a question

[58:08.560 --> 58:15.680]  that i wanted to ask but i i my i was actually asking about dendritic hairs and how they affect

[58:15.680 --> 58:21.840]  voltage hoses um fines yeah the the hoses are like the things that i was calling dendrites hoses

[58:21.840 --> 58:26.400]  yeah and then the hairs are actually called spines the dendritic spines spines so i said

[58:26.400 --> 58:32.320]  spikes instead of spines and that you answered a second question for me before the first question

[58:32.320 --> 58:35.760]  and hopefully it doesn't delete the first question from my memory but i'm just kidding

[58:35.760 --> 58:41.840]  uh so how so how would the dendritic hairs affect the the voltage of the hoses would that

[58:41.840 --> 58:47.280]  it would that cause the would that mess with the order of well actually maybe i should make

[58:47.280 --> 58:52.560]  sure you mean spines so by dendritic hairs do you mean like the the length of the dendrite or do

[58:52.560 --> 58:58.080]  you mean the little tiny points on the dendrite those are spines those are the spots where the

[58:58.080 --> 59:04.240]  connections sort of happen i meant the yeah the the points on the dendrite not not the length of it

[59:04.240 --> 59:12.880]  okay yeah so those are dendritic spines and uh not okay most cells have them not all of them

[59:12.880 --> 59:20.720]  most neurons have them um and how how does how does that affect how does it spines affect the

[59:20.720 --> 59:25.120]  voltage going through the the the dendrites themselves yeah because is that what you're

[59:25.120 --> 59:30.240]  asking yeah yeah because because you know if voltage uh if voltage travels through different

[59:30.240 --> 59:34.880]  materials it changes how fast voltage can can move through material and if there's more material

[59:34.880 --> 59:42.880]  that should slow it down right yeah okay so um wait what's your background you have like physics

[59:42.880 --> 59:49.920]  background right can i just talk about v i v equals ir type stuff with you or like what like um i

[59:50.960 --> 59:56.960]  v equals ir i i'm not uh you're you're just i can hang with you i can hang with you a little bit

[59:56.960 --> 01:00:04.640]  you gotta be you gotta use too many acronyms okay uh so you're like c s mostly right yeah okay

[01:00:05.360 --> 01:00:09.920]  that's good that's good it's just good for me to know which which thing you are i have i've got a

[01:00:09.920 --> 01:00:17.920]  lot of a lot of neuroscience and a lot of c s okay cool cool uh so um and some physics some some

[01:00:17.920 --> 01:00:24.560]  astrophysics some some quantum but not but not as much as other experts out here okay so um

[01:00:24.560 --> 01:00:32.240]  um so okay so let's give some background i think it was this was best put in uh

[01:00:33.520 --> 01:00:39.920]  tim erbin's intro to neurolink thing that 80 000 word thing he you've written that right

[01:00:39.920 --> 01:00:46.560]  or have you read that oh what was it called tim so tim erbin wrote this sort of introducing

[01:00:46.560 --> 01:00:53.120]  article for neurolink when they just came out uh on his website wait but why and i think he titled

[01:00:53.120 --> 01:01:00.480]  it like the brain's magical hat or something it's it has like a picture of a wizard hat and he basically

[01:01:00.480 --> 01:01:07.920]  so so the bit the silicon valley is just full of like c s people right so so and then ilan must

[01:01:07.920 --> 01:01:11.200]  like putting up this like neuroscience thing in the center of all these tech people who don't

[01:01:11.200 --> 01:01:16.960]  know what the fuck it's even for so so basically tim erbin wrote this amazing article it's huge

[01:01:16.960 --> 01:01:20.960]  it's like it's the same size as like reading a chapter in a book it's 80 000 where it's probably

[01:01:20.960 --> 01:01:28.320]  like reading 30 pages or something but he basically condenses a neuro 101 class basically like a two

[01:01:28.320 --> 01:01:33.680]  semester class like like literally i remember reading it being like damn i didn't have to spend a

[01:01:33.680 --> 01:01:38.640]  whole year in uh in intro neuro i could have just like read this article and got the same rough idea

[01:01:40.240 --> 01:01:46.560]  you know um anyways i think this is his i might this might be from my intro textbook

[01:01:46.560 --> 01:01:51.520]  or it might be from his article i can't remember but you can sort of think of so yeah the dendrites

[01:01:51.520 --> 01:01:56.000]  are leaky they're like if you took a hose and you stabbed it a bunch of times with a fork and you're

[01:01:56.000 --> 01:01:59.360]  trying to shove water on one side it's gonna like lose a bunch of water by the time it gets to the

[01:01:59.360 --> 01:02:06.160]  other side but you can also like duct tape the holes and then you you can keep a little more

[01:02:06.160 --> 01:02:09.760]  pressure you know keep the voltage lasting a little longer so that's something called the

[01:02:09.760 --> 01:02:15.120]  length the length constant is like how far you can get the charge to travel and then the time

[01:02:15.120 --> 01:02:20.720]  constant is like how long it takes for that charge to leak out of the holes and these things are

[01:02:20.720 --> 01:02:26.960]  dynamic you can actually plug some so there's certain holes in the dendrite called like uh

[01:02:26.960 --> 01:02:32.320]  inward rectifying potassium channels or whatever and so these can act you can actually shut them

[01:02:32.320 --> 01:02:37.280]  you can send one of these signaling molecules i was talking about and sort of just close them all

[01:02:37.280 --> 01:02:42.480]  and then by closing them all you're allowing uh less you're you're plugging the leak you're sort of

[01:02:42.480 --> 01:02:48.880]  like duct taping the holes and allowing charge to travel further and leak out slower and thus uh

[01:02:48.880 --> 01:02:54.960]  the same signal that was weaker you know that you know was a weak signal because you plugged all the

[01:02:54.960 --> 01:03:01.600]  the holes in the hose kind of goes further down the hose you know and and so so this is dendrites

[01:03:01.600 --> 01:03:06.800]  right so so basically these dendrites can be dynamic in a way so the dendrites can be very

[01:03:06.800 --> 01:03:13.520]  leaky or you send some signal it gets sent some signals to it and then the dendrites become

[01:03:13.520 --> 01:03:20.960]  less leaky and now they can transfer charge even further and and uh so normally let's say it would

[01:03:20.960 --> 01:03:28.480]  take you know in that area it would take like uh one you know one neuron to spike a bunch of times

[01:03:28.480 --> 01:03:31.920]  and a bunch of other neurons spiking a bunch of times and them all happening at the same time so

[01:03:31.920 --> 01:03:37.840]  you really need like 15 inputs to all happen at the same time and that's barely enough to get push

[01:03:37.840 --> 01:03:43.840]  enough charge down to the soma but if you plug up all these holes instead of needing 15 inputs

[01:03:43.840 --> 01:03:50.160]  all at the same time maybe it'll make it with like nine inputs so that's that that's one component

[01:03:50.160 --> 01:03:56.240]  of how these dendrites are uh dynamic so you can sort of plug the leakiness to them uh

[01:03:56.240 --> 01:04:04.240]  uh the spines is the other thing okay so all these inputs are coming in at spines which are like

[01:04:04.240 --> 01:04:08.960]  these little bumps uh honestly there's pictures of all these things in Tim Urban's article too so

[01:04:08.960 --> 01:04:15.280]  like you could actually see pictures of spines um and he uses like tons of analogies every

[01:04:15.280 --> 01:04:21.040]  like in everything so and they're all great you know um i did read his article a long time ago

[01:04:21.040 --> 01:04:25.760]  i just had to look it up to make sure but um yeah it's in front of me right now yeah i

[01:04:25.760 --> 01:04:31.200]  read that thing like i told my like sister who's like not even in nurse she's like a

[01:04:31.200 --> 01:04:36.240]  physician's assistant to read it you know like i like but i think it's mostly directed for like

[01:04:36.960 --> 01:04:42.320]  for like tech people who don't have any neuro background uh but like who know tech you know

[01:04:42.320 --> 01:04:51.600]  but uh so so uh the spines are where the inputs come in and the uh so and the inputs are these

[01:04:51.600 --> 01:04:57.920]  channels that that let mostly sodium ions in and that those sodium ions depolarize that area

[01:04:59.280 --> 01:05:05.520]  and the skinnier the dendrite is the the more resistance it has so like some small amount of

[01:05:05.520 --> 01:05:10.640]  current actually causes a really large voltage in these skinny dendrites but then once it gets

[01:05:10.640 --> 01:05:15.840]  spread out across the dendrite it's sort of like it all ends up the same it all just ends up being

[01:05:15.840 --> 01:05:21.040]  this charge that accumulates and maybe increases the voltage a little bit so it usually takes a lot

[01:05:21.040 --> 01:05:28.800]  of a voltage to cause a spike and then the the signaling of like how do we know what contributed

[01:05:28.800 --> 01:05:35.120]  which which inputs which receptor areas contributed most to the spike that's not known but the way

[01:05:35.120 --> 01:05:41.280]  it's hypothesized now is is through these spines so like if there's signals coming in and the spines

[01:05:41.280 --> 01:05:46.800]  and there's also these calcium calcium buffer proteins and if there's these calcium channels

[01:05:46.800 --> 01:05:53.920]  going in and it holds calcium enough in those spines for just long enough and then a second later the

[01:05:53.920 --> 01:06:00.400]  cell spikes and sends a back propagating action potential so now that area that got the input

[01:06:00.400 --> 01:06:06.240]  now knows that there was a success on the other side you know like oh we got it to spike and so

[01:06:06.240 --> 01:06:11.040]  maybe it gets that signal from the back propagating action potential and because it still has calcium

[01:06:11.040 --> 01:06:17.920]  sitting in the spine together it's like it like somehow signals that spine like hey we did it

[01:06:17.920 --> 01:06:23.920]  you should strengthen uh but but there's no sort of control from the cell that's like hey it was

[01:06:23.920 --> 01:06:28.480]  that spine over there let's go strengthen that spine it's more like that local area is probably

[01:06:28.480 --> 01:06:34.240]  holding some sort of information and probably the idea right now is like maybe it's calcium

[01:06:34.240 --> 01:06:39.600]  is being held in the spine and then if a split second later you get this back propagating action

[01:06:39.600 --> 01:06:44.880]  potential that's the success signal so the spine kind of knows hey we did it let's strengthen and

[01:06:44.880 --> 01:06:49.760]  then the way it would strengthen would be like let's put an extra receptor in there let's put

[01:06:49.760 --> 01:06:56.720]  an ampere set let's add another ampereceptor uh you know which will allow which will just be another

[01:06:56.720 --> 01:07:02.880]  input in the exact same spot so it'll it'll be like two receptors are now there that can that can

[01:07:02.880 --> 01:07:08.320]  open in response to the neurotransmitter across from it okay so let me get let me get this right

[01:07:08.320 --> 01:07:16.400]  so i so i thought that that um uh that's that's spines increase and decrease in number um rapidly

[01:07:16.400 --> 01:07:26.720]  like all the time and they get like bigger and smaller and uh uh so this this receptors on spines

[01:07:26.720 --> 01:07:32.560]  the receptors on dendrites it doesn't have to be spines but yes off uh many times on spines um

[01:07:32.560 --> 01:07:38.880]  um they are constantly getting extra implanted in the membrane and getting removed from the membrane

[01:07:39.840 --> 01:07:49.840]  so uh spines also are dynamic but i don't think you as far as uh i i don't know if the if it's

[01:07:49.840 --> 01:07:55.600]  worth talking about the function right now at this level of the spines being at uh getting at it or

[01:07:55.600 --> 01:08:01.920]  not because that's a slower process the spines growing towards uh a new so it's like if a new

[01:08:01.920 --> 01:08:06.800]  axon is coming in and now it's this new cell that it you know it's this new connection that it hasn't

[01:08:06.800 --> 01:08:12.720]  even been made before then maybe a new spine will grow out to meet it but that's like more of a long

[01:08:12.720 --> 01:08:18.800]  term plasticity thing that's not like on the terms of likes you know right now we're talking on the

[01:08:18.800 --> 01:08:25.200]  scales of like seconds uh milliseconds seconds and maybe like a minute or a couple minutes whereas

[01:08:25.200 --> 01:08:31.680]  fine growth is more along those maybe hours or days you know what i mean so maybe let's

[01:08:31.680 --> 01:08:39.280]  skip the spines growing and and not growing for like just right now okay so um so we have uh

[01:08:40.400 --> 01:08:47.280]  receptors changing on the on the order of either millisecond scales or minutes um on yeah let's

[01:08:47.280 --> 01:08:52.480]  just say seconds since that's a good middle at ground and then and the spines might be added

[01:08:52.480 --> 01:08:57.280]  or removed but it's on it's on it's in terms of hours or days it's it's much longer period of time

[01:08:57.280 --> 01:09:04.880]  now um are the spines uh when when long-term potentiation happens is that part of the process

[01:09:04.880 --> 01:09:19.440]  of spines getting added or removed? Yes yeah the LTP is it's first you would see more channels

[01:09:19.440 --> 01:09:24.640]  getting put in there so you'd get more amper channels maybe more NMDA channels and then

[01:09:24.640 --> 01:09:28.720]  if you're put putting in a bunch more then you're probably going to get some more spines

[01:09:28.720 --> 01:09:34.880]  to meet new connections uh but that's going to be slightly slower i mean i say milliseconds and

[01:09:34.880 --> 01:09:42.160]  seconds and then seconds and hours i don't actually remember the precise uh uh scale of time but like

[01:09:42.160 --> 01:09:47.280]  that in relation to each other that's what they are okay and then uh so when when protein

[01:09:47.280 --> 01:09:55.440]  synthesis happens uh is that um also uh so that so long-term potentiation protein synthesis is a

[01:09:55.440 --> 01:10:00.560]  part of it is can you can you explain how protein synthesis might be connected to spines being added

[01:10:00.560 --> 01:10:16.320]  or removed? Yeah so i don't that's i don't know the proteins involved in spines being added or

[01:10:16.320 --> 01:10:24.320]  removed i know uh yeah so calcium, calmodulin uh complex can be like a signaling thing that

[01:10:24.320 --> 01:10:31.760]  initiates that um and i don't know the details you're going outside of my okay but i know

[01:10:31.760 --> 01:10:36.240]  and then i this is i'll just like i'll highlight this is a research area

[01:10:39.520 --> 01:10:44.480]  awesome and then um thank you i i have more questions i don't know how much more time you have

[01:10:44.480 --> 01:10:51.520]  but i i i just thought we were gonna end up going back to bci's at some point yeah we yeah um yeah

[01:10:51.520 --> 01:10:59.600]  we can so um uh but i mean i you can keep asking me stuff for net till till you run up till we hit

[01:10:59.600 --> 01:11:04.720]  a dead point then and then we can i don't know however you want to do it so uh okay let me just

[01:11:04.720 --> 01:11:10.640]  look over might the notes that i've typed out here so oh shoot i should have somebody was raising

[01:11:10.640 --> 01:11:15.840]  their hand i should have called on them i think they left oh no whatever i didn't even notice

[01:11:15.840 --> 01:11:19.360]  because i was looking at my notes i noticed a second ago but i wanted to finish my thought

[01:11:19.360 --> 01:11:27.840]  before i brought it anyways yeah you're gone uh okay let's go back to your question about um uh the

[01:11:29.920 --> 01:11:37.840]  the bci so so you wanted to talk about the thalamus right well were you the one that had

[01:11:37.840 --> 01:11:45.440]  brought up artificial perception before um yeah yeah i was okay so that's something i've been

[01:11:45.440 --> 01:11:55.040]  super interested in okay um so you know in the you you can uh in like the ar vr or in vr world

[01:11:55.600 --> 01:12:03.120]  you can solve all the visual issues for artificial perception but then you're missing tactile and

[01:12:03.120 --> 01:12:08.400]  you know auditory and or no i guess auditory you have but you're missing some stuff right you

[01:12:08.400 --> 01:12:13.200]  can like put yourself you can wear these gloves that put pressure on things so it makes it feel

[01:12:13.200 --> 01:12:19.200]  like you're touching stuff but what if you could connect in a way where you can just input the

[01:12:19.200 --> 01:12:25.280]  perception right into the neural activity i think we were talking about like broccoli one day with

[01:12:25.280 --> 01:12:29.600]  some guy right and somebody didn't believe that you could like imprint broccoli in your thoughts

[01:12:29.600 --> 01:12:35.360]  yeah yeah that's that's my that's my example that i um i i i came up with that and uh talked about it

[01:12:35.360 --> 01:12:43.440]  in the narrow lays pot a podcast the idea that that broccoli um uh you that it might have a

[01:12:43.440 --> 01:12:50.080]  temple of spatial patterns that are characteristic enough that even if your brain was organized

[01:12:50.080 --> 01:12:54.560]  completely differently from someone else's brain they could consistently rep recognize

[01:12:54.560 --> 01:12:58.480]  that temple spatial pattern of broccoli and you could consistently recognize the temporal

[01:12:58.480 --> 01:13:02.400]  spatial pattern of broccoli and so you could both talk about broccoli and understand what

[01:13:02.400 --> 01:13:07.440]  each other was talking about and talk about the properties of broccoli together even if your brain

[01:13:07.440 --> 01:13:17.200]  was formatted differently um and uh so let's just break this break this down into something uh half

[01:13:17.200 --> 01:13:26.720]  as complex what if you were just uh watching what if you just wanted to uh taste broccoli

[01:13:26.720 --> 01:13:32.720]  and so you didn't you don't need to know what somebody else's taste of broccoli is what if you

[01:13:32.720 --> 01:13:37.440]  just wanted to imprint but let's like let's do something a little more palpable actually let's

[01:13:37.440 --> 01:13:44.960]  say you wanted and actually this is already a problem an issue in uh in prosthetic devices so

[01:13:44.960 --> 01:13:49.920]  let's say you have you're missing an arm but you want to give the sensation that somebody touched

[01:13:49.920 --> 01:13:57.200]  your hand and you're missing arm and I think this this sort of uh this is totally like it's a very

[01:13:57.200 --> 01:14:01.840]  similar issue it's like if you don't have if you do have an arm what if you just want to input

[01:14:01.840 --> 01:14:08.720]  the sensation of somebody touching your arm directly into the uh sensory region for arm in

[01:14:08.720 --> 01:14:13.600]  your brain you know it's let's set a broccoli let's use that and so that way you're not translating

[01:14:13.600 --> 01:14:17.280]  what it feels like for somebody else to be touched and what it feels like if you do touch

[01:14:17.280 --> 01:14:22.800]  let's just say like I want it to feel like somebody tapped on my wrist um without somebody tapping

[01:14:22.800 --> 01:14:28.800]  on my wrist uh that's sort of like I think a simple like artificial perception issue

[01:14:32.400 --> 01:14:37.120]  um is do you follow like does that is that close enough to the broccoli example for you

[01:14:37.120 --> 01:14:45.120]  yeah I mean so I mean I so so I I I lost focus a little bit um but um are you

[01:14:45.120 --> 01:14:50.320]  are you basically saying that if we we're going back to I think we talked about this the last time

[01:14:50.320 --> 01:14:57.600]  and that if you if you stimulate uh neurons that correspond uh or areas of neurons that the

[01:14:57.600 --> 01:15:03.360]  correspond to um that stimulus that it's going to reproduce that that um that sensation is that

[01:15:03.360 --> 01:15:08.400]  sort of where we're going here the artificial perception yeah exactly yeah so if you

[01:15:08.400 --> 01:15:17.920]  you if you look at the area of the brain that uh transmits that okay so so that the area so that

[01:15:17.920 --> 01:15:22.240]  information gets transmitted from the neurons in your arm it goes up through your your brain stem

[01:15:22.240 --> 01:15:26.240]  it makes one stop there and then it goes to your thalamus where it makes its second stop and then

[01:15:26.240 --> 01:15:30.800]  from the thalamus it goes all across cortex so what I've been interested in is seeing how

[01:15:31.440 --> 01:15:35.840]  and then so basically everything kind of goes through the thalamus uh not smell of course but

[01:15:35.840 --> 01:15:41.440]  like everything I'm talking about here so yeah what if I could just decode what if I just could

[01:15:41.440 --> 01:15:49.040]  decode what's going on in the thalamus and then risk and then uh sort of copy that in so basically

[01:15:49.040 --> 01:15:55.360]  like okay whenever you touch the wrist it's like it's a pattern of like these 700 thalamic neurons

[01:15:55.360 --> 01:16:00.400]  and it goes like these 40 and then right after it's these 20 and then these 50 and then these 10

[01:16:00.400 --> 01:16:06.240]  and then these you know it's like this pattern and then when it activates in this way uh you know

[01:16:06.240 --> 01:16:12.240]  those thalamic signals get out to cortex in this pattern and so but but uh you you figure out like

[01:16:12.240 --> 01:16:16.800]  what the pattern is and and there can of course be variability and it's still going to feel similar

[01:16:16.800 --> 01:16:22.960]  the same and if you can reciprocate that that pattern back onto the the thalamic outputs

[01:16:22.960 --> 01:16:29.760]  then you can basically evoke that artificial uh perception and and I mean Neuralink talks about

[01:16:29.760 --> 01:16:34.000]  this and their stuff they're like yeah and then you can even add in perceptions like we can only

[01:16:34.000 --> 01:16:38.320]  see red and purple but what if you could see ultraviolet and so you could do that you know I mean

[01:16:38.320 --> 01:16:46.640]  so you can add to this uh so yeah so um for I mean we so I I talked about this in the Neuralink

[01:16:46.640 --> 01:16:54.720]  podcast which was um years ago and um you know yet it was uh we I was doing the Neuralink podcast

[01:16:54.720 --> 01:17:02.800]  before even Neuralink existed and and and talking about this and so um the yeah so the idea was

[01:17:02.800 --> 01:17:10.000]  that uh I I brought this up to David Eagleman a neuroscientist once at at a meetup and um

[01:17:10.000 --> 01:17:17.200]  and I said uh something like something to the fact that um if you know since you were since you're

[01:17:17.760 --> 01:17:23.600]  you know your the idea was that uh what if you know I guess it was it was an idea of like oh what if

[01:17:23.600 --> 01:17:30.640]  um your thalamus is like is like your your camera and uh you know like if you're designing like a

[01:17:30.640 --> 01:17:37.040]  VR program and your thalamus is your camera and your and the neocortex is is the environment

[01:17:37.040 --> 01:17:44.880]  that's right that's rendering the image right and uh so so the the stuff the stuff comes in and so we

[01:17:44.880 --> 01:17:51.040]  it goes it goes to the thalamus and uh and that's you know you get you get to observe this stuff

[01:17:51.040 --> 01:17:58.560]  um but um but actually it's like the eyes are the camera and the thalamus is more like the

[01:17:58.560 --> 01:18:04.320]  wire so you can still get the in you can still get all the info you need from the thalamus

[01:18:04.320 --> 01:18:14.400]  if you can just hack into the wires you know what I mean yeah like like the like the detect

[01:18:14.400 --> 01:18:18.640]  like the light detectors the camera lens it's like the camera and the lens itself and the

[01:18:18.640 --> 01:18:24.720]  photo detectors are like in are on the eye but like if you can take the but the camera

[01:18:24.720 --> 01:18:30.480]  spends an electronic signal out and so if you can cut off the signal halfway in its tracks

[01:18:30.480 --> 01:18:37.760]  and then just pump out the signal um you know on the other side then then that's the same scenario

[01:18:37.760 --> 01:18:43.360]  right yeah and I feel like that's the thalamus kind of so so I mean I felt that I felt that the uh

[01:18:43.360 --> 01:18:51.840]  the thalamus was kind of like um uh you know a point where you sit up for a second and just

[01:18:51.840 --> 01:18:58.800]  like really think about this um that the thalamus was like a point where you could intercept the

[01:18:58.800 --> 01:19:15.920]  signal and uh you could um let me just uh let's see it would be like um okay so it's it's kind of

[01:19:15.920 --> 01:19:21.200]  like in so so instead of needing to study like every single part of the neocortex to decode the

[01:19:21.200 --> 01:19:27.440]  brain if you could just stick a microphone to up to the thalamus itself then you might be able to

[01:19:27.440 --> 01:19:36.800]  to decode what a person is thinking about across across their brain and no no not thinking about

[01:19:36.800 --> 01:19:43.760]  not at all I think the thalamus just gets you the sensory inputs so it's like it's like uh

[01:19:44.400 --> 01:19:51.680]  sort of seeing seeing hearing right so okay so it'd be the equivalent to uh if you have a neural

[01:19:51.680 --> 01:20:00.080]  network and uh and you feed it like an image right and the the image all goes to layer one of the

[01:20:00.080 --> 01:20:08.080]  network imagine if you could just record all of what happens in layer two of the network and say

[01:20:08.080 --> 01:20:14.960]  you know it's a 20 layer network if you can just reciprocate layer two next time um it should give

[01:20:14.960 --> 01:20:19.680]  you the same out you shouldn't have to show the network the image again you should just be able

[01:20:19.680 --> 01:20:27.920]  to plug in all the the values for layer two and then it it should sort of like if it decodes like

[01:20:27.920 --> 01:20:32.800]  dog or cat right so if you basically instead of showing it a dog or cat what if you just knew

[01:20:33.360 --> 01:20:39.760]  what the uh layer two weight uh not weights the layer two values were for dog for a particular

[01:20:39.760 --> 01:20:48.000]  dog and then you just put them in then you now uh now the network will will think dog but you

[01:20:48.000 --> 01:20:51.280]  didn't even have to show it dog

[01:21:00.560 --> 01:21:07.760]  maybe yeah i don't know maybe that's not but but basically so you're not knowing what all you

[01:21:07.760 --> 01:21:11.520]  don't know all the all the thoughts are happening in the upper layer you know they're happening in

[01:21:11.520 --> 01:21:18.880]  layers three four five six seven if we're analogizing to to machine learning uh to ai uh all the

[01:21:18.880 --> 01:21:25.600]  thoughts are happening in the in the upper layers the thalamus is sort of like trying to just just

[01:21:25.600 --> 01:21:32.720]  grab what the inputs were you know without having to look at what this you know like the screen so

[01:21:32.720 --> 01:21:37.680]  i i guess one of the questions that i really wanted to ask you um had more to do with your

[01:21:37.680 --> 01:21:45.600]  exploration of the thalamus and how signals travel so i mean i obviously i've um well that's not

[01:21:45.600 --> 01:21:51.520]  not so obvious but you know i've i've taken cop narrow one-on-one and and they talk about how

[01:21:51.520 --> 01:21:58.880]  you know that a photon might bounce off your your eye and and uh eventually a signal might

[01:21:58.880 --> 01:22:04.320]  travel um from your eye along the optic nerve to your thalamus and then backwards to the to the

[01:22:04.320 --> 01:22:11.600]  neocortex to the to the um uh to the uh what is it called the um the cortex in the back of your head

[01:22:12.400 --> 01:22:18.560]  yeah v1 and in the occipital lobe yeah but um but i wonder um there's a lot more detail than

[01:22:18.560 --> 01:22:23.920]  that so would you mind maybe like just describing because you because then you i actually i thought

[01:22:23.920 --> 01:22:30.320]  what it's conventionally thought of that the uh that the the note the the gustatory system didn't

[01:22:30.320 --> 01:22:38.160]  travel to the thalamus but you found out that one of five pathways do everything yeah so the

[01:22:38.160 --> 01:22:43.840]  yeah it doesn't so everything has to go through the thalamus before it reaches cortex really um

[01:22:44.880 --> 01:22:55.200]  yeah except the olfactory system the the smell the smell has five paths and uh and none of the

[01:22:55.200 --> 01:23:02.320]  so of the five nuclei that none of them are the thalamus but one of those five nuclei does go

[01:23:02.320 --> 01:23:10.080]  to the thalamus next so basically in once the olfactory bulb you know goes to piriform cortex

[01:23:10.080 --> 01:23:15.680]  and olfactory tubicle and all these other places i forget which one maybe it's the tubicle or something

[01:23:15.680 --> 01:23:20.800]  then that one goes to like anterior dorsal thalamus or something so the thalamus does get like a copy

[01:23:20.800 --> 01:23:28.000]  of the information but it's not the sole router of it you know it's not the it's not the telephone

[01:23:28.000 --> 01:23:35.120]  operator for for smell okay whereas all the other senses you know it like all the other senses but

[01:23:35.120 --> 01:23:41.760]  before it goes to cortex it has to go through thalamus okay and then um so what part of the brain

[01:23:41.760 --> 01:23:50.480]  does the um uh do the does the gussetory system connect with olfactory bulb uh that so smell i

[01:23:50.480 --> 01:23:57.920]  don't that's not my area but it the the primary area that's studied is the piriform cortex okay

[01:23:57.920 --> 01:24:04.080]  and then um okay so then um so so information comes out of the thalamus right and it goes yeah so let's

[01:24:04.080 --> 01:24:09.280]  so it lets ignore smell and so like for all all the other modalities yeah yeah so you get touch you

[01:24:09.280 --> 01:24:16.320]  get taste vision hearing all that information comes through like the the receptors on the outside of

[01:24:16.320 --> 01:24:23.760]  the skin and the ear uh you know like everything there's it's like these sensory transducers

[01:24:23.760 --> 01:24:29.040]  basically and then they make you know they go through the spinal cord or wherever they go

[01:24:29.040 --> 01:24:36.320]  through and then they go up to the thalamus and the thalamus is sort of like the center station

[01:24:36.320 --> 01:24:42.400]  and then from the thalamus it just like goes all over the it doesn't like once it gets to the

[01:24:42.400 --> 01:24:49.120]  thalamus it no it no longer follows that sort of we go here and then here and then go here um it's

[01:24:49.120 --> 01:24:56.240]  sort of like spreads it sort of starts spreading out like in a in a way where it's not it's not

[01:24:56.240 --> 01:25:00.960]  that useful to start saying to keep the hierarchy going you know i mean there is some degree of

[01:25:00.960 --> 01:25:07.040]  hierarchy but uh it's no it's not as strict as it was before it reached the thalamus before it

[01:25:07.040 --> 01:25:12.400]  reached the thalamus it was very stepwise like all the signals go here and then they and there's not

[01:25:12.400 --> 01:25:16.400]  much there's not like a ton of computation decoding it it's sort of like just goes there

[01:25:16.400 --> 01:25:22.000]  and then gets passed on there's not like a bunch of algorithmic computation going on

[01:25:22.000 --> 01:25:27.120]  but then once it reaches cortex now you have all these computations going on so it sort of doesn't

[01:25:27.120 --> 01:25:35.680]  make sense to try to hack what the cortex is doing because it's so complicated but if you just

[01:25:35.680 --> 01:25:42.080]  want artificial perception what makes more sense is just find out what the input is before the

[01:25:42.080 --> 01:25:47.440]  cortex even got it and then copy that and that way you don't have to figure out the you know

[01:25:47.440 --> 01:25:51.760]  thousands of years of research that it's going to take you to figure out what the hell cortex is

[01:25:51.760 --> 01:25:58.080]  doing if you can just grab the signal beforehand right but but what i'm saying is is um so yeah

[01:25:58.080 --> 01:26:02.960]  so i remember now with the neural ace podcast we were saying that um uh we're decoding the

[01:26:02.960 --> 01:26:09.920]  signal on the thalamus but also um stimulate stimulating the thalamus directly to to reproduce

[01:26:09.920 --> 01:26:14.480]  the um the signal right there because then it would then it would just be carried to the rest

[01:26:14.480 --> 01:26:20.240]  of the neural network right that was that was the idea and yes that would be great and then the

[01:26:20.240 --> 01:26:26.160]  then it comes to the heart issue the thalamus is like in the center of the brain and the furthest

[01:26:26.160 --> 01:26:30.560]  possible thing you could reach the most dangerous thing to go for and so it's actually the worst

[01:26:30.560 --> 01:26:37.440]  target but what you could do is try to influence the term the thalamic terminals you know that

[01:26:37.440 --> 01:26:42.400]  that was an idea i i was working i'm thinking i'm constantly thinking about how to approach it that

[01:26:42.400 --> 01:26:47.440]  way uh one of the ideas that i came up with um again this is the neural ace podcast if you

[01:26:47.440 --> 01:26:52.000]  want to check it out but it was yeah i definitely will i i just looked up that guy and he looks

[01:26:52.000 --> 01:26:57.120]  like somebody i should look up the the were you with were you talking with him the stanford

[01:26:57.120 --> 01:27:06.960]  professor uh which one uh it says uh david eagleman i don't have a podcast with him yet

[01:27:06.960 --> 01:27:12.400]  but he's promised that he'll do one with me oh he's not a professor he's a stan alumni stanford

[01:27:12.400 --> 01:27:17.680]  university he teaches oh yeah he is a professor okay yeah yeah yeah but uh but yeah i'll definitely

[01:27:17.680 --> 01:27:25.680]  check it check it out uh that that podcast so so yeah we could probably uh talk more easily if i

[01:27:26.240 --> 01:27:32.640]  if i followed that podcast um but what i was going to go before was um i so i i pitched the idea

[01:27:32.640 --> 01:27:39.520]  that we could target the um we could stick the uh the uh the neural ace up the up the nostril

[01:27:39.520 --> 01:27:45.440]  via like a nano robot would climb up up the nose to the underside of the of the brain

[01:27:45.440 --> 01:27:49.760]  as close as as close to the thalamus as we could get it and that's how that's how we

[01:27:49.760 --> 01:27:58.400]  would decode the um the thalamus and and uh and and and and stimulate it basically yeah and that's

[01:27:58.400 --> 01:28:06.160]  a reasonable like solution like they are ready i'm the stuff with uh they're like putting in some

[01:28:06.160 --> 01:28:14.560]  type of electrode through the vest through the blood vessels um and oh yeah i'm right about that

[01:28:14.560 --> 01:28:20.080]  that that's a new it's a actually it's actual neural ace that they they stick the they they um

[01:28:21.040 --> 01:28:25.200]  there's a it it goes through the it's you can actually stick it in through someone's arm and

[01:28:25.200 --> 01:28:30.560]  then route it up the blood vessel into the brain and then when it gets to where it's supposed to be

[01:28:30.560 --> 01:28:36.160]  it expands i wrote about this as well on so if you look you can see my you can see i interviewed the

[01:28:36.160 --> 01:28:41.360]  guy who who was doing it actually and and that's uh who does who does do that by the way i'm curious

[01:28:41.360 --> 01:28:47.600]  let me check my website i'll i'll tell you the who it was yeah i covered this news yeah i mean it

[01:28:47.600 --> 01:28:53.040]  must have not i mean it must have not been that awesome because it came out like three four four

[01:28:53.040 --> 01:28:58.640]  years ago and i haven't ever seen or heard anybody talk about it oh i you know at least that at least

[01:28:58.640 --> 01:29:04.000]  that work i mean i see talked about all the time and the pop science stuff but like in the neuroscience

[01:29:04.000 --> 01:29:09.040]  side i haven't seen much so maybe it's a slow moving maybe they're maybe it's like in a clinical

[01:29:09.040 --> 01:29:18.240]  step or something uh hang on let me just um i think i need to hit the search button for uh neural

[01:29:18.240 --> 01:29:27.840]  no place on my it may have been so long ago that i wait oh shoot

[01:29:36.880 --> 01:29:43.520]  oh here it is okay so it's called stent road yeah that sounds right so the virtual rally for pain

[01:29:43.520 --> 01:29:49.680]  relief in stentrode a brain computer interface can be implanted in the brain without brain surgery um

[01:29:49.680 --> 01:29:54.160]  so i interviewed uh at the game developer conference of 2019 i spoke to david protrino

[01:29:55.520 --> 01:30:02.080]  he's a phc at mount cyanide um and i talked about his neural ace product called stentrode um and uh

[01:30:02.720 --> 01:30:06.880]  so if you message me uh later i'll send you the link to this article since it's a little bit harder

[01:30:06.880 --> 01:30:11.280]  to find or you can just search my website for stentrode and you'll find my article and it has

[01:30:11.280 --> 01:30:16.240]  the interview with him and also i linked his talks you can see his talk at gdc which you

[01:30:17.280 --> 01:30:22.960]  you weren't able to find before okay yeah no i mean i followed you yesterday or two days ago on

[01:30:22.960 --> 01:30:29.120]  uh on twitter so i i can i can message you and stuff on there okay good um i don't know if i'm

[01:30:29.120 --> 01:30:35.040]  following you back yet but i i hope i'm yeah i don't think you're following me back but you'll

[01:30:35.040 --> 01:30:39.120]  i don't i don't know as hopefully you didn't get too many followers in the last two three days

[01:30:39.120 --> 01:30:44.800]  no i mean i've gotten in the last like four weeks i've gotten uh like a hundred followers and i i

[01:30:44.800 --> 01:30:48.240]  want to follow every single person back but i just i sometimes i'm not aware

[01:30:51.600 --> 01:30:58.240]  yeah i'll message you or something um yeah that's that's that's really cool i mean so

[01:30:59.920 --> 01:31:05.600]  it looks like this this has been like uh you you seem to have like done a lot of interviews

[01:31:05.600 --> 01:31:12.000]  and podcasts about it uh maybe over the you know like three years ago and stuff um i'm reading

[01:31:12.000 --> 01:31:18.400]  your thing right now i when i was reading like your your vr stuff too i was i thought you were

[01:31:19.280 --> 01:31:24.800]  i was getting the vibe that you were interested in like integrating those two things too uh yeah i

[01:31:24.800 --> 01:31:31.680]  am i in the in the earliest podcast i talk about how it opens the door to having vr and ar without

[01:31:31.680 --> 01:31:37.520]  um direct uh brain stimulation so some of the software that i'm writing now for vr applications

[01:31:37.520 --> 01:31:44.320]  will will eventually be um neural a software that's great yeah i was trying to learn a frame

[01:31:44.320 --> 01:31:49.760]  actually because that seemed like the best thing that could at least communicate between people

[01:31:50.480 --> 01:31:54.640]  but i i mean i don't know but you know like i only know how to program in python and

[01:31:54.640 --> 01:32:00.560]  and batlab and stuff so i'm not like i'm so i i'd be happy to help you with a frame stuff

[01:32:00.560 --> 01:32:06.720]  because i lead a coding meetup and um we are it's really about helping people to to make

[01:32:06.720 --> 01:32:10.960]  whether it's our code and um in addition to aprim i'm really excited about um there's a

[01:32:10.960 --> 01:32:16.640]  there's a class on 3js it's it's really great that i'm that i'm taking and then um i'm really

[01:32:16.640 --> 01:32:23.680]  excited about react 3 fiber um because uh it has um mainly because it has it's it's it's a

[01:32:23.680 --> 01:32:30.720]  normalcy simplified the process of of creating uh really awesome uh 3d applications that run on the

[01:32:30.720 --> 01:32:37.360]  web but also it can run natively whereas whereas aframe uh doesn't run natively at the same speed

[01:32:37.360 --> 01:32:42.960]  it doesn't run at native speed if you run it if you run it in an app and unfortunately so um

[01:32:42.960 --> 01:32:48.560]  so i'm excited about react 3 fiber for that reason and also because of the fact that the the pool of

[01:32:48.560 --> 01:32:54.720]  react developers is much is is much more massive than the pool of of aframe or 3ds developers

[01:32:54.720 --> 01:33:00.960]  and so that means potentially um you know i think that all the facebook and and twitter and

[01:33:00.960 --> 01:33:05.920]  netflix all those big companies will they'll create the ar and vr versions of their websites will be

[01:33:05.920 --> 01:33:10.320]  the exact same websites they have now but they'll they'll be 3d and you'll see them in ar on your

[01:33:10.320 --> 01:33:15.360]  phone or with ar with your glasses etc but they would build with react react 3 fiber and i think

[01:33:15.360 --> 01:33:21.040]  those companies don't realize this yet but i but i i um i can i can see this trend happening because

[01:33:21.040 --> 01:33:26.160]  the because the masses of of react developers at facebook will will um will realize that they have

[01:33:26.160 --> 01:33:33.680]  really the best tool they have for creating vr is is with uh r3f or react 3 fiber so react should

[01:33:33.680 --> 01:33:39.600]  be something that i look into more than like it uh other things i think that just the sheer scale

[01:33:39.600 --> 01:33:44.720]  of the number of uh react developers means that it's it's going to become the dominant

[01:33:44.720 --> 01:33:50.560]  technology but um but but in the short term i'm i'm like um you know maybe i would i would say

[01:33:50.560 --> 01:33:57.760]  recommend maybe learn 3js because 3js is at the core of both aframe and react and if you are a master

[01:33:57.760 --> 01:34:05.040]  of 3js uh then then um then what you can do in both react 3 fiber and an aframe it's going to be

[01:34:05.040 --> 01:34:08.800]  it's going to be massive but i can help you a lot more with aframe stuff there's a whole community

[01:34:08.800 --> 01:34:16.400]  of people who are who can help with the react 3 fiber of stuff as well so so as a complete

[01:34:16.400 --> 01:34:22.880]  infantile like i i don't even know what 3js is what where what would be like a good intro uh

[01:34:23.520 --> 01:34:29.680]  i mean i i i know like i mean yeah so what would be a good place to start

[01:34:30.960 --> 01:34:37.440]  so um so one is that you can join my discord uh which is the xrweb.com and that will give you

[01:34:37.440 --> 01:34:42.960]  the link that will give you an invitation link to my discord and then um two and that's a developer

[01:34:42.960 --> 01:34:48.720]  discord for building web xr apps and and uh and we organize meetups around the topic and and there's

[01:34:48.720 --> 01:34:54.320]  lots of resources inside the discord to lots of different links to uh react through fiber to 3js

[01:34:54.320 --> 01:34:59.680]  to aframe and um lots of great code examples that show you some of the amazing things you can do

[01:34:59.680 --> 01:35:07.120]  with all of these and then finally um you could just go to 3js.org um and there's really excellent

[01:35:07.120 --> 01:35:14.400]  documentation and um 3js journey you can find on you can go to 3js journey uh just type it into

[01:35:14.400 --> 01:35:19.040]  google and that's a great class it costs it's usually 100 bucks but if you wait and if you just

[01:35:19.040 --> 01:35:23.920]  follow the guy on twitter and get his notifications eventually he's gonna say okay it's 40 off today

[01:35:23.920 --> 01:35:27.360]  for the first 20 people or something and then you just get it you get a good discount if you

[01:35:27.360 --> 01:35:34.640]  want to discount but um but it's only 99 bucks if you don't get a discount okay um and that's you

[01:35:34.640 --> 01:35:38.240]  know that's like way cheaper than the university class and the value you're getting from his class

[01:35:38.240 --> 01:35:42.560]  is just um i think it exceeds many university classes right they might cost a thousand bucks

[01:35:42.560 --> 01:35:47.920]  or it exceeds the value of many conferences that cost more than a thousand bucks i think so i i had

[01:35:47.920 --> 01:35:53.520]  no trouble paying it i was just like yeah this is worth it but um so 3js would be something to

[01:35:53.520 --> 01:35:58.960]  learn kind of first uh just because that will allow me to pivot anywhere yeah that 3js is the core

[01:35:58.960 --> 01:36:04.880]  and then um and of course um i so i i helped create a um i co-created an eg application

[01:36:06.880 --> 01:36:13.360]  that was working in uh webvr and we used aframe to do it so um so you so my code is open source

[01:36:13.360 --> 01:36:18.000]  and so you can immediately use that in the same application we're using because uh it basically

[01:36:18.000 --> 01:36:26.240]  it visualizes time series data like it takes the um we collect the the voltage signals from the

[01:36:26.240 --> 01:36:31.520]  sensors on the forehead and uh send them in they're collected as hex values by the machine

[01:36:32.480 --> 01:36:38.880]  and um then they're converted into numbers and then there we run some program to to reduce noise and

[01:36:38.880 --> 01:36:45.280]  and uh uh there's a there's a server some low pass low pass filters basically for the noise

[01:36:45.280 --> 01:36:51.440]  yeah low pass filters um i think uh then there's this is like 2018 so i'm like

[01:36:51.440 --> 01:37:00.240]  like is it is it i i get i get a feeling that like we have a lot of non i like our

[01:37:01.200 --> 01:37:07.280]  like our skills are like just touching but not almost like not overlapping like the things

[01:37:07.280 --> 01:37:11.280]  you're saying right now like oh god i'm you're probably like i'm going to go look and it's

[01:37:11.280 --> 01:37:16.720]  going to be so above my head at all the things you're telling me right now uh well i'm happy to

[01:37:16.720 --> 01:37:20.000]  i'm happy to like like again like if you if you connect with me on the discord we can go over

[01:37:20.000 --> 01:37:24.800]  the code and and and in one of the online coding meetups with the with the community and and teach

[01:37:24.800 --> 01:37:29.680]  everybody at the same time and and if you want we could record it and just share it with everybody

[01:37:29.680 --> 01:37:36.800]  but um or whatever but um but uh but yeah so so the so the signal i have i i i i guess we're

[01:37:36.800 --> 01:37:41.600]  kind of winding down but i want to see if i can squeeze in a couple more questions um so so the

[01:37:41.600 --> 01:37:47.680]  signals pass into so just so just so jeff hawkins he was talking about um the the fact that the a

[01:37:47.680 --> 01:37:54.800]  neural column um is is now going to represent uh you know an an object in in front of you like

[01:37:54.800 --> 01:37:58.960]  you're hoping you're holding a bottle and it's going to track the rotation of the bottle the

[01:37:58.960 --> 01:38:04.720]  appearance of the bottle the um uh the sounds that the bottle makes and all of your expectations

[01:38:04.720 --> 01:38:08.160]  of the properties of the bottle and it's going to sort of create a it's going to hold on hold on

[01:38:08.160 --> 01:38:22.640]  one second i'm having a glitch okay yeah okay so the speakers speakers were the wrong speakers

[01:38:22.640 --> 01:38:27.120]  were playing and it was talking over you sorry okay i'm listening now sorry about so so the

[01:38:27.120 --> 01:38:32.240]  idea in the thousand brains is that a neural column will um will is a every object in the world around

[01:38:32.240 --> 01:38:36.240]  you that you're seeing right now is going to be sort of managed by by a different neural column

[01:38:36.240 --> 01:38:42.560]  um and uh so so and there's lots of there's lots of ideas um that's just one of the ideas and some

[01:38:42.560 --> 01:38:46.720]  of the ideas are sort of conflicting and and that's why i sort of like there's some good questions

[01:38:47.680 --> 01:38:53.840]  and um so that so so one of the ideas is that a neural column is going to uh manage all the

[01:38:53.840 --> 01:38:59.600]  properties of that one bottle and so that you'll you'll have um uh the it'll manage all the the

[01:38:59.600 --> 01:39:04.480]  orientation of the bottle it's distance from you it's distance from your hand where your fingers

[01:39:04.480 --> 01:39:08.880]  are on the bottle uh what the bottle looks like what his appearance is what the bottle sounds

[01:39:08.880 --> 01:39:13.200]  like it has anything in it like all of these different all of these different um sensor streams

[01:39:13.200 --> 01:39:17.440]  come in through through different parts of your of your sensor apparatus and then they

[01:39:17.440 --> 01:39:23.840]  get they go to the thalamus and then and then um and and then they go to you know like you have

[01:39:23.840 --> 01:39:30.800]  at the point of the thalamus you have not yet formed the percept of the existence of a bottle

[01:39:30.800 --> 01:39:37.680]  that at the point of the thalamus it's still in the mode of like edge detectors and like pixels

[01:39:37.680 --> 01:39:42.560]  think of it as like pixels are now like a lot are now like it's like at the layer where it's

[01:39:42.560 --> 01:39:49.200]  like talking about lines maybe instead of pixels it's not there is no cohesive bottle yet that's

[01:39:49.200 --> 01:39:56.320]  probably two or three steps up the ladder okay um i i'm with you on that like you know but in

[01:39:56.320 --> 01:40:00.960]  in terms of like you know so it's taught it's it's commonly thought you know the the hierarchical model

[01:40:00.960 --> 01:40:07.200]  of learning you know the v1 might have lines and edges and then and then curves and then and then

[01:40:07.200 --> 01:40:15.840]  maybe the v2 will have a higher level of shapes right and then v3 would have shapes and would

[01:40:15.840 --> 01:40:20.240]  would you have complex objects with multiple shapes connected to them or something right

[01:40:20.240 --> 01:40:25.360]  and then yeah and so once you get to cortex those models start being like highly debatable but

[01:40:25.360 --> 01:40:30.320]  at least in thalamus you know it's like at still at some low level like that but yeah that's that's

[01:40:30.320 --> 01:40:34.400]  the idea so so he so he was talking about how in the mass brain the mass brain is mostly like

[01:40:34.400 --> 01:40:40.880]  level one cortex and so that means that for the mouse most most of its its world is just level

[01:40:40.880 --> 01:40:45.120]  one cortex even though there's six layers it's just you know the amount of tissue in level one

[01:40:46.480 --> 01:40:51.200]  it just dominates and and so the idea that he he was talking about in his book a thousand brains is

[01:40:51.200 --> 01:40:58.160]  that uh that we could have entire objects like whole complete objects in just the level one

[01:40:59.120 --> 01:41:05.200]  of the cortex and then um i started thinking about you know well you know people um uh

[01:41:05.200 --> 01:41:08.400]  sometimes say the eye is really part of the brain because there's so there's so many different

[01:41:08.400 --> 01:41:19.200]  layers to when to to um it's it's great so that's developmentally it's it's um so like

[01:41:19.200 --> 01:41:25.760]  developmentally there's three types of derm it's like ectoderm mesoderm and endoderm and so like

[01:41:25.760 --> 01:41:29.920]  the endoderm becomes like your stomach lining the mesoderm is like in between and the ectoderm

[01:41:29.920 --> 01:41:38.560]  becomes your skin and your brain so so basically um the eyes actually come out of the ectoderm

[01:41:38.560 --> 01:41:44.480]  that's not your skin so it cut your eyes developmentally your eyes literally are your brain and then

[01:41:44.480 --> 01:41:50.560]  the part of your brain your eyes actually develop from is is the same as the diencephalon which is

[01:41:50.560 --> 01:41:56.480]  the part that also has contains the thalamus so the so basically when you start describing things

[01:41:56.480 --> 01:42:04.720]  in a developmental sense that um the eye really is brain the eye is just sort of like a it's like a

[01:42:04.720 --> 01:42:12.000]  part of the brain that did its own thing um yeah so yeah it is it is part of the brain right it grew

[01:42:12.000 --> 01:42:17.280]  brain from cells that were set aside to be brain cells and it seems like in the eye itself we have

[01:42:17.280 --> 01:42:23.360]  these descriptions of um you know what like we covered earlier that uh there's there'll be cones

[01:42:23.360 --> 01:42:28.800]  and receptors in the eye that only track when an object moves from left to right um and when an

[01:42:28.800 --> 01:42:35.600]  object uh is a certain color and right and um so that means that like at the eye itself maybe I'm

[01:42:35.600 --> 01:42:40.320]  thinking I don't know if this is what it means but I'm thinking maybe edge detection and color

[01:42:40.320 --> 01:42:47.760]  detection and like the v1 type of activity is really happening in the eye itself um instead

[01:42:47.760 --> 01:42:57.440]  of in the v1 um well here Julia you're you're you're you work in the visual system I don't know

[01:42:57.440 --> 01:43:05.520]  how to get you okay all right well the retina um it's probably work no she's right she's around

[01:43:05.520 --> 01:43:14.000]  here it's like the other room we're I mean I'm like I mean we're in the same spot but I just

[01:43:14.000 --> 01:43:23.520]  oh okay get a page plus nope Julia oh she's at your house like literally yeah yeah

[01:43:26.320 --> 01:43:31.680]  it's funny it's actually also funny to me that I thought of that like like uh 20 seconds before

[01:43:31.680 --> 01:43:42.560]  it made it to to my language before I said it that's also funny where's the invite to speak

[01:43:42.560 --> 01:43:51.920]  there it is okay there anyways so like um so I don't think there's too much uh context that reaches

[01:43:51.920 --> 01:43:57.280]  that first of off the retina gets like basically no input from the brain it doesn't get a lot of

[01:43:57.280 --> 01:44:03.120]  input it doesn't get I don't think I mean maybe maybe some crazy sparse like non-functional

[01:44:03.120 --> 01:44:09.120]  like vestigial like one sort of axon gets in there to control the muscle or something but

[01:44:09.120 --> 01:44:14.000]  for it basically the retina doesn't get any information from the thalamus or from the brain

[01:44:14.000 --> 01:44:21.200]  and um so there's no context it's literally just like did this pixel get light flashed on it or

[01:44:21.200 --> 01:44:26.160]  not kind of thing uh there's there's a little bit more going on because like the ones that get light

[01:44:26.160 --> 01:44:31.920]  can sort of inhibit the ones next to it so there is a little bit of computation there but you

[01:44:31.920 --> 01:44:36.240]  definitely don't want to think of it as at like the level of like it it can formulate what an

[01:44:36.240 --> 01:44:41.920]  object is at that point it's the best just it can basically just detect edges at that point

[01:44:41.920 --> 01:44:50.480]  and then in the thalamus first well the thalamus sort of doesn't the thalamus cells sort of don't

[01:44:50.480 --> 01:44:57.280]  connect to each other really much at all so there's not a lot of like I mean there's modulation that

[01:44:57.280 --> 01:45:01.040]  could happen and sort of like ignore this or maybe pay more attention to this but there's not a lot

[01:45:01.040 --> 01:45:06.400]  of computation happening in terms of decoding what the object is in terms of the thalamus so

[01:45:07.120 --> 01:45:12.880]  I don't think any of the actual percept is formed until you get to be one once you get to be one

[01:45:12.880 --> 01:45:18.080]  there's all sorts of cells that are connected to each other and and connected outward and inward

[01:45:18.080 --> 01:45:24.560]  and in a circle and just like all sorts of crazy connections that are clearly doing some sort of

[01:45:24.560 --> 01:45:29.520]  computation but at least at the level the retina and then at the level of thalamus things are sort

[01:45:29.520 --> 01:45:34.800]  of like one thing connects to I mean I'm exaggerating here but like it's almost like one thing connects

[01:45:34.800 --> 01:45:40.240]  to one thing connects to one thing and it's just like passing it on a little bit with with some sort

[01:45:40.240 --> 01:45:46.400]  of math uh think of it as like a gross structure modulation but not it's not decoding much and

[01:45:46.400 --> 01:45:50.800]  then and then there's this huge jump right when it gets to be one now it's like really starting

[01:45:50.800 --> 01:45:56.240]  to figure out what it is and then v2 and now and since it's actually since thalamus is sending it

[01:45:56.240 --> 01:46:01.600]  to both those areas at the same time anyway maybe v1 and v2 aren't even the best names for it because

[01:46:01.600 --> 01:46:07.440]  v2 gets gets sent stuff directly from the thalamus too it's just it also gets sent stuff from v1 so

[01:46:07.440 --> 01:46:12.320]  now you just start everything starts getting connected to everything so clearly things are

[01:46:12.320 --> 01:46:17.520]  getting decoded there but yeah I definitely wouldn't start thinking that like the information of bottle

[01:46:17.520 --> 01:46:24.800]  is held in the retina or in the thalamus it's definitely the idea of of bottle I think sticks

[01:46:24.800 --> 01:46:30.960]  in the cortex and if if the cortex is making some prediction because the cortex does send

[01:46:30.960 --> 01:46:38.400]  a lot of information back to the thalamus um I don't know I uh I don't know how much the cortex

[01:46:38.400 --> 01:46:47.440]  predicts like bottle and then sends it back to the thalamus um but it's if it is getting sent

[01:46:47.440 --> 01:46:53.760]  that way it's definitely not just that because the thalamus is also getting the the the inputs

[01:46:53.760 --> 01:46:59.520]  from the outside world so so if the thalamus maybe the thalamus might be matching the actual inputs

[01:46:59.520 --> 01:47:04.160]  from the predicted inputs but that's that's the most I could think of it doing

[01:47:04.160 --> 01:47:13.520]  okay so um so so you were saying so the so the signals so Jeff the Hawking's idea was that uh

[01:47:13.520 --> 01:47:19.600]  you have a neural column and uh it's two ideas one is that a neural column will be uh managing

[01:47:19.600 --> 01:47:24.240]  all the properties of an object with a reference frame so you'll have part reference frame is like

[01:47:24.240 --> 01:47:28.240]  you have part of the the neural signal will be very stable that would be like at a higher level

[01:47:28.240 --> 01:47:34.320]  that might be the the v2 and then the the stuff in the v1 which be lower level will be changing

[01:47:34.320 --> 01:47:41.840]  rapidly and and so the reference frame idea works like you have like um um I guess it's I guess it's

[01:47:41.840 --> 01:47:47.600]  he described as like the relationship between like uh grid cells and place cells and um some of the

[01:47:47.600 --> 01:47:53.600]  some of the cells only change um a little bit because they're about where you are in the grid

[01:47:53.600 --> 01:48:00.400]  and some of them will change um uh rapidly because they're about the the changing context of

[01:48:00.400 --> 01:48:06.240]  of the location where where you're at like what's changing while you're there so so I read so I did

[01:48:06.240 --> 01:48:12.160]  my undergraduate senior thesis on predictive coding in visual cortex and my advisor had me read

[01:48:12.720 --> 01:48:18.160]  on intelligence which I think I've read like chapters five or six or whatever the chapters

[01:48:18.160 --> 01:48:22.240]  where he's actually describing the structure of feedback connections and predictive coding

[01:48:22.240 --> 01:48:25.440]  so I'm familiar with that what's new to me right now what you're talking about is like

[01:48:26.160 --> 01:48:34.400]  um I mean I I guess yeah what what what is new beyond that the on intelligence book I I know

[01:48:35.680 --> 01:48:42.480]  is analogizing or making making columns sound similar to some sort of like place cell structure

[01:48:42.480 --> 01:48:47.040]  or grid cell structure um and you're saying these are columns probably in like v1 or v2

[01:48:47.040 --> 01:48:53.360]  well so I mean so so by neural so a neural column of course is um you know there's 150

[01:48:53.360 --> 01:48:59.520]  neural columns large neural columns right and and they and they um uh they they represent

[01:48:59.520 --> 01:49:05.120]  in a you know everything from uh from v1 to v6 it's like it's like the entire stack right

[01:49:05.920 --> 01:49:11.040]  and so his in one of his papers he described how that entire stack of columns that might be um you

[01:49:11.040 --> 01:49:15.680]  know is it like a is it like a millimeter or centimeter I forget the the neural column but

[01:49:15.680 --> 01:49:21.600]  um so the you have like these like columns these big columns in some sensory areas like 400

[01:49:21.600 --> 01:49:25.680]  microns yeah but there's these things called and those are only in some places but there's

[01:49:25.680 --> 01:49:30.000]  these things called mini columns and they're predicted to be like everywhere and you can kind

[01:49:30.000 --> 01:49:34.720]  of just sort of if you squint your eyes just right you can kind of see them in between the

[01:49:34.720 --> 01:49:42.000]  blood vessels um and so that that idea of these mini columns or micro columns uh is that that

[01:49:42.000 --> 01:49:48.000]  they're like everywhere in the whole brain is like structured into these sort of mini columns and I

[01:49:48.000 --> 01:49:52.960]  think that's what you're talking about these and they're like 50 microns um wide and like yeah

[01:49:52.960 --> 01:49:58.480]  there's a gazillion of them but yeah yeah so basically the one the one of the hypothesis

[01:49:58.480 --> 01:50:02.880]  on that is that they're like sort of the functional unit of the brain like the way that the layer

[01:50:03.520 --> 01:50:08.320]  you know layer four cells go to the layer two three cells and the layer two three cells go

[01:50:08.320 --> 01:50:14.400]  into the layer five a cells and those go to the five b and six like that sort of uh motif is sort

[01:50:14.400 --> 01:50:22.320]  of consistent throughout the brain and uh and so there one thought is that those mini columns are

[01:50:22.320 --> 01:50:27.760]  the functional unit of the brain right right and um yeah so that was um there's a hundred by

[01:50:27.760 --> 01:50:39.200]  150 those 150 thousand or 150 no i don't think there's anything like that i think maybe in yeah

[01:50:39.200 --> 01:50:44.800]  maybe he gave an example of of like if you i don't know i i don't know that number no

[01:50:46.080 --> 01:50:52.400]  let me look this up uh so there's about a hundred million cortical cortical mini columns in the

[01:50:52.400 --> 01:51:03.040]  neocortex okay okay i got you so if the neocortex if a mini column is 50 microns wide and you divide

[01:51:03.040 --> 01:51:09.120]  the total surface area by that i can imagine you're getting about 150 million okay got it and it's

[01:51:09.120 --> 01:51:15.200]  also saying that um there's they each have 110 neurons each um giving uh one million to two

[01:51:15.200 --> 01:51:22.640]  million cortical columns so i'm what i'm like i'm not like confused what is he talking about like 150

[01:51:22.640 --> 01:51:28.480]  thousand i'm sort of confused what he was talking i need to look i need to compare this data um but

[01:51:28.480 --> 01:51:34.160]  uh because because definitely there's a big difference between like 150 thousand and and uh

[01:51:34.160 --> 01:51:40.160]  and two million cortical columns but no okay now that you said those things that you just read to me

[01:51:40.160 --> 01:51:46.000]  that sounds right like did you say like a hundred like a thousand neurons per mini column or 150

[01:51:46.000 --> 01:51:53.920]  neurons per mini column or something like that so 110 um neurons per mini column okay yeah yeah

[01:51:53.920 --> 01:51:57.680]  that's that sounds yeah that sounds right okay so i get what you're talking about you are talking

[01:51:57.680 --> 01:52:03.280]  about mini columns and yeah there's a bunch of them like uh a million million or millions

[01:52:03.280 --> 01:52:10.720]  uh over the whole brain got it so so so so so Vernon Mount Castle in the um in the 40s 50s and

[01:52:11.520 --> 01:52:18.320]  70s what was doing John Hopkins yeah he was the first person to describe columns he described

[01:52:18.320 --> 01:52:23.120]  them in the somatosensory cortex yeah and it's it's always funny that kuhl and weisel got the

[01:52:23.120 --> 01:52:29.200]  Nobel Prize for columns and visual cortex when Vernon Mount Castle like discovered it first

[01:52:29.200 --> 01:52:35.120]  and and he discovered all this other stuff first and it's almost like everyone knows like that guy

[01:52:35.120 --> 01:52:39.200]  like discovered so much stuff but he somehow didn't win the Nobel Prize and all these other people did

[01:52:40.560 --> 01:52:47.280]  yeah that's just like that's uh that's very strange huh i didn't know this but um

[01:52:49.280 --> 01:52:56.160]  yeah totally oh man um okay so the idea is that um so that you'll have a single neural column

[01:52:56.160 --> 01:53:03.360]  will then uh you know will have like activity towards the top that is very stable and the

[01:53:03.360 --> 01:53:08.560]  inputs at the bottom of the column will be very will be changing a lot and that's because the um

[01:53:09.200 --> 01:53:18.080]  the column itself is maintaining the um either an object or uh or a word and and at the top

[01:53:18.080 --> 01:53:23.920]  and that's the reference frame and the bottom is um is the references or the properties of that

[01:53:23.920 --> 01:53:28.640]  reference frame so the reference frame can represent an object or or um or the idea of an idea

[01:53:29.360 --> 01:53:34.400]  right and all the all the connecting all the multimodal connections to the idea of the idea

[01:53:35.440 --> 01:53:41.040]  um so superficial layers are holding more contextual information about the thing and the

[01:53:41.040 --> 01:53:50.400]  deep layers are deep by deep i mean like lower uh are are um holding more actual information about

[01:53:50.400 --> 01:53:57.120]  the thing is everything yeah so so the no so the deeper layers would be like um uh would be

[01:53:57.120 --> 01:54:03.920]  would be saying that okay this this thought the notion that uh this uh space is going to hold

[01:54:03.920 --> 01:54:09.440]  is going to be the notion of a bottle and then it's almost like it's almost like the um maybe

[01:54:09.440 --> 01:54:13.440]  maybe like an entity component system what i don't know if you're if you're familiar with

[01:54:13.440 --> 01:54:21.600]  entity components systems but um it's like you have like um it's like um so i i guess this

[01:54:21.600 --> 01:54:29.760]  won't be a good example um but it's it's it's it's like um it's like uh a way for the brain to track

[01:54:29.760 --> 01:54:35.840]  objects um so you have like a space you have like a a bunch of different spaces each column

[01:54:35.840 --> 01:54:41.120]  will be a space where a different object is tracked and um but that object has a lot of

[01:54:41.120 --> 01:54:47.200]  properties and the properties are multi-sensory and like i said it's like how is the how close

[01:54:47.200 --> 01:54:52.560]  is the object to you what does it sound like um you gotta what's it feel like what's it look like

[01:54:52.560 --> 01:54:57.200]  what's it sound like yeah and and so well what i feel like those are gonna be a different air so

[01:54:57.920 --> 01:55:02.640]  so each area might have you know however many mini columns so there's the visual area there's the

[01:55:02.640 --> 01:55:08.960]  auditory area and so the visual areas just cares about vision the audit auditory area just cares

[01:55:08.960 --> 01:55:15.760]  about auditory but then you have these higher so once you go past like v1 v2 v3 v4 then you're

[01:55:15.760 --> 01:55:21.280]  getting into all these like higher order areas they're multimodal so they mix sound with touch

[01:55:21.280 --> 01:55:28.000]  with with vision and so so this is like in the parietal cortex or like you know areas that are

[01:55:28.000 --> 01:55:33.920]  known for multimodal integration and so i feel like the context of this conversation might be

[01:55:33.920 --> 01:55:39.200]  be more applicable if we're saying like maybe in the parietal cortex the mini columns there

[01:55:39.760 --> 01:55:46.000]  are integrating all this information and maybe hold the are holding a concept of of an object

[01:55:46.640 --> 01:55:54.560]  and yeah it's not known i don't think how how these different modalities like touch vision

[01:55:54.560 --> 01:55:59.680]  etc get integrated in those areas that well you would know more right Julia like how they're

[01:55:59.680 --> 01:56:05.040]  integrated in those areas do you know anything i wish but i don't feel like enough to contribute

[01:56:05.040 --> 01:56:10.320]  to this beyond the fact that yes there are areas that are definitely responsive to some combination

[01:56:10.320 --> 01:56:18.400]  of sensory features well like warnex warnex area you know it's it's it's it's many right many

[01:56:18.400 --> 01:56:23.120]  different uh because language has a visual and and auditory components and they have to come

[01:56:23.120 --> 01:56:33.040]  together somewhere right so maybe um um warnex area i only know it from like if there's an

[01:56:33.040 --> 01:56:39.200]  issue where somebody has warnex aphasia which is damaged to that area they talk and talk and talk

[01:56:39.200 --> 01:56:44.480]  but they don't make sense so they'll be like yeah go to the thing and like none of their sentences

[01:56:44.480 --> 01:56:50.960]  actually mean anything um and and so like so they kind of don't know what they're saying whereas

[01:56:50.960 --> 01:56:55.680]  brokes area maybe they do know what they're saying but they can't actually say anything they just say

[01:56:55.680 --> 01:57:03.600]  the same word over and over or something so so uh i i wouldn't i mean i don't know enough about it

[01:57:03.600 --> 01:57:09.200]  to say for sure but i kind of want to say that i wouldn't call those like language integration

[01:57:09.200 --> 01:57:17.280]  areas but i do think they're probably there do exist premotor areas that probably do integrate

[01:57:17.280 --> 01:57:22.480]  language and maybe they are like the warnex are involved in it maybe there's areas around it also

[01:57:22.480 --> 01:57:27.200]  involved in it yeah that probably represent what you're talking about where like you're you have

[01:57:27.200 --> 01:57:32.240]  some thought on the edge of your tongue and it involves this vision that you have or a sound

[01:57:32.240 --> 01:57:37.200]  that you have you know like i can imagine that somewhere um yeah i mean i just realized that

[01:57:37.200 --> 01:57:43.120]  the warnex area a damage to warnex area could could could be uh disrupting like um the oscillations

[01:57:43.120 --> 01:57:48.960]  that allow the the coordination of of um of many different patterns that affects language most

[01:57:48.960 --> 01:57:53.120]  most visit most visibly but it may not have anything to do with integrating language

[01:57:54.000 --> 01:57:58.800]  yeah oscillations is a super contentious topic and that's right in the middle that's where my

[01:57:58.800 --> 01:58:05.840]  project is dead on and uh you know i can there's not a lot known in a definitive sense about what

[01:58:05.840 --> 01:58:10.640]  the hell oscillations are doing which is why like i've had old old advisors be like don't do it don't

[01:58:10.640 --> 01:58:16.160]  go in that field that field has been studied for 30 years and it's still like nobody knows for sure

[01:58:16.160 --> 01:58:22.000]  anything but but yeah there's a lot of theories about oscillations that's for sure um just not a

[01:58:22.000 --> 01:58:30.640]  lot of uh consensus yet um okay so then so so in terms of how information travel like my whole

[01:58:30.640 --> 01:58:35.600]  my whole question journey was like about how information travels through the brain so then

[01:58:35.600 --> 01:58:43.440]  it comes in through the b1 and it comes in through the uh um you know the the a1 or the audio cortex

[01:58:43.440 --> 01:58:48.800]  one and and this and the touch stuff comes in through the somato sensory cortex right yeah yeah

[01:58:48.800 --> 01:58:56.320]  b1 and a1 and s1 the that's the primary court court disease and then um uh and and then what

[01:58:56.320 --> 01:59:04.640]  happens is at some point um which you see here in touch uh come together and um and i am not exactly

[01:59:04.640 --> 01:59:10.240]  sure what where they come together but um but maybe the where they come together is um i have to look

[01:59:10.240 --> 01:59:16.640]  that up unless you know but but where it would be somewhere like the parietal cortex or the posterior

[01:59:16.640 --> 01:59:22.240]  parietal cortex yes but in general just you can just refer to it as multimodal areas and just leave

[01:59:22.240 --> 01:59:30.800]  it as unknown prida and post-pridal or just um multimodal areas yeah and that way you don't have

[01:59:30.800 --> 01:59:39.680]  to be specific yeah multimodal area is really the keyword um so is it possible that um that there's

[01:59:40.320 --> 01:59:50.160]  any traffic backwards to uh to the original the the primary um yeah there's probably recursive

[01:59:50.160 --> 01:59:55.200]  like uh feedback yeah there's tons of it there's probably even more of it than there is before

[01:59:55.200 --> 02:00:03.680]  it's just like yeah there there there's almost no place where there isn't there's also like these

[02:00:03.680 --> 02:00:09.120]  dual pathways going through the fallowness so like you think of the traditional kind of hierarchy

[02:00:09.120 --> 02:00:14.960]  going through the cortex where you have like okay the eye to the lgn the first order visual

[02:00:14.960 --> 02:00:20.560]  limb nucleus and then primary visual cortex and then that's going to secondary visual cortex let's

[02:00:20.560 --> 02:00:25.200]  say and then onwards from there but then you also have like these complementary loops through the

[02:00:25.200 --> 02:00:30.400]  thalamus where it's going from primary visual cortex to like this higher order part of the

[02:00:30.400 --> 02:00:37.360]  thalamus called the pulvanar and then that's also going to v2 so it's like for every cortical kind of

[02:00:37.360 --> 02:00:43.440]  hierarchical jump you also have these um like parallel pathways through the thalamus yeah

[02:00:43.440 --> 02:00:50.320]  difference copies um okay so then um i wanted to talk about okay so so when you said that there's

[02:00:50.320 --> 02:00:58.400]  more um feedback than feed forward um are we talking about horizontal feedback there's all

[02:00:58.400 --> 02:01:06.400]  sorts so like so v1 has all these projections to v2 but then v2 has a bunch of projections back to

[02:01:06.400 --> 02:01:12.720]  feed one or back to v1 and sometimes there'll be more in number but like maybe each one will be weaker

[02:01:12.720 --> 02:01:18.320]  you know what i mean so you can't so more is a very uh loose term there because it's like yeah

[02:01:18.320 --> 02:01:24.400]  there's more axons but they make weaker connections so in a functional sense you know maybe they're

[02:01:24.400 --> 02:01:28.720]  weaker maybe they're about the same but yeah they're basically there's just think of it as loops

[02:01:28.720 --> 02:01:34.480]  everything it's just like nested loops i think is the best way to think about it okay um so so then

[02:01:34.480 --> 02:01:39.840]  i was reading about you know um the proposal for short term short term memory a recent paper

[02:01:40.640 --> 02:01:44.880]  they were saying that you know they were saying that i think i may have you may have heard me

[02:01:44.880 --> 02:01:50.960]  explain this but they're they were saying that a neuron could might its activity might last um

[02:01:51.840 --> 02:01:56.000]  for for 10 milliseconds it's total activity i guess is what you've clarified earlier but then

[02:01:56.000 --> 02:02:00.960]  if a neuron could talk to another neuron it could remind they could remind each other and and um so

[02:02:00.960 --> 02:02:08.560]  a neural circuit could carry a memory longer than a single neuron could um if if the if the the loops

[02:02:08.560 --> 02:02:16.880]  between um you know neurons in in clusters or micro micro circuits or micro uh so the question

[02:02:16.880 --> 02:02:21.680]  yeah micro shirt yeah so like basically if there's loops of individual neurons or small

[02:02:21.680 --> 02:02:26.960]  sets of neurons they'll be able to maintain activity between each other and oftentimes that

[02:02:26.960 --> 02:02:33.280]  activity uh will will result in some sort of oscillatory thing that you can read out on like a

[02:02:33.280 --> 02:02:37.840]  EEG electrode and you'll see like an oscillation and stuff um are you familiar with that study um

[02:02:37.840 --> 02:02:42.800]  do you know if um when they were talking about like um micro micro neural circuits were they

[02:02:42.800 --> 02:02:52.240]  talking about micro columns is that sort of like different or is micro circuits okay so micro

[02:02:52.240 --> 02:02:57.600]  columns is not a term that everyone uses but it it it's it's that term you're talking about

[02:02:57.600 --> 02:03:04.880]  and uh micro circuits describes a a type of neuroscience so it's basically just describing

[02:03:04.880 --> 02:03:10.560]  a scale so like if you're talking about EEG you're getting you're sticking a probe down in there

[02:03:10.560 --> 02:03:16.800]  and you're getting a readout of like the net of like a hundred thousand neurons right if you're

[02:03:16.800 --> 02:03:21.840]  getting an LFP recording you stick the probe down there and you're getting like a readout of like

[02:03:21.840 --> 02:03:29.280]  the nearest maybe 300 neurons around that LFP recording um and then you know in imaging you

[02:03:29.280 --> 02:03:33.520]  can zoom it you know you can image like the whole on a gross scale or you can zoom in and

[02:03:33.520 --> 02:03:38.800]  actually see individual cells and I think micro circuit neuroscience basically just describes

[02:03:38.800 --> 02:03:46.720]  getting really in there and looking at the the interactivity of individual cells and cell types

[02:03:46.720 --> 02:03:54.480]  whereas um you know a more gross level neuroscience would be like yeah we scanned the brain and v1

[02:03:54.480 --> 02:04:00.000]  lit up and v2 lit up and v3 lit up which says nothing about the individual cells in that area

[02:04:00.000 --> 02:04:07.920]  yeah okay and do you know so um I read something that uh the the micro circuits or the clusters

[02:04:08.720 --> 02:04:15.280]  or the micro columns were in in the mouse where we're range in it uh were spaced out in a hexagonal

[02:04:15.280 --> 02:04:22.960]  pattern uh does that make does that ring a bell are you talking about grid cells which are placed

[02:04:22.960 --> 02:04:31.200]  in a uh triangular pattern uh so so I so I guess I'm not because this is referring to the the

[02:04:31.200 --> 02:04:36.960]  mouse I believe this is referring to the mouse neocortex um and but it really made me think of

[02:04:36.960 --> 02:04:41.360]  grid cells because they were describing uh heck I have to maybe have to dig up the paper send it

[02:04:41.360 --> 02:04:48.000]  to you after after after this but there I was wondering if if um if the hexagonal pattern of

[02:04:48.000 --> 02:04:53.520]  so you've got clumps of cells and I I'm in there they're calling them neural circuits but it sounds

[02:04:53.520 --> 02:04:58.720]  to me like you're describing these they they they take the shape of clusters and I thought

[02:04:58.720 --> 02:05:02.960]  they're describing micro columns they're not using the language of micro columns and then

[02:05:02.960 --> 02:05:08.480]  someone else is saying that these um micro columns are arranged in in hexagonal patterns and now I'm

[02:05:08.480 --> 02:05:15.520]  thinking about grid cells in the neocortex of the mouse um and uh so this is all like this

[02:05:15.520 --> 02:05:19.440]  this is actually two different papers and maybe I'll send them to you later but but have you

[02:05:19.440 --> 02:05:28.320]  heard anything about this or maybe I'm just I'm looking at a paper right now talking about it but

[02:05:28.320 --> 02:05:32.960]  it's not something I'm familiar with it it like sounds unfamiliar when you mentioned it but not

[02:05:32.960 --> 02:05:42.160]  enough for me to comment on it so please do send those what sorry please do send those papers that

[02:05:42.160 --> 02:05:46.400]  you're thinking of all right so I'll send those later and then maybe we can have another conversation

[02:05:47.280 --> 02:05:54.320]  yeah and then um I wanted to ask um so so what about um so information also travels uh we've

[02:05:54.320 --> 02:06:00.160]  talked about traveling you know vertically and and up and down and horizontally uh

[02:06:02.160 --> 02:06:09.360]  from the primary to to the to the other areas of cortex and back and then um what about going

[02:06:09.360 --> 02:06:14.800]  up from from from the bottom to the top and then back to the thalamus can you describe that pathway

[02:06:18.160 --> 02:06:22.160]  uh I think that's the pathway we studied are you talking about the thalamic loops

[02:06:22.800 --> 02:06:29.200]  yeah uh so yeah so um I'm just so have we already covered that so what I'm saying is like so

[02:06:29.200 --> 02:06:34.960]  information comes from thalamus to the to v1 and then goes up to v6 and then from v6 back to the

[02:06:34.960 --> 02:06:41.840]  thalamus is that right uh no think of it like this so it goes from thalamus to v1 to v2 to v3

[02:06:41.840 --> 02:06:49.760]  to v4 to v5 etc right except it's not just that v5 or v6 goes back to thalamus v1 goes back to

[02:06:49.760 --> 02:06:54.960]  thalamus v2 goes back to thalamus v3 goes back to thalamus v4 so they all go back to thalamus

[02:06:54.960 --> 02:07:00.800]  and it's a nested loop like that oh they're okay okay so they're all they're all in their own loop

[02:07:00.800 --> 02:07:05.680]  yeah so so it's really interesting to sort of think think of the brain in terms of loops because

[02:07:05.680 --> 02:07:10.320]  we were talking about um the the the neuro scientist who was on clubhouse was was uh

[02:07:10.320 --> 02:07:16.720]  who's writing the rhythms of the brain in the brainstem uh he was he was uh he he he drew out

[02:07:16.720 --> 02:07:20.560]  this picture I don't know if you were there for that top for his talk but yeah yeah yeah like David

[02:07:20.560 --> 02:07:25.200]  or something yeah that's the same and and so he so he he he drew out this he said this is how I

[02:07:25.200 --> 02:07:31.600]  imagine this works that there's there's a bunch of arrows and they're and the arrows are um all

[02:07:31.600 --> 02:07:36.400]  arranged in uh you know they're all spaced apart from each other and they're all pointing different

[02:07:36.400 --> 02:07:42.400]  directions and if they point inwards and that means that um that it's it's able to the it's able

[02:07:42.400 --> 02:07:47.760]  to retain its its its shape and if they all start pointing outwards and then it falls apart and the

[02:07:47.760 --> 02:07:52.880]  organism dies because it stops breathing and the arrows the direction of the arrows is represented

[02:07:52.880 --> 02:07:59.360]  by a synaptic activity and um and they and they represent um you know like if you inject the mouse

[02:07:59.360 --> 02:08:08.320]  with uh with an opioid or or I think an an opioid opioid simulator um some other drug that does

[02:08:08.320 --> 02:08:14.000]  accomplish this the same effect but basically it causes the the synapse to malfunction then um

[02:08:14.000 --> 02:08:18.800]  they're not able to make the mouse is not able to maintain its its its equilibrium uh at the

[02:08:18.800 --> 02:08:24.320]  brainstem and then it sort of like loses control and then um but then but I you know I typed it

[02:08:24.320 --> 02:08:28.800]  into google it just like I just typed out exactly what he said and I looked at it at the field of

[02:08:28.800 --> 02:08:36.400]  arrows and and and and I and this is it was like this is exactly what um it's a it's a sort of

[02:08:36.400 --> 02:08:45.840]  description of a dipole of an electrical dipole and so it's that's so amazing it's like this is

[02:08:45.840 --> 02:08:50.400]  the description of a dipole and so here we have like the whole brain and you know is you know

[02:08:50.400 --> 02:08:55.600]  has dipole activity and then it's like a dipole inside the brainstem maybe you know in sort of

[02:08:55.600 --> 02:09:00.320]  like his description and then we've got all these loops and we've got small loops and loops inside

[02:09:00.320 --> 02:09:08.240]  other loops and it's like this and this um this fractal of of loops and and um and maybe a fractal

[02:09:08.240 --> 02:09:13.760]  of dipoles and it's just so so interesting and I don't know if we should leave it there for for

[02:09:13.760 --> 02:09:22.000]  tonight but then yeah totally and and uh also these these neurons are being held at a specific

[02:09:22.000 --> 02:09:29.520]  voltage because they're holding a net uh negative charge and then the so that means the extracellular

[02:09:29.520 --> 02:09:38.080]  space is net you know to the neuron is 60 milliwatts more than them and so each of these neurons kind

[02:09:38.080 --> 02:09:44.080]  of are like in a way a pole and then whenever there's current well I guess once there's currents

[02:09:44.080 --> 02:09:48.480]  traveling through them then you have these poles and then they and since all these neurons are a

[02:09:48.480 --> 02:09:53.120]  little all the least excitatory neurons which are most of them they're aligned in the same direction

[02:09:53.120 --> 02:09:59.840]  so so these poles sort of summit and that's how you get these readouts from the EEG which is on

[02:09:59.840 --> 02:10:04.880]  top of the skull you can think of it as like miles away from all the neurons that you have this giant

[02:10:04.880 --> 02:10:09.360]  skull separating them but you're still being able to read out this electrical signal because

[02:10:10.000 --> 02:10:15.360]  everything is aligned you know with each other so these these uh poles are sort of summating

[02:10:17.520 --> 02:10:20.960]  wow um and um

[02:10:27.840 --> 02:10:31.040]  okay so this is definitely there's definitely a lot more to talk about

[02:10:31.040 --> 02:10:36.640]  about in the future but at some point we've got to uh we're gonna say that's uh that's enough for

[02:10:36.640 --> 02:10:46.000]  one day yeah totally uh yeah I'd love to bug you about like other stuff anytime um yeah follow me

[02:10:46.000 --> 02:10:51.120]  back I don't think I have a picture on my twitter but I'm the only scott prop maybe that followed you

[02:10:51.120 --> 02:10:56.320]  yesterday or two days ago is it on your profile let me see if I can find oh so it's not so maybe

[02:10:56.320 --> 02:11:01.680]  that's why I couldn't find you because it wasn't on your your um yeah yeah it's not on my stuff okay

[02:11:01.680 --> 02:11:10.160]  let me look at that where and I'll see if I can find so you could probably just follow Julia first

[02:11:10.160 --> 02:11:15.360]  and then I'll I'm on her small group of follows oh okay let me make sure I'm following Julia

[02:11:15.360 --> 02:11:20.960]  there's a few profile I haven't logged in in a while so I haven't followed you yet but I will

[02:11:20.960 --> 02:11:27.440]  cool um let me see if I can see followers followers no I need to look at following

[02:11:29.120 --> 02:11:40.400]  um but I scott here's followed by scott so scott follows you okay I am as is scott s us i yep

[02:11:40.400 --> 02:11:46.800]  so I am following you okay oh you are okay cool yeah and in fact in fact you're you're actually

[02:11:46.800 --> 02:11:53.920]  um uh one of the people that I receive a hundred percent of all the notifications from so I like

[02:11:53.920 --> 02:11:58.560]  don't ever post anything on there but yeah cool well if you ever do I'm guaranteed to see it

[02:11:59.920 --> 02:12:04.000]  well I'll send you that article actually that I wanted to send you a couple days ago that was

[02:12:04.000 --> 02:12:08.160]  because we were talking about neurolink and I was like kind of making the argument that they're a

[02:12:08.160 --> 02:12:13.680]  little heavy on the engineering side and not caring as much about the neuro side as as possible

[02:12:13.680 --> 02:12:17.920]  yeah I know they care about it some but like there was an article I felt like expressed it well that

[02:12:17.920 --> 02:12:23.920]  I wanted to share with you but I'm definitely gonna look up uh those uh podcasts that you did uh oh

[02:12:23.920 --> 02:12:29.200]  yeah and if you if you send me anything uh there was a couple things you said that'd be easier to

[02:12:29.200 --> 02:12:35.920]  send me you should totally send uh direct message me on Twitter or whatever that's tough sounds good

[02:12:35.920 --> 02:12:44.400]  um all right cool all right all right have a good night yeah this is really good thank you bye thank you

[02:12:44.400 --> 02:13:08.800]  bye