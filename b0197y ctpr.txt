b0197y ctpr
(audio transcription needs edit)
Scope of book part 1 Neurons Brain Sort
00:00
Okay, so I need to write a whole section about everything that I know about volumetric video and 3D semantics segmentation and voids and the game of life. And what and why that's interesting and I need to, right? So yeah, that's like that's the 3ds semantic. Segmentation. And so I need to walk through all of that.
00:29
Like this is like how do I, how does the machine to object segmentation and how does a human being do objects orientation? And I need to explain how a like imagine a 3D neuron network and unity. It was built in unity or built-in unreal engine or built-in? Yeah, let's do unity.
00:54
Okay, had better luck with the idea or we could say where we are right. The idea is like a what if we built a 3D known network that was made out of boards, but the boys had cages and so the boys actually over the boys represented the neurotransmitters inside the cages.
01:20
And and so they each like the game of life, they're going to change their position based upon incoming data and their position in space represents the the 3D known network reworking itself. And so I think of like a three known network. So, so we have semantics segmentation is figuring out which which dots belong to together which dots are, you know, belong in the same group and then you have a idea noising and retracing the AID.
01:59
Noising is is this you know, training machine really high resolution pictures and then you can give it like a lower resolution picture in the AI sort of this sort of like polishing the picture. It's sort of like finishing the picture. It's like you're giving it part of an image and it finishes the rest of it.
02:20
That's what that's what everyone that worked does and AID. Noising is sort of like this same process but sort of like applied to like a noisy point cloud. That represents the picture you're creating and then but you're sort of like, in, you know, it's chasing out the noise, it's improving the image quality.
02:44
And and the speed at which you can get their workflow done so that you can get really nice rendering. So you can do your modeling and moving around, a really complex scene, really fast and are to in order to do like advanced, you know, ray traced seems that look really beautiful and so that you noising is this sort of like they I applied to point clouds in each frame and in the three dismantling segmentation is going to step further in its identifying objects and then again and and connecting objects to its classifying.
03:31
This the semantic connections that those objects have, it can turn 3D objects to 2D objects. They can do shape completion. It can do gans synthesis and you know, where you can, you know, against this could be like, what is it? What is the interpolation between an avocado on a chair as I illustrated by JP3?
03:55
We're really, really thinking about pointing at plus plus, which still outperforms even multimodal networks by opening AI, and by deep mind. So and by faceobok, so we're still like in terms of like point cloud three, semantics segmentation point cloud networks because the data that it's learning is multi-dimensional, it doesn't suffer as much from the issue of noise.
04:32
And so it's more robust and it doesn't need, you know, techniques like dropout as much or like, like right. It doesn't need a sort of like modus, networking modifications to deal with overfitting. It's overfitting. It's not going to be much of a problem because the
05:06
Because the I think it could be because what's being learned is more robust because it's because it's like there's a higher number of of vectors and between dots connecting between dots and a three-dimensional image and there is and a two-dimensional image and that higher density of vectors is giving us sort of a sort of function as or texture or weight.
05:48
So the image. So it's it's it's a little bit more digestible to a known network. I know that work that is basically digesting and then reconstructing, it's digesting. It's the image in a sense and then, but it's doing it in a way. Like where every piece of the network is is contributing is moving every, every note in that network is moving in a way, not moving.
06:21
Well, I mean moving in and I mean, in a convolutional network, there's nothing moving, but in a 3D, neural network. There's this sort of there's a 3D shifting of positions which is like an analogous to the changing of weights in a in a convolutional network. And let's see, that's still going.
06:51
And so, yeah. And so, okay. So we're discussed that we need to be all about the volume metric video and how we are. Oh yeah. So I was saying, I thought of an exercise, okay? So it's like what I want you to do is like it's like stretch your arms, man.
07:11
Okay, stretch your arms out so that your hands are so that you, it looks forward. So that you, you can't see stretch your arms up to your side. So you can't see your hands. Okay, you got to look for it though because if you look to your left or to your right, you'll be able to see your hands.
07:35
And that is your hands. Are we out in the periphery? And started. Here. Okay, you I can't say my hands. When they're stretch out to to my side I don't know if you can maybe your body is shaped differently or something, but I I can't see my fingers if they're both if both hand or stretch out to my sides.
07:59
And so the point is like, so just imagine your now that you're doing this exercise, consider the whole scope of everything that you can see and this moment, right? What is the consider your field of view? See your field of view is like 220 degrees, okay? I have to look this up to see what the human field of you actually is but it'll say it's for now, it's 220 degrees and your hands are just outside that.
08:35
And if your hands are just outside your field of view, you're not gonna be able to see. So, I mean, I guess your hands be pretty hand out to each side and they're at 108 degrees, right? And then like if you bring your hands in you know just just a little bit.
08:52
Yeah, then they can become visible in your periphery. Just sort of like study the edge of your periphery by looking straightforward. Moving your hands like closer and then farther back until they kind of like disappear, right? And the point is to just really like understand your field of view as a as an image as like a movie or as like a scene in virtual reality, that has to be rendered.
09:23
Now, imagine you put on the Oculus quest headset. And this covers your eyes and it renders an image to and the image changes based upon where you're looking. So, if you air in a VR headset and you looked to your left, you will see what the if you're in a, if you're in a, if the, if you're in the yard, you're playing a program.
09:53
Like I want to say that's a popular one. Gosh, if you're playing half-life Alex, and you looked to your left and you can see there's a wall to your left and a look to your right. And there's a, there's a train to write. And this thing is, is the thing about the viewer headset, right?
10:23
So the viewer head side is traveling on your face and as your physical head turns to the right, it's going to change the image here. Now, the reason I'm trying to get you to think about this is that the veer headset is generating image based upon where your head is pointed.
10:41
But let's go one step further and and say that your brain is generating an image based upon where your head is pointing. But your brain is essentially doing for you for your consciousness. What virtual reality is doing for for so I want you to see that there is a sort of analogy between your your vision and what you can see in the whole scope of your field of view from from your abstract left arm to the out stretch, right?
11:16
Arm and the few that has to be rendered for a beer headset to work correctly. When you're turning left or right? There's an analogy there. And I think that, that that's part of why John Carmack, had the great idea to start working on, on John Carmack from Oculus. And also the creator Doom that he had the great idea to sort of start working on on artificial general intelligence because here, you have with BR the sort of analogy for human vision, right?
11:53
It's an analogy. Now, the, the computer is not seeing this computer's not observing this image, right? So you get to this, you get to this, you know, question that people bring up well. Okay, if you're brain is rendering an image, okay? Who's observing it? Where's the where's? The observers are a little man inside your head.
12:15
That's observing it now, but but I think we do have we do have a way to to solve this but but, but I just want to say that, let's let's say that. At the very least, if you get outside your own head for a second and imagine an animal and like like a dog.
12:35
I saw this video once of a dog in a kitchen and the dog was looking at a it was in dog's indication. It was looking up at the table actually. There was a, there's like a forget what it's called, but there's like a there's like a little table in the middle of the kitchen.
12:54
Okay, it's looking up at this little table and then it's looking up over at the take the, the countertop on the, at the edge of the kitchen. So, it's looking from, from the ground up to the, it has, to do the, you can't, the dot can't look and move its head in two different directions in the same time.
13:12
So it has to do this in a certain sequence. So, first dog is looking at the floor and then the dog looks at the table in the middle of the kitchen and then the dog looks over at the countertop and there's like food on the countertop and then the dog is like looking at a chair.
13:28
And then I see the dogs sort of like walk through the steps again. So dogs then looks back at the floor, looks at the counter and the looks at the, the sort of it's not. It's it's like a kitchen counter. That's in the middle. It's not like a table in the middle.
13:44
Take a kitchen counter in the middle of the kitchen. That's not connected to any walls. The dog looks up there on the square thing in the middle, the counter in the middle and then the dog looks again over at the at the regular counter. That has a food on top of it.
13:58
Looks at the food. Looks like the chair and then it does the cycle again. So that doctors one more time where it looks at this at the center median medium and then looks at the the kitchen counter with food on it and it looks at the chair and then the dog
14:26
Uses a chair. It steps up onto the chair. To grab the food off the counter and then gets back out of there and in in between the dog is also looking to see if anyone's watching. See if any other you know if any human beings are in the vicinity, that might observe the dog taking the food off the kit off the kitchen counter.
14:49
But but this this is this is all captioned video. So so I guess a dog was being really sneaky and didn't expect that. Someone would be videotaping is no concept of video taping and dogs mind or so. We think, but it's but it was caught on camera and was really interesting.
15:07
And so if you think about this now we talked about how you know, when you look when you look when you look straight, when you look straight ahead and maybe our headset, you're being you're being. There's an image that's being ranchered when you're looking straight ahead. So let's think about this, was a dog.
15:24
Some of the dog is looking straight ahead. The dog. Spraying let's argue. I'm gonna argue the dog is brain is rendering the chair to the dog. It's rendering the medium and in the middle of the kitchen, the sort of countertop in the middle of the kitchen. It's rendering the countertop on the edge roof, the kitchen.
15:45
It's rendering the food to the dog but it's doing this in a sequence. Just like a like a movie, you know? Or like a like a VR movie like a volumetric video and and and so I don't know if you've ever been in volumetric videos, make live volumetric videos like that like versus thing called hype VR that I saw at CES and height VR was this was was my was my first, it's it.
16:14
I've really had the opportunity to try volumetric video but it's really impressive because you can, you can lean to your left into your right? And you are moving inside the video. So you're getting video quality that you're moving inside. So if you lean to your left or to your right, your perspective changes inside that video and it looks like row video.
16:34
I mean it looks like a row high quality movie and the VR headset allows you to do actually walk around a little bit within you know when I tried it that at CES I was on the it was on a headset called device free which is before the HTTV was released, was like the like the beta version.
16:57
It's like they had the, the competitor competing product was the Oculus developer kit to August developer kit, to pre-seeded, the Oculus consumer version, one, the Oculus Rift, consumer version one. And so in the five pre preceded, the ACC five which was a the product app product that was released after the VIP was released.
17:25
But in the five free at CES and 2016, I tried height VR and it blew me away. I mean it was it was better than so brush in terms of blowing. My mind like I tried top rush at San Francisco virtual reality events in May of 2015, almost a whole year for CES in 2016 which is in January.
17:53
So there's like a difference of like, you know, it was made of 2015, a January 2016. I was there was a difference in time there. So till brush was like, one of my first VR experiences. I may have been my first VR experience in in a was also in a in an early vibe headset.
18:13
I was before the vibe free HT5. It would divide HT5 had just not the, the predecessor to the HC5. Pretty had just emerged and at the time it was just the I don't remember what it was called, but this was not the consumer version. This was a only few people got to try these.
18:37
This was like super early and agency HT5. We are so great because you had position tracking that you didn't have not just for your head and your body that you could you can walk around a little square space about 12 by 12 or it could be even bigger 15 by 15.
18:57
But you know, most the time in San Francisco spaces are pretty small. So I think we, I think, you know, there's maybe it's less than and that 12 by taller space where I tried it. But, but the point is that toothbrush was like I was painting levite in space in the air.
19:19
It's extremely precise tracking. So amazing and then hype. Here was this movie in virtual reality and I really like, I just like, I'm here's the thing I hear, you know? I like, if you can create a robot where you're basically causing the robot to reconstruct a three-dimensional representation of the world around it inside the robot's head, here's thing is is and then the robot has to learn to navigate this 3D space.
19:57
So it's getting this 3D data and it wants to learn how to wrap it, navigate this space. Now this is just as a concept it's significant interesting because you start to think wow this is a kind of internal representation. You know, I it's at this point, it's it's like well I don't know how there's I don't know how like, like at this point.
20:22
If this is like, if you just try it VR, it's like well, what are you gonna say? What how are you going to describe? Basically, what how does this? How do these, how these if, if you are, if you do images being rendered inside your brain, how do these how does images conscious?
20:42
How are these images aware, how these images, you know? How does the, how do you go from putting 3D images inside a robot? Because you can put like this. Here we have here. We have a inside a robot. But how do you go from that robot? Like, I mean if you think like I don't know, any VR experiences is 3D experience, so you running a 3D program inside so that's only part of it, that's something part of it.
21:14
And I have to, and you even to, even to establish, that's happening in human brain, or in a dog's brain. And and how that's happening that by itself, is is a significant challenge. But what I can say is that, I have I've seen that if you have a 3D known network and and I'm not, and the guy who showed me this, I can't tell you what his name is because he doesn't want to, he doesn't want.
21:47
He's, it's super super paranoid. As I can't tell you what his name is, I can't draw any connections to him. He he's, it has his work and his own way of going about creating self-aware networks. It is something that I I'm gonna tell you all. I'm not gonna like, I don't even know everything that he's working on anymore.
22:17
But he's done some amazing. And I've considered mostly, I think he considers it mostly aren't work, but what I saw was that if you have a 3D network of boards and a 3D game engine that this reading network can represent, it can like, well here it look, the thing that he showed me was that if you plug in a web camera history, you know, network would change its physical structure enough to represent what was on that webcam.
22:57
So it's a it changes this three structure to to sort of match the incoming signals. So it it creates a 3D representation of of the 2D image just coming in from the webcam and with multiple depth cameras. It's able to it's it's able to construct 3D images. Now now if you look at, you know, more recently, we've seen basically the the light are fusion that is, you know, used being used by var ho they develop their own variation of light.
23:45
Our fusion, which is which combines depth depth you combines the, the short range depth theta of lighter, which is, which is, I see, it can be used as a sort of time of flight since or in order to get depth within about five meters. I'm really good depth information.
24:06
Very precise depth information and it combines that with the video of feed so that you get really precise details because light are not super detailed. But if you combine them both together, you can get really great detailed debt. Highly detailed depth information. If you combine the video, which is highly detailed and the light art, which is highly specific, highly specific with depth information.
24:33
And so, light are fusion data is way that algorithms. It's it's a deep learning algorithm. It's a neural network that can basically capture the real world from cameras and modify the positions of the incoming data to sort of reconstruct, a model. And and that is sort of the point of what I'm saying is that a 3D neural network is capable of representing any three objects?
25:19
Right? And it doesn't have to be like it can be a sparse and distributed representation. Right. But here's the thing is the thing about your field video again, okay? And think about how your brain doesn't have to render the back of your head when you're looking straight ahead. Okay?
25:43
It doesn't have to render your eyeballs when you're looking straight ahead unless you're looking into a mirror, right? But if you're just, if you're just looking out onto the street or or at a wall, this your brain doesn't have to render your face to you. If you there's no reflection of your face anywhere, it doesn't have to render that, right?
26:09
So just think about that. So think about like in your brain you have a 3D nail network and I'll go more into how your your brain is a 3D neural network and there's lots of lots of neurotransmitters and and, and, and ions were charged atoms. That are are changing places really fast and there's lots of lots of movement happening with it with the changes of, in positions of neurotransmitters and, and ions.
26:52
And it's so much movement. That the, that's a physics of how all this happens. Is really, as far as I know, it's still a bit of a challenge to to to accept. I don't think there's an equivalent yet in terms of, you know, human manufacturing that can match what is, is the speed of.
27:24
Because the idea is that, you know, if you see if, if these if these if these atoms are moving so fast in the brain, you know, wire why aren't they destroying the walls of and the structures because it's just it's just you know. There's there's just it's it's just a kind of a machine that we've human beings have never built in.
27:53
It seems like it would be very hard to build something that, you know, in fact, I mean, you know, there are now neuromorphic ships, where, you know, ions are basically being transferred from from one place to another. And in these were built, sort of with maybe the idea of replicating brain function, but also getting to a place where we can create artificial neurons that can actually interact with real nerves.
28:23
That can actually, you know, except memories and transfer memories or recognize patterns and contribute to brain function. The way I real neuron, does basically to send and receive communication and into, to be sort of state machines for the communication of neurons. And by state machines, I'm talking about, you know, changes in the and what in in the firing patterns is a result of and incoming signals that are
29:11
That are, you know, both small and large. But also affecting these signals are affecting the. The, what's it called? What they're affecting. What's what's not going to fire? Or they're affecting? Basically the inhibitory system. So the inhibitory system and I like the book that really explains this really well, is rhythms of the brain 2006, a 2006 book, rhythms of the brain.
29:43
The inhibitory system is sort of allowing really complex carvings of pattern carvings to happen. So so so you could have tuned there on there, too neural networks. Like the typical convolutional network is not really. It doesn't have an inhibitor system, it doesn't have a, it doesn't typical one doesn't have a state machine.
30:11
It's not it's not considering the information and multiple times steps. There is a new neural network that is designed to sort of like have like a multi-time step operation, that that is meant to be some sort of artificial consideration to to, to try to get a better picture of what's happening with less data.
30:42
And that's interesting. So we've seen the graph nodes, we've seen their perceptron from deep mind and we've seen, I mean, the perceptron no, not the perception is, there's a new receiver, the perceiver and then the there's the consider. So there's a like, time steps. But what the reason that multiple time steps are power are are powerful as you can.
31:15
You can take the same number of parameters and massively multiply the number of possible. Patterns by having an inhibitory system, that allows like the lowest level features of a neural network to be knocked out ever. And and to make way for other other patterns to emerge instead. And so you create the opportunity for for lots of novel patterns, lots more novel patterns to occur by having a sort of
32:06
System of, I kind of think about it is, you know, I think of, yeah, you can think of like neurons is like sequences of a piano playing out. But the piano is, instead of, you know, a piano it's just like is if it's a piano is too dimensional in some sense.
32:38
And so I met the keys to the keys are going from from the piano players, respect the keys are going from from left to right. You could have pianos with multiple decks, okay? And then, you know. So so and ends and that sense. So like the so maybe the first dimension of the piano is as just
33:09
I'm what I'm trying to say. Is that is that? While the piano certainly has only like there's no inhibitory process for a piano. So, there's definitely a constraint on the number of different musical patterns, I guess that might emerge. There's some sort of constraint. I mean, I mean, I don't it's there's a note.
33:36
There's a maximum number of possible patterns, right? And that's a huge number because there's a lot of different keys but it's a bit but but a piano is I guess a finite state machine. Is not an infinite. It's not there's not infinite possible states to piano, eventually the the number of possible combinations of keys that can be played with piano player that has 10 fingers, you know, and a number of possible combinations that can be played at each interval of the music, right?
34:16
It is. Is this one thing? Okay now, but let's go back to the observer. So, so we need to like it's not enough to just have a piano playing in your head. There has to be something that's observing it, but in fact there is and and this the thing that's observing, the piano music that I'm proposing is being played in your audio cortex or the image.
34:45
That I'm proposing is being rendered in your visual cortex or or the feelings. I am imagining on the being rendered in a and it was hard to describe way but but in in your somehow sensory cortex, so the whole you know, the primate three primary sensory quarter of course areas idea is that is it?
35:17
These these patterns are being generated there but they're being generated there because because you're observing them on the outside, okay? That your observing amounts that because your cells are listening to the incoming patterns and then and and so yourselves are rendering these patterns and then you're listening to them to when they come in to your brain.
35:51
But but what's interesting is that, is it not only are you rendering these, the you're rendering these patterns for other neurons and other neurons are listening because they need to know when they're going to fire need to predict when they're going to fire. Because if because because that affects the neurons equilibrium and idea is that the to sort of like the neuron is, it's because of tensagery.
36:29
They're not is is very in tune with its environments. Very in tune with what's causing changes because those changes those changes from its environment are inhibiting parts of the non. And it's a very, it's a very sensitive instrument than they're on. It's like, it's just like, look at your hand.
36:52
Your hand is, I think of your hand is like, the as like the fractal of a neuron and your fingers would be like the dendrite, like the branches and the core of your hand, the center of your hand is like the soma. Okay. So your hand, you hand. It is collecting a lot of sensory data now, touch your hand and in the center in the palm.
37:10
Because that's like this so much. So just definitely like signal collection happening there and it then, right, there's signal collection happening it's it's a fractal. And in that, whatever you touch is creating signals on this under the surface of your skin, your touch is creating signals and where your hand is being touched, okay?
37:37
If you close your eyes and someone touches your hand, you can guess what you can guess, where they, they are. Touching your hand, some people can visualize. I can visualize where someone touching my hand. If my eyes are closed in a sofa bug cross off your hand, you might you might be startled and there's a system in your body.
38:00
The autonomic nervous system that's the sort of like unconsciously listening to every event like that. So if like, if so bug does land on your hand, suddenly you become aware of your hand because you have this data pattern but cause across your hand you might you might react really fast.
38:20
You might jerk your hand back. That's the at the autonomic nervous system that sort of like out of your conscious awareness. It's doing all these things. It's listening, sort of all the time to a new incoming sensory data. When your hand is a good analogy for dendrite. In fact, it's the it's a fractal of dentry.
38:36
I'm just gonna I'm just gonna argue. It is a fact that your hand is a factor of attention. I just I've thought about that. So many times, I just can't unsee that and so like, imagine they every single neurons dendrite, the essentially, like a little hand. We say it's like a tree but it's, you know.
38:55
Okay, then your hand, your hand is a fractal, like a tree, also, right? Think about the, the roots of a tree, right? And so, each, each part of each of your fingers is, when, when you touch each part of each of your fingers and go ahead and do that.
39:20
If you want the, that each part of that is, is each touch. It's gonna be sending a pattern to your hands and down and and then it's gonna be sending that pattern a long along. The the nerves in your hand all the way up your arm into your brain.
39:44
Eventually, it's going to go right up to your eventually, it gets to the somatosensory cortex. And then and then and and you know, that's at the high, that's at the highest level. And, you know, there's a whole like, you know, how information travels as a whole chapter about how information travels in the brain.
40:10
And I started like, I went into it, with F Scott about, you know, going going from like the eyes. But then, you know, there is this, there's this whole area about how sensory information comes in with with the audio cortex and I there's a lot of details that I don't know because I don't I there's 200 years of neuroscience research out there, right?
40:49
Because, you know, since since they since the late 1800s that, you know, the first, when, when 200 plus years, there's, you know, it's it's older, but it's kind of like, you know, I feel like when we start waste, we start ahead Ramon Kajal and Kajal were we're, you know, basically creating the first, you know, ink drawing's of neuro anatomy of how lots of neurons were connected together in the brain.
41:28
Okay? That we kind of.
41:36
You know, I think that that really was a shifting point, a big shifting point, there's a lot of historical, you know, points and history of neuroscience that, you know, we got to cover from, you know, from from, you know, the first brain computer enters interfaces, like, you know, eat electroencephalogram and, and, and, you know, with hands burger.
42:05
And and and and so we've got to talk about, you know, sort of like the big points in history of of neuroscience.
42:18
You know that I'm, I was sort of want to skip past the story of Phineas Gage because it's sort of, like, sort of like high level macroscopic. You know, neuroscience primitive high level connect to my ideas about brain function. I mean there that sort of like, review this sort of like, you know, allows us to think about the ventral stadium and and the they make the law.
42:56
And and the, the orbital lobes is a, as a as being a sort of feedback cycle that we're or things are, we're lower, impulses, and desires, and fears, and then your, our balanced out by considerations and thoughts in the overall frontal lobes. And if you destroy the orbital frontal lobes, you may not give a fuck anymore.
43:24
About anything related to, you know, strange related to any of your desires and you may not have consideration anymore for other people or for yourself or for life related to any of your fears. So it's like having no more conscience. It's like it is an analogy, is that there?
43:51
The idea is that you, is that there is these these, these different brain components are sort of, in a functional tank, set integrity. So so 10, segurity, it 10 security. Let's Google can't Google. Doesn't know. 10 secretary, but 10 segregated and whatever. So the idea is that I really can't look at the transcript because so let's because it just stops me.
44:33
I'm like, man. This these words I'm not being spelled correctly. The idea is that, you know, of one brain area, keeping another area in check sort of functional tank. Segregated between different brain areas. But what's what is interesting? Also, you know, and in that effects, many different treatments so you have someone who has OCD and one of the treatments is, is to become aware of OCD, to understand the symptoms, understand what it's happening, to identify be able to think about it.
45:07
So that the parts of your brain that are thinking about in the prefrontal cortex are able to sort of regulate and remapt the OCD. And so you're creating a sort of a funk sort of like it. Like a 10 segregated network is like, when, you know, so communications is network, is when thinking invented in 60s or 70s.
45:29
And it's this idea that when part of your phone network is destroyer taken offline by, it could be during a war or something. The communications network automatically, sort of remaps it's pathways around that broken node. So that the rest of the nodes can continue to communicate and this is this is considered a neuron like our brain like thing.
45:58
This 10 segregity feature where where if if part of your brain is damaged, it could be from brain cancer or something. You could have some very dysfunctional behaviors but it's also possible that with therapy that other parts of your brain could adapt and grow and learn to become the functional balance.
46:24
Again, so you so. So the idea here is that, you know, someone like a Phineas gage could have a pole shoved through his opposed, he could stop giving a fuck about other people. He may know that, you know, what morality is in an intellectual way, but no longer, you know, cares about hurting people, and no longer cares about, you know, doing bad, things and others are terms.
46:51
You just, he stops having a conscience, right? It becomes capable of evil that he was that was that he was not capable of before his. Is it before the injury to his frontal lobe to his oxypus? Orbital loaves, excuse me. And but the, but the principle of functional tensory is that that I think is that I you're going to have some other brain area that, you know, it with the right therapy, some other brain area could grow to feel it to fill in and adapt to the damage part.
47:37
And so this is, you know, I guess it's a better argument for why neuroplasticity works. Then then, then the argument that your brain creates, then, that neurogenesis is something that happens. Because even though, neurogenesis does happen, it's not, it's not particularly, there's no explanation as to why neurogenesis is going to be relevant to neuroplasticity.
48:04
It's just sort of like a leap in logic that that I don't think is is justified and or nor is it necessary? You can say that the neuroplasticity or your brain sort of like, learning something new at an older age is is true. It's true. Is, you know, with the right therapy, you can have improvements in your functionality, you know?
48:35
They say, if you, if you have, if you have one of the things in the neuroplasticity book was, if you're, if you're, if you write hand, you know, some people sometimes, some people like they had a stroke or something, and they lost their functionality in their right hand or something, or they're right arm.
48:57
And so one of the therapies was, okay, we're gonna tie down your good arm, so your good arm, can't move. So you have to use your right arm that can barely move. And in the idea of being that, if you, if you don't do this, if you start to just use your good arm, then then then you're then you're your body, just gets into a situation where it never sort of struggles to repair the bad arm.
49:31
And so the bedroom stays bad, but if you bind the good arm down. So you have to try every day to use the the the band arm to accomplish your goals. Then then things start to change, then the improvement in the function of your of your bad arm of your damaged arm actually improves.
50:00
It's like there is there's enough healthy tissue nerve tissue, that repairs can start and repairs can continue. And, and this gets us to a situation where
50:21
You know, it is possible. We say okay well it's possible but the there's there, it's not clear. Why? It's not clear to me at least why not? Neurogenesis is even relevant, because you, because you'd have to argue that neurogenesis would was somehow essential for new learning and it's not, it's not, it's, it doesn't happen.
50:49
It doesn't happen that often. And we can say that learning that the mechanisms of learning are or not completely, they're not totally dependent on neurogenesis. They're actually dependent on on primarily on completely other other factors and then that's my argument. So, I'm gonna say that, you know, that, that, you know, if you're using, if you're using something then, you know, if anyway, I'm going to my funeral
51:43
Okay, so it looks like I was talking about Phineas gay, prefrontal cortex and then tang integrity of function.
51:55
And but I wanted to get not really spend a lot of time on opinion stage but because there's just a whole lot more
Transcribed by Pixel

Audio
