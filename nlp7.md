the neural lace podcast 7

The Neural Lace Podcast #7 Guest Android Jones (The Vision Agency)
https://soundcloud.com/user-899513447/the-neural-lace-podcast-7-guest-android-jones

see note a0249z.md or this article https://medium.com/silicon-valley-global-news/gti-2017-gpu-technology-conference-the-neural-lace-podcast-5-with-guest-jules-urbach-the-ceo-of-17c4067a648e

Audio Transcription by OpenAI's Whisper

I'm Andrew Jones. I'm from Colorado. I work for a company called Vision Agency

and we produce an experience that we're calling microdose VR. And today here here

at AWE and you're playing with the Muse. What was the idea behind that?

I met Chris, the inventor of the Muse at the the MAPS conference last month.

He came out to a workshop that I gave and we kind of hit it off and started

talking about the different visions that we had for BCI and VR and what the

possibilities were and then he showed me a photo of this new prototype that had

seven sensors built into the foam padding of the Vive and it just happened to

be that we were really for the past six or seven months we've been going to the

different conferences trying to find what would be kind of like the you know

the Cinderella slipper of like graceful way of starting to integrate EEG feedback

and biofeedback into microdose so we could procedurally generate different

visual parameters and it just worked out really gracefully that we had a call a

couple weeks ago and he says hey there's this thing called AWE and we're like

that's in two weeks that's totally unreasonable how could we ever make a

build and incorporated that we'll do it you know and so we're here wires all

these cords make it happen amazing you have this like awesome hacker workstation

over here with like that's that that's that's totally hackening right now

welcome to the Neuralace podcast number seven guest Android Jones co-hosted by

Michael Bloomberg and Pfeiffer Garbissi this audio was recorded at the 8th augmented

world expo AWE 2017 the largest AR VR event in the world so Android Jones was

at AWE 2017 to showcase the latest product from the vision agency that

integrates microdose VR with the new EEG headband from Muse that has been

integrated into the headband of the HSC5 the Muse headset is one of many

examples of how biometric sensors are going to be integrated into basically

all AR and VR AR and VR glasses will have built-in EEG into the headsets

themselves in addition to eye tracking and cuba dilation tracking heart

monitoring all sorts of biometric sensors in this podcast we also talk

about Nvidia's new GPU cloud or consumer level for the consumer version of

Neuralace will need a ton of computing power for each and every person the

reason is that even after we have created a language for computers to talk to the

human brain introducing visual auditory and other sensory concepts that are not

actually there we can't send those patterns directly into the brain we have

to first listen to the patterns that are there at present from the environment

that each person is in so that we can send only the difference between the

pattern that is there and someone's brain in the pattern that we want to be

there so they're basically three ways to use EEG with AR and VR first way is to

use EEG with artificial intelligence so you can predict someone's emotions and

intentions at the interface level the second way is to use EEG to gamify the

virtual or augmented reality environment third way to use EEG with AR and VR is to just let the EEG drive the raw changes in the light sound and tactile effects so that you're providing a mirror for someone's brain wave signals so they begin to form concepts of how their thoughts and emotions are brain wave signals how their thoughts and emotions are changing the light and sound effect so

Neuralace research in general regards the human brain as a special kind of hard drive and in

this episode we talk about the the NVIDIA GPU cloud and how the future of AR, VR

devices integrate biometric sensors and artificial intelligence but if you're

still skeptical about how advanced neuroscience research is getting I have

three great examples if you can find on my medium page about this podcast and

those examples you can also find in my Facebook group called self-aware

networks you can find these links at VRMA.io and VRMA.org so three great

examples of how advanced neuroscience research is getting is one is that we're

rapidly gaining new understanding of how the community brain networks function at

large scale and an example is in Nature magazine Nature.com human brain

network it's an article it's called human brain networks function and connect

them specific harmonic waves check that out. The second example of how we're

rapidly gaining new understanding how our brains are assembling information is in

a cell.com article called the code for facial identity in the primate brain the

third and final example of how advanced neuroscience research is getting and how

we are rapidly understanding new ways to stimulate our brains with wireless

targeted transcranial magnetic stimulation for example that will

eventually allow for next-generation computer to brain interfaces is another

article in cell magazine cell.com called non-invasive deep brain stimulation

via temporally interfering electric field so I hope you check that out. Now the

podcast begins first you're going to hear Android and he's responding to a

question you need some type of outer reflection like you can't be like oh

it's never like oh man I feel super alpha right now or oh man my EEGs are this

like we're totally disconnected with those so that the information that your

body and your mind is producing that you're not that you don't have like a

visual connection with we're using environmental elements to create that

visual connection and it's something that it may not feel apparent but we're

hoping that over time you'd be able to kind of recognize you know if you put

someone that had a really clear piece of state of mind you put him in here they

would get an environment that would be drastically different than someone that

was totally like distracted or you know so you'll be able to procedurally

reflect states of minds from anybody just getting into microdose and then this

could be used as a tool to for somebody who is having a difficult time being

calm and centered to achieve that state as well I mean I I definitely think it

was I mean I started using Muse a few years ago and you know I'm a good case

study for this because I definitely value the benefits I totally understand

what meditation can do that like stillness of the mind but I have a really

difficult time being bored and feeling you know it's very hard for me to like

settle my mind like I've my path of meditation is through a creative

flow spate where I just I get to a place of kind of the devotion towards the art

and kind of losing myself where thoughts change like the frequency of my

thoughts change and kind of go away so I think I get my I'm able to kind of

scratch that meditation aspect like through my art but when I sit down and

meditate it's very challenging but when I started using the Muse a couple years

ago like it gives you that that gamification of the feedback of like

oh cool now all of the birds are coming okay cool there there's the birds and

then it'll it gives you a little record objective record of like oh you did it

this much longer than last time and like little goals like that's the only way I

could and I was doing really well with it till I like broke it but I probably

still been using it if I didn't break it I'm hard on technology now we got some

more but yeah as I think it's the pulse the poll right now pulses definitely are

giving us the most reliable most kind of one-for-one like that's me like there's

something just kind of when you see it when you're kind of watching your pulse

and they never seen your pulse you kind of you just get it you know like I've

done this we've been kind of coding and bashing this together for three days and

I can see like right now my heart rate I was just in it I could see that my heart

rate is way faster than it was like last night when we were testing it out

because I've got all these people and I probably had some coffee and then all

these things so that's kind of a neat I mean that opens up a whole cool thing of

just being able to take your take your just take your data like being able to

just see the different aspects you know maybe we could like map your cholesterol

level someday so so so EEG like your diagnostics entertainment that's our

niche so with EEG you can like you have different points on the brain and you

get a number from each of those points so it could be just like a pulse where

you can assign a different graphical change to each number and that's another

way to you know change the graphics with what the numbers you're getting from

question for you what would you if you could if you had a sensor that was that

have a similar resolution of the muse what would you want to and you could

have any of these things change and manipulate and modulate the environment

around you what would you want to attach say like some of your EEGs to like what

would feel like an appropriate environmental response to the conscious

manipulation of your own EEG waves so I so I did do a project similar to this

in 2012 and I attached I use EEG to drive different changes in light and

sound effects so I attached like different sound effects like reverb and

pitch and just you know different bass effects anything you can use to modify

the sound it also the light so the brightness the the the colors all those

so your brain waves would drive changes in the light and sound effects and I

also incorporated I encoded the sound with binaural beats it worked really

good because we'll see binaural beats are really the really the brain is really

interested in binaural beats so it causes you to focus more so if you really

want to focus with meditation if you're listening to binaural beats you they

can put you in a deep meditative transfer easily but the idea was to let the to

encode to encode the sounds with the binaural beats but to let the signals

from the EEG cap the numbers drive the visuals the colors changes and stuff and

also to drive the the sound effects so say of a be played like with alpha like

getting into an alpha state like what do you think alpha would like right now

we're using alpha I think we do a combination between we're actually

experimenting whether we can make the clouds part and dissipate we can make

it like overcast or clear dynamically and we're kind of playing between using

like the clear function or the alpha to make that happen EEG we've had a hard

time really narrowing down what makes sense with EEG because so far it seems

a little erratic and unpredictable you know like I can't I don't know I can't

I don't know enough about it to hold a state of mind and go oh my even to say

like okay low EEG what makes the low EEG or it makes the high EEG it seems like

it's all over the place you know it's like I move my eyes back and forth I

know that we're trying to find like how can what can I do physically to hack and

manipulate my brain waves more consciously so that that's that's

completely true in fact so what the companies will say okay I want you to

levitate this ball and what they do is is they say okay so if the marker it goes

if the EEG sensor goes above 106 then the ball levitates a little bit higher

or if the thing is it's like it accumulates the data so it's not like

you're holding that we thought about that too that maybe so there's something

and every time you you maintain a state for a certain amount of time something

happens so the Sun gradually rises not you don't just rise it in one fell like

mind thought or mind sweep but you have to hold a state of consciousness for a

certain amount of time and it gradually brings it up so the problem with that is

so the first time you learn something you have a ton of brain wave activity the

first time you learn a car your brain is a spiking incredibly the second time

you try it your brain wave levels are cut in half the third time they're cut in

half again it's like there's no way to build a consistent sort of tool to

monitor your brain in that sense because your brain because your brain will

become so it has a program to like reduce the amount of power that it takes

to perform a certain task so you're never gonna hit the same levels and on top

of that everything in the neocortex I think the neocortex is really the I'm

talking about you know on top of your head the need going here so that's the

so the forehead is still you know the part of your you'd be measuring the

neocortex as well but what the from my hypothesis is everything's still really

scattered so you're not in terms of all the you know your senses come into your

thalamus that's the integrate there but when they go into your neocortex they go

all over the place everything is scattered and messy and it's it's it's

gonna be hard not only to get the same signal twice but to get the same level

of signal twice then here's a good question for you because you've tried

microdose several times and you've seen like a lot of its evolution to most I'd

say 90% of the people that try microdose for the first time whether it's not so

much the events like this but we were at lib and Coachella and Burning Man we're

gonna be like symbiosis week 90% of first timers like never tried VR just you

know pop and share pop and digital cherries left and right what would you

imagine that someone who's what would the brain waves what would be like the

the common denominators of what kind of brain waves we could expect from people

their first time in VR with microdose like what you think what are the patterns

you think we would see so we can then gauge to anticipate or work around those

specifically well whenever someone's learning something new you're gonna see

their brains working a lot harder so that's when you'll get the most well

defined brain waves but the but that's you know the one-time experience and the

next time of course yeah yeah and do they and I guess you need to show that

you'd want some type of calibration to like see what their mind looked like

before that too right so maybe like you could do a little before they put the

headset on it could be like hey this is what your mind is like right now this is

your brain this is your brain on microdose that's a good one glad you're

recording this yeah so that would be something you could like show a

difference right the headset on they would it would go into microdose levels

anyway well the real difference would be like in subsequent times using

microdose you'd see that their overall brain wave activity would decrease as

their skill improved with the program oh interesting so we could reward the

decrease like new levels because we're kind of thinking of it to be a really

cool way to progress through the game where like in like that that and that

original Bart Bartow level we had that the different portals would only open up

if you got to a certain brain state you know like that would be a neat way to

progress and kind of reward people so you're like no points or anything and

no badges but like you get to a brain waves like oh great you've achieved this

now you have access to these particles in this new realm and it could kind of

spawn out from there that's actually a really interesting idea that never

occurred to me is to continually modify your program to reward decreasing

levels of brain activity as some sort of indicator of your brain becoming more

efficient at doing what it's doing it seems like it's also like a natural way

to you know in like a lot of games you get in and one of the first questions is

like do you want like easy medium or hard like it's a great way to automate

that where to keep the person challenged like you give them more stimulus the

lower their brain waves get to kind of keep the brain up at a high stimulated

level like we could have like a stimulation kind of a quota that we

want to reach for people so they get more particles or more worlds if their

brain is that work or is that well the paradox is that if you so if you focus

in these e.g. games you can push your brain waves up higher but if you're

really like you know focus on learning something then your brain should be at

the same time making that more efficient so like like people who are really

skilled to driving will not are not going to show as much brain wave activity

as someone who's brand new to driving so so if you're really like focused on

mastering your e.g. then it should actually it should become harder for

you and not easier for you a paradoxically if so the thing that the

brain reacts most violently to for any e.g. is light right it's why you can

induce seizures and so if this inherently has a ton of light so couldn't

you stimulate the brain using those like different patterns of light well you'd

need I mean so you'd need that's up there's a there's a study there's a

field of science called optogenetics but in order to stimulate the brain with

night with light you actually have to alter the the genes so that you have

light receptive genes because normally our genes are not receptive to light I

mean well I mean we do have you know obviously the ganglia neurons are you

know impacted by the light if you're if you're if you have epilepsy the

flashlights at you to try to get you to see if you have a seizure well what

they're doing is they're they're flashing lights at it because they can

cause neural activity in your ganglia neurons by bouncing photons off of your

retina but that's not the same as like internally like inside your brain like

the lights won't do anything unless you have unless your cells are sensitive to

light and they're not by default so but there are gene therapies you can use to

make your internal cells I don't mean like light that's like you know if

you're in the experience he was talking about making you more oh you just mean

like in VR like if you wanted to change someone's brain activity by using a VR

experience you could use light in that experience you can't light yeah yeah

you could that's exactly what they're doing I mean I mean that's why like I

think it's hard to say like what's like what's the chicken and the egg like if

you have a VR experience it's reacting to like EEG patterns and then making light

out of those patterns like you're probably having those patterns also

somewhat because of the light that you're seeing yeah so what we're also

playing around with is actually using the erraticness of the data to

procedurally generate geometry so just kind of imagine like you're just seeing

what the data looks like in like a three-dimensional form and you know I think

having access to that it's the kind of thing where it's it's it might be lend

itself more to like kind of like a skill tree like you kind of learn it's not

something like at first you're just like oh wow this is that but if you spent time

with it it's something that by actually seeing your brain waves being generated

in real time then you could start to like more you know we're kind of we're

just looking for opportunity to make this more of like Jedi training then you

can kind of find ways of like okay if I think about this how does that change

this wave or if I if I'm really just is that possible so the what I would do with

EEG is I would use it as a mirror to understand my own brain waves and this

goes back to what I was doing in 2012 your EEG can drive the graphical changes

and that's like that's what I would do is drive the graphical changes not try to

control it and you know if you're like meditating just this allow it to like be

an experience of you not try to control it just allow it to be a reflection of

you and through that visualization and the audio as well as assuming you're

the brain was reflecting the audio you begin to have you begin to create

concepts of how your brain waves are causing the changes of light and sound

and you begin to create concepts of your brain waves themselves and that has

enormous therapeutic value just by itself without don't try to control it

just let it be a mirror of you so then the next question is this based off like

where the EEGs are on the spectrum you know we basically kind of we corral

everything into between like one zero to one you know is where we use that as

like a live particle parameter then it's like where what's the appropriate

mapping between like high and low on EEG from between like light and sound like

is it something is it like a reward base where like one is really low saturated

and one is do we make one less favorable and one favorable or like one blue and

one orange so I would not I wouldn't I wouldn't so sir your question was EEG

patterns I profile them yeah so I would not do that at all like I would just like

take the entire sensor data you know from zero to one sorry from from sorry

that's sorry the particle data is zero to one but take the sensor data from zero

to 160 or whatever it is and just just map like you know particle effects

throughout the in that entire sensor data that's what I would do so that you're

not that there's not it's not it's not necessarily about different effects

occurring at different thresholds and we're trying to like get that effect to

happen you just want I want to see the whole spectrum because I know that my

brainwave activity will be inconsistent between one session and the next like it

will be it'll be less expressive as I become more skilled so that means that

I would want to be able to see effects on a gradient curve so if you have I

don't know what the music there's like four sensors or whatever if you have what

eight-channel EEG it's an 8-channel EEG so if you have eight channels and they're

they're all like let's say it's it's a gradient basically all your effects all

your particle effects are on a gradient and you just match the gradient to the

sensors so you have a sensor gradient and you have a channel gradient from your

EEG and just match them so that no matter so that you can still experience all

kinds of cool effects no matter what outputs come in from your brainwaves and

because we're kind of I mean there's argument that we're digital but we're

kind of like analog in a sense that I want to see that sort of analog with

what I mean is gradient basically but you know sort of like because my

electromagnetism has a topology and if you can you know not chop it up into

pieces but instead sort of like highlight the spectrum of it as it's

flowing like like like a fluid dynamic and electromagnetic space that would be

very interesting yeah that's good you know I think maybe I've kind of approached

this because of my experience of the Muse of the like the thunderstorms or

birds chirping that I kind of had a binary approach to the sort of like reward

system of like negative positive but yeah if you make the whole spectrum just

different you know that could be an interesting way of doing it too we were

kind of playing originally to where we were making going from like instead of

cloudy to clear sky to so that was actually the key inside of today you know

like do things like hey the moon's cool hey the Sun and Sunbeams are cool too and

just kind of shows you you're going here there like warm and cool maybe like more

like elemental changes like water fire like different different polarities that

are both favorable you know and believe it or not that was really my goal that I

wanted to communicate to you was to not do it so my whole theory in 2012 when I

was doing my company was to to get away from like you know EEG for games is bad

because we're trying to like use it as a control your brain waves you don't fit

until like it's not a button you're not pressing a button when you're sending

your brain waves you're sending a there's a gradient right and so that's my

whole theory was yeah we should like have the use the machines in a way that

are reflecting that gradient back to us and so that's really like I was like yeah

don't don't get away from the gaming concept of binary no totally the game

or is the is the gradient like these different kind of wave states so instead

of trying to hit like a point like lots of activity you you're trying to reflect

the fact that there are many different kinds of states so it's not actually a

point but it's the state of the wave like alpha theta waves yeah so if you look

there's lots of great charts for great brain-wide visualizations where they're

showing like waves in a in a chart like they're showing this great four

dimensional visualizations of and that would be good to just like if you could

put that into a VR experience if you could just or take the graphics from

microdose and make them you know just very reflective of all the nuanced four

dimensional changes in your brain waves that a that an EEG device like the news

can track and generate and sort of integrate that high to level high level

dimensionality and if your brain is in any in any respects like a neural

network it's going to thrive on that data and you'll find all these sorts of

amazing patterns and that's what I found out in my research in 2012 so that's I

think that would be great if you guys do that as we just keep coming up with

these like new things that are that make it so much more effective than older

versions that we just want to get these new things refined but then we find like

another new thing so before you were doing you know VR with EEG which is

this like frontier technology and stuff you you're doing two-dimensional art

with adobe products for example so how do you like how do you think of the VR as

an art how do you think of EEG as art and how are you sort of like you know

from your perspective where you going with it well sure yeah I think the one

really easy way to correlate it is that the two-dimensional art that I've been

making for the past 20 years that's really it's the art that people see the

two-dimensional that the two-dimensional like kind of transmission that's just

the the it's just like it's the residue that happens after the experience of

making the art the experience of making the art is what it's all about that's

that's the core I in order for me to afford as much experience as I can

possibly take I realized that it was effective to find some type of business

transaction of transforming the residue of that experience into you know the

life-support token so I could just keep buying more of the experience for

myself you know it kind of like we're all kind of free-range slaves buying our

freedom back bit by bit that's part of the game but the experience that I have

making art is very three-dimensional and it's really like I said even beyond

whatever dimension or whatever form or the tool from using a waycom or a brush

or a pencil or you know a piece of chalk it's where the that experience of being

in touch with you know the creative dynamic conscious forces of the universe

is what's the most exciting thing and so EEGs are great because EEGs are a

measurable way of tracking the experience that you're having and you know I think

over the years my art has gotten I've been more and more attracted to tools

like Zbrush and Cinema 4D and Max and now the Unreal Engine is kind of

represents you know the current pinnacle of the my endless pursuit for the

nonstop creative orgasm because it allows so many different dimensions of

creative input you know from sound to the VR editor to sequencer to animating

to modeling to compositing to the particle cascade engine it's just this

full library of little input places where I can express you know the

unlimited potential of you know human creativity and so I think that's why VR

is like even when I'm making a piece if I'm making a piece in Adobe Photoshop

I'm using layers but when I'm able to kind of relax my mind and get in my

flow state like I can the computer monitor I've always just seen that not

as a flat representation but more of like a window into another world that I'm

creating even with the layers like I'm seeing the layers with like inches of

space in between them in my mind and they're just being collapsed like on the

screen like that's that's what's happening in kind of the inner world when

things are happening and so now I guess they don't have to it's much more

tangible I can actually put them on different layers and I can kind of share

I feel I feel like I'm able to share a much deeper aspect of my creative

experience that people now through the art than I ever was before in any kind

of two-dimensional incarnation so do you still have those files with all the

layers separating your art I mean could you some of them I'm kind of a you know

I my process in the past usually just based off the the lack of Moore's law

or my ability to do a Ford you know more hard drives like I kind of get to a

point where I'll have like a hundred or a hundred and fifty layers and my my the

one thing that kind of one thing I miss about being a traditional painter because

they still have a whole background of academic art and sculpting and all the

prima was the level of like that level of like committing to something like

something really powerful you know every time you say yes to one thing you're

collapsing all the fractal possibilities everything that wasn't that and that

commitment is meaningful there's something really meaningful about like I'm

gonna take this painting in this direction and there's no backing up you

know there's no net you kind of go for it and awesome things happen and you

learn things by failing that way and with digital art you did quickly realize

you didn't have that you had like undo and you just save as you could have

versions that you save all the time and I just saw from looking at my own

process of doing it every day that the more versions that I saved and the more

layers that I saved by unconsciously or subconsciously there was this process of

just non-committing to what I was doing and that non-committing was a really

it wasn't a very powerful way to embrace creativity or be in a flow state when

you're always because whenever you're saving you're basically picking yourself

out of the moment and imagining this future possible potential of yourself

going back to that save point so you're not surrendering to like what is and

all the possibilities so my process was usually like get to a point where the

meat was as soon as the layers start becoming a hindrance like I have to go

back and I don't know which layer I was on or I noticed like a performance or

that it crashes my computer or there's I'm suffering some type of performance

it because it's having to keep too many too too many things in place in the

layering software then I usually just collapse it and be like okay now I'm

committing I'll save it before I do that and then I okay boom collapse it and

then work from there it somewhat merges the world of no undos and infinite

undos into something that I found workable so I do have layered files

but you know if you look at a finished image there isn't a file that has

every single one of those layers all the way back to a white canvas to the

disappointment of many after-effects artists

I'm wondering what you're going to create when you have when you have the

ability to change reality itself through neural lace I'm looking forward to that

too very much so I've been wondering like I've been thinking about you know I

have this great plan for how you know this we're going to achieve neural lace

and there's a number of different directions people can go to you know

apply artificial intelligence to brain waves and to figure out what those

patterns are correlate them with the you know AI figuring out what the real

world patterns are and but then the question is like well how now when we

send stuff back into the brain there's we could you know potentially we could

have a sensor the CBI stuff and so instead of a BCI instead of brain

computer interface like a CBI so a computer brain interface right so it's

writing to the brain you're exciting about you're excited about that I am

but but you're gonna break the blood barrier no no planning to do this

wirelessly huh planning to do it wirelessly so you're a non blood

barrier breaker well the interesting thing is that much of the old brain is

below the blood-brain barrier like that out like you can reach the thalamus is

below the blood-brain barrier you're the guy was talking about like entering

the thalamus like through the nasal cavity that's right pretty intrusive

well I'm thinking we may not I think blood-brain barrier somewhere in there so

through I guess I want to say blood-brain barrier I mean just like

breaking the skin or some type of right the nasal cow the nasal cavity is a

very sketchy territory it does all these it's very lumpy it's not a smooth

channel it would be very easy I was talking to a nurse who stuck a had to

take a tube up the nasal canal and that was very sketchy and the sensors that

I'm currently looking to use to do this research are a little bit too big they're

the chances that they nick something and break something inside the nasal cavity

are pretty big so what I'm thinking instead is you know to I the naval

cavity is still important but I really strongly believe that wireless

technologies can determine the sensor that I'm planning to use has the ability

to determine where a brainwave is coming from so we can still kind of target the

thalamus but from outside and it won't go through it won't go through the top it

won't be on your forehand but I mean these are lots of these are lots of

research questions and engineering questions but I my hypothesis is that

we could the other thing is okay so look David Eagleman I talk about way too

much my podcast but he has he talks about these tongue strips and back strips

where you have a camera on someone's forehead and you attach the the camera

to the tongue strip so

so I was going to do my nearest research on a nvidia supercomputer and I asked

nvidia for one of their supercomputers and they didn't say no but they but they

they wanted me to produce a prototype first like with on a lower-end hardware

like nvidia like I have a 1080 but they just I went to the DTI conference and

they sent me an invitation to join their beta for their cloud GPU

that's right so it's basically like it's just like Amazon AWS except it's all

GPUs in the cloud instead yeah we're like invidiates we are our microdose team

you like bow down to the altar we have a green altar at our house and yeah well

that I mean they're like I don't know if you saw the keynote Jensen's keynote

they're building another universe Jensen said we need to build another universe

and I don't think his employees really heard him when he said that because I

asked them about it and they did and he really said we need to nvidia is building

another universe and that's they really mean like a universe on top of this

universe or or just you know like so that that that's segwayed into he was

talking about Isaac and Isaac is the simulation for robots to learn faster

robots to learn you know how to do stuff in the real world faster and so you

can you can massively speed up how long it takes a robot to for example kick a

hockey stick into use to flip a hockey stick to hit a ball into a goal that is

actually a lot of work for a robot to do and with by creating a virtual universe

for Isaac they're able to multiply that robot the robots experience times you

know a huge number so you have like 100,000 robots sorry so I asked that's

no it's an important point so they they asked I asked nvidia I said you know

what about yourself driving car can't you can you put that inside this Isaac

program and apparently none of them had thought about that and I asked multiple

levels the top people the middle people they and but they all thought it was a

great idea the thing is is that in order to create this for self-driving car

which is basically a robot you have to basically create another universe which

is what Jensen was talking about in the first place you have to create a planet

basically in order to accelerate the development of self-driving cars so

getting back to maybe I'd rather stay it up so getting back to so so for

neural-based research I think a lot about applying how to apply artificial

intelligence to to medical imaging to EEG but also to VR to end into AR so I

say because we have this nvidia cloud coming and we're able to render the

very best computer graphics light fields of photorealistic computer graphics in

the cloud on the GPU and stream it over networks that could be over a Wi-Fi or

over 5g to a pair of glasses that's going to enable VR experiences that have

that have a rendering quality that vastly exceeds the what you can do with a

single desktop GPU because you have a cloud of GPUs right so it just multiplies

the graphics and there's a pipeline that can just send you the basically the

pipeline it's just sending you what you need to see to your mobile device so

they figured it out so they can do this amazing render but only send what they

need to send you and so that way you can have a needed rendering that's part

that's one of the turns out foveated rendering was part of the initial

design but it's not actually necessary they figured the pipeline that will

and they foveated render will enable further optimizations but for what

they're talking about it's not even necessary so that the great news is that

web AR is going so basically it's the future of the web where you're just

walking down the street and you want to you want to access something that's

normally you look at your phone you look at your laptop or your desktop but now

it's just like the world itself is your web browser Wow yeah as long as the

bandwidth is strong enough it could totally be rendered somewhere else

because all it needs to do is just send to screens or you know to a phone right

that's not that's not a lot of pixels ultimately it's making those pixels

accurately in space and depth you know corresponding to like where you are and

how you're interacting with it that is most of like the processing power that's

I didn't know that actually but so yeah so so that is so what I think about for

Neurlase though is so there was a project called deep dream by Google where

they used of course you've seen it because it's they they had AI doing art

basically where you train they trained the neural network on images of snails

and dogs and cats and then they showed it clouds and what they said was every

time we run this through the network we want the network to draw the what it

thinks it's saying AI Paridolia yeah so my my my thinking is that you know we

can apply this sort of deep dream deep dream which I believe the program is

free and open source you could apply deep dream to EEG so the AI draws what it

thinks it's seeing in your EEG waves and that appears inside microdust VR yeah

I think was what's more interesting even than that though and this is where I you

know it was easy to kind of look at a dystopian future and imagine robots like

taking over a factory jobs it was always like oh but we're artists you know like

never gonna it's never gonna have you know the intro the perspective that we

have on life and art was gonna be like art was gonna be this one little place

where like robots couldn't take over your job but then once I started seeing the

deep dream stuff and just realizing like all the data is being collected like it

will be because we'd like to think that we're really complicated like our tastes

and like our our sensibilities are like incredibly these like sophisticated

things but we have a range between one and 60 or one and 160 as opposed to

between zero and one six you know yeah so but what it makes me realize is like

oh man dude the computer I mean the AI is gonna be not all it's gonna be one of

the reasons it's gonna be the the ultimate artist in the future is it's

like not what's gonna separate us I think it's gonna be interesting one of

things that's gonna make human art still novel and interesting is going to be

the subjectivity of it is that it doesn't totally make sense is that it

pulls like just strange like non sequiturs that don't have any correlation

together like that will be interesting it's like that surreal like how did you

why would you combine those two things like the absurd data will maybe be more

compelling because what it's not very we're probably not very far along from

like some type of like a deep learning neural net to basically be looking it

could just I mean if you just like had any I just hang out on reddit and just

being able to take the track the data especially once we get into the point of

if there is a you know say we have classes or whatever that are monitoring

like our pupil dilation once we can have some sort of like a measurable input of

like awe or like when something when a human thinks something is beautiful and

like how their retinas dilate or how their pulse changes I mean you could

you could you could you could fake it just by like like number of likes or

upvotes or we retweets but it's not going to be very long before a

sophisticated AI can figure out exactly what kind of images people like or don't

like and combine those things very quickly into like the most liked like

popular type of images ever because it doesn't have the judgments that an

artist has like I have boundaries you know like there's some places I just

don't go because I'm too good for that or I don't like that or I had a bad

experience with that the AI isn't going to care about that it's just going to go

right for like the bottom of the brainstem and just like the jugular of

like this image has all the different aspects and subject matters and colors

and compositions and pixel density it's going to be like the number one ultimate

type of art that someone's going to want like in their house and that's going to

be crazy to see that you know I really think so now I'm not just studying

Neuralace but also how to build artificial cortex and eventually

artificial brains and I really think it's possible to build an AI that's sort

of stacked in a way in a structure that it can become a sort of curator of the

you know a decision maker of the kind of patterns that it's

choosing to create like it can have boundaries as well as the point I was

getting to but so I really think that you know that so there's the sensors that

I'm planning to use for Neuralace and I'm planning to apply artificial

intelligence to them and I'd love to see you know some AI and inside the micro

dust VR and I don't know if you're sure yeah yeah we definitely like that's

kind of part of a roadmap is we're looking for you know we're looking for

that special brilliant weirdo out there it just doesn't fit in anywhere else and

we've just created this spot for them in microdose like we've got that we've got

the desk we've got the chair ready for our like lead AI micronaut so you're

out there let us know oh me me I that's me okay great that was easy that was

fast no so no of course I would I'd be glad to help out with microdose but so

where I'm going with with with the with the Neuralace is that I mean not with

Neuralace but with artificial intelligence is is not just it's not

just about graphics but you as an artist as you sat there you said there's all

these things like when you painted multiple layers or something you're

imagining in some instances like there's there could be a millimeter or

centimeter or an inch of distance between the layers that you didn't really

have the ability to add into at the time so you had this this sort of like

what I'm thinking is that if you're creating art and maybe you'll go back

and do some 2D art while you have this really advanced EEG cap on it and these

new sensors that I was you're gonna use for Neuralace where they can pin they

they just have improved sensitivity and graphical fidelity in in terms of I mean

not a sensor fidelity but what I'm what I'm saying is they can pinpoint like

they're kind of like light field sensors but they can pinpoint the origin point

of what's coming to them like light field cameras can pinpoint the origin of

where lights coming from and so if we create a cap of these while you're

doing art then I'm thinking that you know at some point the computer could you

and the computer could collaborate on artwork where you're drawing and it's

and it's creating stuff also I'm wondering how that would work I mean

there's lots of different ways that could potentially work and or you could

just like I'm I'm thinking at some point the computer could start to pull out

stuff that's kind of from your imagination but then like you were

saying without a curation thing it would make something different than you

would make but well I think the part of it I think where artists are gonna have

a little bit of like real estate like shrinking real estate in the future is

going to be creating the parameters of like the sandbox like we're gonna be

able to curate like the images that something draws from or we're gonna be

able to kind of create like the parameters of like the brushes and like

here's a bunch of landscapes and here's some portraits and here's this and

here's that and it's gonna be there's gonna be a field of artists that just

understand the right ingredients and the right way to introduce these things

into the like the deep they're they're they're they're they're they're they're

custom like deep dream algorithm you know that that's gonna be kind of a

signature style might but maybe just like there be image libraries you know

like the curation of the image libraries might be the aspect you know

definitely what AI is producing now is very much depending on its training

data so you could train it on your artwork for example and then give it

clouds and it would produce variations of your artwork in the clouds so going

so going back to what what part of the brain should we so so date so I was

talking so there is this so there's a tongue strip that's connected to a

camera for people who are blind and you turn the pixels from the video feed

into electrical signals on the tongue strip this can also be done with a

back strip basically the back strip or the tongue strip is like if you imagine

it's just a grid and if a pixel gets dark then you have an electrical signal

that that stimulates your your that part of your body and what happens over time

is your brain figures out visual information from the camera so you

learn people who are blind can see through this camera that's connected to

it to their tongue or to their to their skin on their back it turns out if you

stay in experiments with rats if you take an eyeball and you plug it into the

rats audio cortex the rats audio cortex so you have you have an audio cortex you

have a visual cortex so like the visual cortex is like in the back of your head

you know we had an audio cortex yeah so your brain I imagine it's gonna be a

future product that's like I'm definitely gonna make a particle called

audio cortex that's definitely whatever happens it's definitely gonna be a

particle name I'll send you more names of neuroscience items and yeah I just I

think the visual cortex just gets so much attention I don't watch sports at

all I almost kind of look down on people who watch sports but I thought to

myself if they named the sports teams after parts of the brain I'd probably

watch sports so the point is you can plug in an eyeball into the audio cortex

and your brain will figure out how to process it process it visual that visual

information so your brain is like a generic learning algorithm any sensor

that you plug into it from any place it can figure out that data so we don't

really need to in terms of in terms of neural in terms of neural lace in terms

of sending input back in a computer-to-brain interface like you were

saying earlier we don't really need to go to the thalamus what I call the

sensory integration center we you can just plug in something anywhere but I'm

thinking that your eyes might be the fastest way to get information into your

brain and so why mess with something that already works VR might be the input

for the audio and video VR might be the best the fastest input for for neural

lace that's a great I mean it's a much less obtrusive way of getting data inside

and in order for neural lace to become a mass commercial product we have to

think about you know getting the brain signals wirelessly you know with with

you know Facebook is working on all this really advanced medical imaging that's

non-invasive it doesn't involve surgery and so you could use non non-invasive

sensors to get at your brain data and deep and figure out what those patterns

are and then we can send data back in through the eyes and the ears without

being super invasive but my research does involve figuring out like not only

what the what what a brainwave pattern of of your controller is but also

figuring out you know how we send how the brain sends patterns from clumps of

neurons to other clumps of neurons and what the communication protocols are so

we could in addition to you know what if we figure out what the communication

protocols are but we decide to use a non-invasive device we could use the

knowledge of the communication protocols of the brain to design the optimal

exterior device for let's say you want haptic feedback right it's just like it's

just a could be a glove or something or a ring I said I like the idea of

information coming in and out between your optical nerves versus like your

spinal cord or having something go through your nose to your thalamus you

know a lot seems like a lot less points of air there for sure it makes sense

why mess with with evolution when you can work with evolution we've all

received data that way I'm a big fan of that for sure

this one is music this one is not music this one has audio reactive music this

is this is as reacted on music this is reacted on you

don't have both audio reactive and music reactive at the same time we might do

that for tomorrow but now we wanted to focus on one version that just really

focusing on the data from the muse because if we use audio reactivity to

this is too many things so you might not notice your brain wave I don't think

you wouldn't yeah it would be distracting from trying to focus on the

brain waves but our engine supports them both simultaneously they're not so are

the so the brainwaves are only driving some of the light and sound effects

they're not driving all of it no not all of it I mean there are some things where

your controllers driving some of it yeah still there's your controllers we

still have some pressure sensitivity because the idea is really just trying

to you know create as intimate experience as possible so we still want

to make sure that like there's some type of reaction happening from their hands

and pressure sensitivity and velocity like that still physical movement drives

a lot of it but behavior wise when you're looking in a particle and you're

seeing something some state change like whether it grows in size or it turns or

rotates or pulsates or the clouds moving that's all so what what I would think to

do is you know for the purposes of making your brain waves more noticeable is

it draw to dramatize the reactions from your brain waves to so you have really

profound visual effects and really profound and I don't know how like what

what you have in terms of what you can play with and how you can dramatize like

in terms of like if you if you put everything instead of because we were

saying you know instead of having everything like you know the cloud the

sky is either dark or light instead of that you have a spectrum of sky and

but if yeah if you can make the whole sky like sort of like into a gradient that

that dynamically adjust to every tiny change in the sensor activity from the

interacts on use then that's just that would be a more dramatic and you might

notice your brain waves more and they stick out more maybe I think partly the

challenges for the purposes of this demonstration is that I agree that like

the subtlety things are definitely kind of like where it's at in the long run for

someone that's using this like you know it's dosing on a regular basis and then

but the challenge we're doing here like what are the things that someone's

going to see a visible measurable difference in like two to three minutes

of a demo while there's like drones flying above their head and all these

other things happening too you know so it's like quick payoff and obviously I

think the the long-term being able to recognize it over time for people that

are going to be you know we definitely want to create the kind of experience of

people like you know come back to it over and over which I think it's so

going back to Google's deep dream the Google's deep dream it looked like the

program was sort of hallucinating like in the clouds it would see snails and

dogs and cats right and they said that they achieved this by you know basically

creating these feedback loops and they make in making the feedback loops work

harder and so I kind of my hypothesis is that when people are when people consume

certain kinds of psychedelic chemicals what's happening in the brain and this

is there's research around this is that their brains are working harder that

there's a lot more brain activity like if you had EEG going and you know when

someone's you know first working on learning a new task like programming

gonna have a lot of intense brain activity you know but as as Fong you're

he's the chief programmer is that his role as so as he becomes a master of

Unreal Engine like his brain activity chief engineer yeah yeah so as he gets

really good at coding and doing audio reactive stuff and eventually maybe EEG

is driving the audio reactive stuff or whatever you're gonna do it's you know I

actually lost my place so so we we were saying like one step before that oh yeah

okay so the idea is that when people are dosing with these psychedelic

chemicals it's forcing the brain to communicate a lot more and the brain is

organized in a way it's like there's a lot of little feedback loops there's a

lot of big feedback loops like the stuff that comes into the thalamus goes

through the whole neocortex you know like your light like bounces off your

retina it causes the ganglia to to send signals along the optic nerve to the

center of your brain and then it goes back to the visual cortex which is the

occipital lobes and the pride of lobes in the back of your head and the same

thing with your ears you have data comes from your ears it goes to the

thalamus and you know I can't wait for you that's an awesome roadmap so the

visual cortex is like a sensory I'm sorry the thalamus is like a sensory

integration center where all those stuff all that stuff collides on the way

into the brain but then it goes through the neocortex and the neocortex is like

a sort of like a deep learning hierarchy but the type of like a pyramid but the

top of that pyramid is the thalamus so it's like the entry point is the

thalamus goes through the neocortex and it goes back to the thalamus it's a big

feedback loop the entire brain the entire brain is a fractal of feedback loops

and even our perception of reality is in some sense a feedback loop in terms of

like when we observe something like the observer effect we are actually

affecting our what we're observing we're affecting it there's some sort of

messaging between what we're observing and the observed you know yeah I mean a

lot of times you know another more kind of kind of layman's way of kind of

putting that too is I think that our what our brains also kind of represent is

you know the highest pinnacle of you know at least a few hundred thousand years of

evolution of the best technology of like the meaning making machine you know

what our brains are really good at is making meaning out of chaos what what

the reason that we're here is that we were we evolved to be a look at the

planes of the serengeti and recognize the shape of a lion over the shape of a

gazelle and we saw that abstract shape and we could be like that's a threat or

that's gonna be food and the ones that figure that out are the genes that kind

of went down and so that's we still have those processes and on different types

of psychedelics I think that especially like they will talk about just they all

do different they're all very different and I don't like when they get all

grouped into one word you know but say for LSD I think LSD is like steroids

for your your meaning-making machine technology definitely you know it takes

you're so uncomfortable with chaos it'll just make anything up and it'll grab

and it'll it seems like the processor gets kind of it gets overclocked in

trying to make connections that are there or are not there and that

pareidolia just goes to the extreme and that's definitely and that I guess yeah

the machinery of a feedback loop does make sense on how those are kind of

like the gears of that that kind of turn the pareidolia that it's so desperate

to find something that it's willing to fill in more of the gaps and it does

usually and believe more in its results than it does in normal situations and

you've seen people that get caught in those loops where to physically the day

there's a guy that got caught on a loop over and over and over again so so

neurofeedback it literally is a feedback loop of so so so what you're doing like

if you look in a mirror that's a form of neurofeedback if you are meditating and

you're humming that's a form of neurofeedback why it's because what

you're seeing or what you're hearing so okay let me just stick to one at a time

and not to try to describe the whole thing so what you're seeing in a mirror

is your reflection and that that image goes to your eyes and your eyes it's

it's what comes into your eyes is connecting to the how you move and how

you move changes how you're like how your reflection moves so that's the

feedback loop so then that goes back into your eyes changes your movements it

goes back into you know you see it in the mirror again and when you're humming

to yourself like if you're meditating and you're chanting to a home or

something that's a feedback loop because your what you're hearing is causing

subtle changes to your movements in terms of your vocals because that's

movement as well say that your vocals are the same thing as moving your hand

right and those subtle movements are picked up by your brain which further

gets incorporated into the next movement that you make so that's a feedback

loop EEG is a feedback loop your entire brain is a fractal feedback

loop it's inside a universe it's a feedback loop that in now what you're

doing with microdose VR is you're creating a feedback loop from your

brain directly into the computer directly into someone's eyes and as that

impacts a person in their internal neural network in their brain it's going

to cause more changes to their movements including to their hands which is

going to further change their brain ways which will further change the light and

sound effects which will go back into that person through their eyes and their

ears and on and on and so this is a vehicle for it for increasing someone's

awareness if someone's overclocked with psychedelics or something they're just

going to pick it up that much faster that's that that is not a false

statement we have we have the video now for sure and that's what we're kind of

and within that context I think what's exciting about you know this is our

first entry point into using EEG and it's obviously like a really amazing long

fractal road of potential branches to follow in all of our futures but I

think one of things one of the places on that infinite roadmap we're looking

forward to is really getting to that sort of what they call this the closed

loop of the fractal loop where that feedback if we can start to identify the

feedback of the flow state and that's what we're really after that's kind of

the you know the feedback of the flow thing for yeah because once we can get

that and you can get some sort of feedback and to then to encourage as

much of that state because whatever happens during the flow state you know

I'm not a chemist but I'm sure that the the type of neurochemistry that the mind

is able to create within a flow state it's got to be just amazingly okay so I

heard of I got really deep into you know you know Ken Wilbur and I've studied

Ken Wilbur I've got really deep into this stuff and and one of the the things

that I read or watched in a video was was how there's two you can someone

categorize there's two paths of enlightenment one is like the Buddhist

path where you meditate and learn to calm your mind and the other one is the

avatar's path where you are creating the universe and those are two valid paths

there's no reason why an avatar should try to calm their mind and follow the

Buddha's path those are both legitimate past so when you're talking about a flow

state if someone's playing basketball and they get into a flow state and that's

their enlightenment fine but it's there's no reason why they should also

try to become like a meditating monk those are both valid past yeah yeah no

I'm I favor more the avatar path but we're you know we've designed this around

actually trying to accommodate at least as far as in the world of like the

creative imagination your creativity or the audio or intelligence is like your

audio intelligence your spatial intelligence which is responsible for

like painting sculpture most of the create visual creativity create creative

exploits and then your movement intelligence of like dancing and flow

you know our goal following like the Ken Wilbur superhuman model about raising

your levels he like a lot of times we specialize when you specialize in one

thing you become less of an expert in everything else and so we're really

excited about developing microdose into this kind of a platform to inspire

and you know give permission and opportunities to like the next you know

generation of these young minds like my little daughters you know it's like I

want to build a whole I want to build a new art movement for my daughter for

when she comes at age and she chooses if she wants to be an artist or not I just

wanted to be an entire new movement that she can select form and the pantheon she

might be an accountant but I just want to make that option for her she might be

a Buddhist instead of an avatar yes her chart her chart does have some spiritual

teacher stuff in it you know but it's got a lot of organization too yeah but I

think that that's where it's going like we're making this we want to make a tool

and make an experience that allows you to increase your intelligence is on I

think the things you're I think you're gonna learn in kind of a and so much

faster when you have access to the three of those things developing in a in a

complimentary and simultaneous matter than you were like you're gonna take

music lessons you're gonna go into a keto you know it's like we like to box

things box our possibilities so we're really trying to open as many

possibility creative possibilities and give people just another and give you

know giving people access to a new type of creativity and giving you know the

this universal infinite force of creativity more access to humanity too

you know it's a two we're just trying to like just just open up the like the

bottleneck in the bandwidth between the doors of perceptions yeah that's my

that's my I'm hoping that you know within the next year or so as soon as I

start start working on this as soon as I get the sensors be I hope I'm hoping

it's with the next month as soon as I get this new sensor from this company in

San Diego that I'm consulting for as soon as I get it in the next month I'm

gonna start you know I'm I'm actually you know my my my chart is to learn how

to do has learned how to use JavaScripts due to do to command 10th the

TensorFlow libraries and I'm gonna take advantage of the NVIDIA GPU beta so

that I don't have to have you know someone donate a super computer to my

cause to get on for sure yeah so here we are at AWE 2017 and for perhaps the

first time in public people are experiencing their brainwaves inside

this product microdose VR thank you very much thanks Micah
