"Yann LeCun on a vision to make AI systems learn and reason like animals and humans"

https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/?__cft__[0]=AZUQha0pJBd_kCWpxr1Kn-ma0vQLTqWN6QAlLJzf9MHB86xUlvPFxFSIr-aQc47F33GchU1tGb5QbIhrcTVtGNeelZ5fuch_fViBVqEMv54zPcMq0eRsdFK_etDQ0__YfBI&__tn__=-UK-R
https://twitter.com/ylecun/status/1541491973290446850?s=20&t=wcJwi4jLwUCmfLnhe4ZigA

I have several thoughts on Yann LeCun's model released on June 27th 2022, I am not done reviewing it, but off the top: the glial cell is easily described as a configuratior cell.

If could be that each of those roles are filled by both individual cells, by cortical columns, and by the whole brain (taking turns).

Mapping those roles to large regions of the brain is I think an outdated idea in neuroscience (that some neuroscientists do not yet realize is an outdated idea) The new thinking is that every part of the brain is doing a little bit of everything, at all scales.

Those roles might be what György Buzsáki calls the Brain from Outside In (matching observation to a concept instead of creating a new concept from observation.) György Buzsáki's book "The Brain from Inside Out" contradicts the typical "Outside-In" process of applying our models to the brain, instead of creating models from the brain.

The biggest voice in the space arguing for the generality of brain function across the brain might be Jeff Hawkins from @Numenta If you mix the ideas of Numenta together with Buzsáki, then you grok the majority of my book that I'm working on here.

coincidence detection & difference detection, in the human brain, via phase comparison yields a physics based process for choice at the cell level, cell assembly level, and whole brain level

but prior learning provides a learning framework for considering whether a detected difference is good or bad from the organisms perspective

# Yann LeCun's Talk on youtube https://www.youtube.com/watch?v=DokLw1tILlw

Minute 39:57 Yann LeCun: "A Path Towards Autonomous AI", Baidu 2022-02-22
Slide "training a JEPA"
It appears that Sx and Sy represent the same image presented in 2 different views, possibly different by orientation, magnification, or some other variation, it could be that each value in the image was randomly multiplied by an imaginary number.

The point is he takes the Sx variation, maximizes the information content (his dialog about max and min reminds me of how a dendrite sensor branches out to maximize its data sensation potential, and minimizes its data collection to a phase pattern with duration & frequency properties and then passes that phase change to the array represented  by its exit terminal)

The Sx variation he argues will become a predictor of error for the Sy variant, but the Sx in his model is modified by a information content minimizer, (again reminding me of how nerve cells maximally collect, summarize, and distribute phase representations out to an exit terminal array.)

but the point is that he wants minimization of information in the error predictor so as not to confuse the neural network as to what it should be paying attention to in generating feature representation I guess. on the side of Sy he is saying the encoded representation of Sy minimizes it's error prediction by comparison to the minimized information from (Sx, Z) (Z being the minimization function), but in a sense he is attempting to bias the results (or weight the results) of the identification of pattern Sy by comparing it to a sparse representation of the features of the Sx variant of Sy.

It's not a bad argument, I suspect that in biology there is no need to bias one side vs the other, so if you had the same program running in each of your brain's hemispheres, (to abstractly pull in an conceptual allegory that stimulates creative imagination for discovering potential applications of this concept in compatible substrates)

What seems obvious to me but is probably not obvious to everyone is that while the brain is self similar enough across the entire structure (Jeff Hawkins talks about the self similary of the neo cortex, and the similarity of the Hippocampus to a cortical column for example) the tiny learned custom variations in each part of the brain will serve to generate a natural VAE Variational Auto Encoder effect for comparing learned patterns in one modality to same or similar patterns encoded in a different modality.

I think Yann LeCun's presentation causes us to either intentionally or accidentally think of the whole brain as a variational auto encoder. Although he did not say this explicitly in words his usage of an image representing the brain behind his diagram of the parts of his autonomous AI sort of point us to this idea.


# Stacking layers of Jepa, so JEPA is like an abstract VAE Variational Auto Encoder, that has a minimized or maximally abstracted variation, or a sparsely represented comparator representation.

In Hierarchical or Stacked JEPA each layer is doing the same Jepa sparse VAE process, and passing it's learned representations upwards.

From this talk I gather that one can encode the abstract prediction of a JEPA as a frequency that resonates away into its layer after some number of intervals related to its duration and that would bring JEPA closer to biological realism, or JEPA could be combined with back propogation or combined with reinforcement learning, but interestingly in this model neither back prop nor reinforcement learning is necessary for JEPA to differentiate and reason about it's goals.

Lets recap: So a Jepa can be stacked, so that higher JEPA layers represent larger time scale functions in addition to functions that exist at higher levels of (chunked up) feature representation. The higher layer of Jepa might look for conditions in lower layers of Jepa that predict how close or how far away it is from some random goal such as driving to a desired location.

# At the 54:42 Minute Mark:
It is interesting that Yann LeCun describes the higher layer of JEPA as corresponding to a longer span of time, for me that evokes comparisons to the Reference Frame model of a cortical column that Jeff Hawkins talks about in "A Thousand Brains" because you have a fast changing input layer at the bottom, and a slow changing reference frame layer at the top that might indicate a pattern that changes on a larger time scale. Similar to the concept that place cell data changes quickly and grid cell data changes slowly as the grid cell data is a reference frame for the place cell (I guess that is an interpretation)

So each hierarchical layer of JEPA is doing Sparse Comparative VAE based on abstracted differences instead of being based on summations of generative models (like object segmentation or semantic segmentations of points) that LeCun believes cannot scale beyond a certain neural network size.

# End of notes about Yann LeCun's talk.

"Gradient-based learning drives robust representations in recurrent neural networks by balancing compression and expansion"
https://www.nature.com/articles/s42256-022-00498-0
