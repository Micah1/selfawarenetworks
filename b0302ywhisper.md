b0302y (Transcribed with OpenAI's Whisper)

Title: Neuralink Numenta Neuropysics NerveGear Happy Hour 

(This was originally a Clubhouse Chat)

------------------------------------------------------------------------------------------

Alright, let's go ahead and start the clubhouse chat. Start the room.

Alright, welcome. I'm going to invite people up in a second. I am just inviting some people to the room right now.

Welcome everybody. I'm going to invite people up here in a second. I'm just inviting people to the room real quick.

Just hang on a second. We're getting started soon. I'm just making a bunch of invitations before we get started.

I'm excited about invitations. I'll be happy to invite everybody up who's here. I'll just do that real quick.

Invite or raise your hand. Welcome, welcome. Make sure I have my volume up.

I'm sending an invite out to everybody.

I already sent one out to Anne. Anyone else, just raise your hand. Welcome everybody.

I'm going to go ahead and... This is a recorded talk, by the way. Because some people don't have a clubhouse on Reddit.

I managed the Reddit group called Neuralace and they said, hey, can you record it so we can listen to it afterwards?

And I said, okay. I just have to tell everybody that I'm going to record it and then Clubhouse says it's okay.

Let me know if you have any issue with that and we'll make sure to stop positive recording when you're talking if you prefer.

Welcome everybody. I thought this would be a really great topic because there is a lot of crossover between what Neuralink is doing and what Numenta is doing.

There's also a lot to say about neurophysics and nerve care. So I think Numenta is closer to the discussion of neurophysics and Neuralink is closer to the discussion of nerve care.

So let me go ahead and pause and just welcome everyone and give everyone a chance to say hello and speak.

Yes, a very interesting topic. I had a room in the Digital Humanist Club about, what is it now, 12 hours ago, something like that on the ethics of Neuralink.

So just looking at human brain interface more broadly and all of the areas of crossover and that turned this Clubhouse for instance into a discussion of every type of related topic.

But a very interesting space and I think with Neuralink being I guess more evasive and having grand ambitions than a lot of the other sort of external devices

and ultimately looking at producing feedback and also augmentation for healthy humans, not just their shorter term ambition to treat illness and disability.

It's a very interesting space and it seems to be a real crossover between biotech, AI, other computational tech as well as surgical work and even animal research.

So really touching on lots of parts of advanced science at the moment and a good illustration in my mind of convergence.

So I'm very interested in what other people have to say on the topic. Thanks, I'm David, I'm done speaking.

Wonderful.

Katerina, do you want to go next?

Hi, thanks for having me.

Yeah, I'm a neuroscientist so I do, so my main expertise is actually electrophysiology and vivo and in vitro recordings, slice recordings and imaging.

And yeah, that's why I'm very interested in the topic for human application.

So thanks for having me in the room and I'm done speaking.

Wonderful, thank you.

And our next speakers, Joseen, do you want to go and then Dave and then Kenneth?

Sure, sure.

Yeah, so I'm a machine learning engineer at the company and we are working on a clean energy smart grid and so on.

However, most of my background have been in healthcare.

I've worked in genomics and also on clinical trial and so on.

So yeah, I haven't worked directly on EEG signals and so on.

However, many techniques that I use in machine learning actually apply in those ways.

Thank you.

Good morning from Malaysia.

I'm Dave.

My background is on fashion design, also computer science.

So I have a very diverse interest and I'm here to observe the advancement of the technology and also I'm in the journey to searching of the balance between technology and human touch for the society.

Thanks, I'm done speaking Dave here.

And Kenneth.

Hi, my name is Ken Granville.

I prefer to be called Ken.

My company, for the past decade, we've been focused on creating a system that bridges the gap between how humans think and how we express ideas, how we communicate and how that translates into machine behavior.

And some of the things we focused on, aside from patents relating to signal processing, that allows us to more precisely and efficiently process large amounts of data and map that data to semantics such that we can produce code, generate code that's also highly efficient.

We've also focused on creating what some would agree is equivalent to the periodic chart or the periodic table with elements that allow you to essentially understand what things are made of and composed of.

And so our equivalent to that is a semantic periodic chart for computing, where we have 256 elements that map meaning units to what humans express, whether it's words or gestures or typing text or any signal data, basically, such that we can then in real time

competitively generate highly parallelized code, the benefit being the ability to see the machine produce results and have that correlating to what you actually meant, which you can validate.

So we call that part natural language dialogue.

So it's quite a complex and long project that's culminating in some products coming to market this year. So, Ken Granville, I'm done talking.

Thank you, Ken. Frank, do you want to say something?

Sure. Thanks, Micah. So you're always interesting in your rooms. It's like drinking from hot fire hose. So for me, I mean, my interest in this topic is essentially connect to my fascination with brain, how it functions and my take on my angle approaches the have a background in a

I have a broad background, a breath, better than depth. I consider myself a material engineer. So microscopic going towards microscopic, hopefully, hopefully, and see how that, you know, interface with the brain.

Be it electrodes or drop delivery or, you know, the, there's no wearables, especially flexible substrate electronics and things like that. Also new materials.

Also, I also take a philosophical approach, as I consider myself also had a training in, you know, mathematics and all that doing modeling and is like I can contribute, you know, bring all, you know, different field of concepts and contribute.

Yeah. Yeah. Thanks again.

Thank you very much. Chirita.

Hi, I'm Chirita. I'm currently a PhD scholar and I'm working on using applying rather machine learning, mostly statistics to solving problems related to cancer prognosis and prediction.

To make life detection cheaper with liquid biopsy have more discover, you know, better biomarkers and gene panels for liquid biopsy with machine learning.

I'm very much interested in the working of the brain as well. And I think that understanding how the brain works can, can be maybe a key to developing better AI, better architectures.

There's just interest and I'm not like really well versed in that, but really interested. Thank you.

Wonderful. Thank you everybody for introducing yourselves.

I'm glad you're all here. So I'll go ahead and start by introducing what I think is the major link between Nementa and Neuralink.

But I want to also sort of like give a brief description about what we each of them is just just a very general description.

I know some people have heard a lot and some people have heard a little.

So the big focus, the big research focus at Nementa, you know, I guess recently we've been hearing a lot about cortical columns.

So there is a researcher called Vernon Mount Castle who proposed that the neocortex was made up of basically identical cortical columns everywhere

and that these cortical columns are helping to organize the information in the brain.

And so what Nementa is saying is like, okay, well, we have this idea, it's the idea is reference frames.

And the idea is reference frame is similar to the relationship described between grid cells and place cells in the entering of cortex.

And the idea is that when you have an object in the room with you, that object might be tracked by at least one,

let's say it's your phone, the EV analogy is your phone because usually if you're using clubhouse you have a phone, right?

So you've got, you probably have at least one, maybe thousands of cortical columns that are each rendering a visual representation of your phone.

That's one of the ideas here.

But you would also have, and I asked Jeff Hawking this when he came to clubhouse, I said,

but you would also have like probably a thousand representations of the phone in your audio cortex,

which would be based on audio representations of what your phone might make if you banged it on a table or if you scratched it with your finger.

And then you might have a thousand representations of how your phone feels based on where you put your finger.

And that would be in your somatosensory cortex.

And then you'd have the representations in each of your primary sensory cortices would be voting.

Basically, you know, I guess if the cup is predicted, if the phone is predicted many times in your somatosensory cortex,

in your visual cortex, and your audio cortex, the many different, the thousands of different cortical columns that are each representing a version of your phone

will eventually vote on some version of your phone that is accurate enough, and then you don't need a thousand representations in each of part of your sensory cortex.

And so in that case, they would just, you know, I guess they would just compress down to one.

But the key thing is, so a lot of people don't know that the columns themselves are not multimodal,

the audio representation of your phone is going to be linked to the visual representation of your phone and the somatosensory representation of your phone

so that you could trigger, you know, the visual activity that correlates with the audio activity from the visual cortex or vice versa from the audio cortex.

And so that's interesting because what we're talking about is that the representations of what you can see, hear, and feel are basically in the cortical columns.

And this is what can be reached, the cortical columns can be reached, of course, with, you know, like the Utah Array, I guess,

but there's different, you know, you can stick electrodes in the brain and stuff like that,

but specifically Neuralink is coming along with really some advanced hardware that, you know, when you have,

when they're doing things that are light years ahead of the Utah Array in terms of like by putting 16 different electrodes on each of the hairs,

I say hairs, it's not really hair, each of the sensor strands.

That means that you, so imagine, I guess I like to describe this as like, imagine that you have a completely dark room and each electrode represents a microphone.

And so if you drop in three different microphones, then you could potentially begin to triangulate where sound is coming from and where spike is coming from, potentially.

But even then, like you might think that, because a neuron can spike from different places, you might think that the same neuron is actually more than one neuron.

Or you might think that two neurons are actually one neuron, because you don't have enough information with just three electrodes to really try and give it where that spike is coming from.

And to make sure that it's just coming from, so it's a lot harder to do imaging, medical imaging, when you just have three electrodes.

So now we have 16 electrodes for each of those strands, and then we have, you know, I guess hundreds of strands, you know, so it adds it to like, right now it's like, it's like 3,000 electrodes in one area or something like that.

And I guess, but what's cool about that is that your ability to develop an accurate spatial resolution of what neuron is firing and where it's firing,

and sort of distinguish between two neurons that are close to each other, and also figure out when it's the same neuron that's firing in different places, that is going to really improve a lot.

And so I asked, what I asked, I got to ask Superta who works with Jeff Hawking yesterday, I said, so do you think neural link devices would be able to decode cortical columns?

And would that lead to decoding? Basically, on my mind, I'm thinking that basically leads to decoding images from the brain, sounds from the brain, and human thoughts from the brain.

Because it's, you know, basically what we're talking about with cortical columns is that not only are we doing sensory stuff in the somatosensory cortex,

but when you talk about the cortical columns in the prefrontal cortex, you know, we're talking about ideas.

We're talking about, you know, you put it in language areas, we're talking about language, we're talking about words.

And so the brain's representations of words, ideas, you know, high level concepts.

The idea, so what Superta said, and I'm paraphrasing, was that, yeah, if you put the electrodes in the right places, like all over the place,

then you could do things like figuring out when a neuroimager, the other part of my question is, would a neuroimager be able to tell that voting has happened, right?

And would a neural link be able to decode cortical columns and voting by listening to individual columns and decoding them?

And Superta's answer is basically yes, yeah, if you stick enough electrodes around in the right places and you forgot what's firing and when,

then yeah, we could eventually decode these micro cortical columns and figure out exactly what they mean.

We could figure out the network protocols between them, and we could figure out what they're representing in terms of, you know, visual, what images and what sounds, and so on.

And interestingly, I sort of like asked him the question, well, could I, like, this was, I guess, kind of silly.

I said, if we have this network protocols, could I, like, you know, do an HTTP request to the audio cortex to query some information from the visual cortex?

And he said, well, you know, aside from the HTTP part, which is, you know, you don't actually use HTTP on the brain, but aside from that, basically yes.

Because there are already examples in neuroscience history where, I mean, like, you know, basically you hear, like, think about it, if you hear something, that might trigger a thought, a visual memory of what, a visual prediction of what you're hearing.

Like, if you hear the sound of a big cat, like a puma, then you might visualize it. You might visualize a bobcat for, you know, for lack of recognition of what exactly it is, right?

But it's a big deal when you hear that sound, right? So anyway, I'll go ahead and pause and let people reflect on that.

This is Ken, and that is definitely fascinating to me from the standpoint of being able to map what is an accurate reflection of a signal that you can verify discreetly,

not simply as, you know, some textual or other representation, but a signal that you can map to meaning and represent it in a number of ways.

You re-render it as what it would represent in terms of something that is an image, like an image of a big cat, as you suggested. I find that very fascinating.

It's very consistent, by the way, with our work to sort of allow the emphasis to shift away from what we use in language, for example, which is text, or, in the case of machine code, also text to something that's more distinctively meaningful

so that you can verify it and represent it more efficiently. And again, Ken here, I'll stop talking now.

So in mice, we already are doing that. I mean, it's simpler, but how we do it is we visualize with, like, NWM cell mice lines, we visualize which types of cells are active and which type of context are during a specific memory that the mice learn.

And if we stimulate exactly those cells, we get, like, the same behavior or whatever that memory, that programmed memory was before, we can elicit that.

And actually, we can transfer that from one animal to another, that context learning and combined with a fear, for example.

Yeah, I believe that we can't have transgenic people to visualize, so we have no other choice than make grids with a lot of electrodes in humans.

So, yeah, if we have enough data, I think we can recreate memories and, you know, help people that have Alzheimer's and other disorders to compensate, basically, for some sort of damage or stroke or whatever.

We just need to record enough data over a long period of time. I don't believe that in every human, every pattern will be the same.

I just think to be able to help people when they have a stroke or when they have some sort of disorder that made lesions in the brain and so on.

The ideal situation would be that people would have already those type of recording devices in their brains beforehand when they were healthy, so that once the damage occurs, then that this type of devices can just, you know, take over and replicate what the brain was doing before, ideally.

And doesn't it matter then that there's a semantic representation that can be mapped to the individual's way of thinking?

Yeah, exactly. So, not every representation will be exactly the same in every brain based on different experiences, based on different context data and so on.

Yes, what really makes that a big part of our focus of my team is that we've observed what happens in computing with programming languages and how so much is not represented.

There isn't a semantic representation sufficiently to determine what does this mean in context of the code writer or the person who had the idea or what they were trying to achieve.

So being able to, on a very granular level, capture that meaning and represent it semantically is, in our view, the way to personalize it and make it very much more precise and efficient and we needed to build a different approach to how the data is managed as well.

And so we focused on translatable data structures that use sort of a blockchain kind of approach with hashing ideas or tokenizing all of the very discrete data elements in the math that supports the machine behaviors.

The only question I have for everyone, because I'm thinking about this, let's say we can map all kinds of memories out and we can replicate them and make people that get Alzheimer's or so remember and perform a regular life.

I'm not sure if it will feel the same way because you won't have maybe, you don't have this neurotransmitter release in the same way and in the same abundance because you have an electronic device taking care of retrieval and so on.

So it will be a memory retrieval, but it won't maybe have that much of an emotional link to it.

That's a very good point if I may real quickly, in other words, did the person feel fear when they heard the big cat or did they feel a sensation of delight because they actually know the big cat or own the big cat or have trained the big cat or, you know, a myriad of possible emotions are those also retrievable.

So you're referring to a qualia, is that right?

Yeah, we're talking about a qualia being represented in your internal representations of what you're seeing and hearing being represented in the cortical comms of your visual cortex and your audio cortex and what you're feeling represented in the cortical comms of your somatosensory cortex.

And that being a decodable pattern, that neural link will be able to detect because it's going there, it is going to be decoding that.

And then, so Catarina was just saying that she's already doing a version of this with the animals that she works with, with the mice, and it's probably a more basic version.

You know, there's a lot of time in the room with the mouse, so that might make identifying whatever the mouse is interfacing with a lot easier than, you know, with a real human being.

If you're not in a controlled setting, if there's like tons of things, how is the neuroimager supposed to figure out what is what, right?

I think you'd have to simplify the environment a lot.

But also human beings might have a much more complex pattern, and figuring out which part of that pattern is the correlate to the thing that you want to detect.

It's probably going to be difficult because it's multi, it's a temporal pattern that has many different, well, I don't know, maybe Catarina can speak to this, but my question actually to Catarina is not only that,

but also about stimulation.

So someone said that a neuroscientist in another group said that Neuralink is probably going to move around.

I read in the book Rhythm of the Brain that these brain-computer interfaces can't move around every time you breathe,

including like the kind of electrodes they call, what is it like, something like tri-electrodes, I forget, but they can move around a lot,

and that means if you're trying to stimulate the brain, that electrode might stimulate a lot of things that you're not intending to stimulate,

and that's a very messy problem.

And so the neuroscientist that I was talking to is Scott, F. Scott, he was saying that what could probably happen is that you could end up stimulating a whole bunch of things you didn't intend to stimulate,

because you might have a neuron that is running through the area that you're stimulating,

but it actually is also going to the other side of your brain or to the other side of your body, maybe it's going to your foot or something.

And so you could be doing a lot of accidental stimulation, and for that reason he was saying that doing stimulation in human being might be a ways off,

but I'd love to hear sort of like what Catarina thinks.

Do you have some ideas about how we might have like a really high-resolution stimulation of the human brain,

and I guess we'd have to adjust it constantly if these sensors move around?

So there are some new technology developments that we have in the institute also for monkeys that,

so the implants I use, like previously the implants that I used, you had to like drill big holes in the skull and fix them with dental cement,

because we of course want to be sure, but so how I stimulate is different as with optogenetics.

So there is a push from some groups to get it into humans, so the good thing about optogenetics is that you use a genetic code

and you use a specific promoter so you can introduce those into the brain,

and by using those specific promoters you can choose which type of neurons in which brain regions by injection location.

I should express that light receptors, they are from algae, and by using that tool you then introduce a light source into the brain,

and that's how we do it in animal studies to be very, very precise with the cell types and which cells and where you want to stimulate,

and limiting toxicity, like excitatory toxicity with electro stimulation, if you use light it's less,

and you have a more temporal precise way of stimulating in milliseconds.

I can stimulate for only one millisecond and only elicit one action potential, or even if I use less light, I don't even elicit the action potential,

I just elicit EPSC, which is under the threshold that the neuron gives this information to the next neuron.

I just make this neuron a little bit more excitable on a specific region of the brain, but not the whole neuron gets excited,

so it stays in that neuron, so that's the level of specificity, like location-wise and temporal-wise,

and the amount of stimulus we want to elicit, that's what we have in animals.

So to fix that problem, I think the solution would be to use that in humans there.

I think only a very few clinical trials for people that are blind, that are starting to use optogenetics in humans.

We don't see too much neural damage, we don't see brain tumors with that technology,

and it's been done now for over 10 years when I first started making transgenic mouse water.

It was in 2007 or so, so I think that would be the way to solve this.

The other thing is you can also, at the same time, introduce proteins that give a fluorescent signal at the same time,

so you could also record what's going on in the brain, and we can trace those specific neurons that expressed this over time

while they, let's say, learn a fear in a specific context, and we change the context and see what those cells are doing,

and then we extinguish it, and then we see how the excitability changes.

So this would be the technology to solve that problem.

What it doesn't do, though, is fix neurons when you get Alzheimer's, right, so that wouldn't address that issue.

So Ken, what you said made me imagine that there could be a future version of Neuralink that has light electrodes,

and that the masses will be basically, their DNA will be altered so that they can respond to optogenetics.

Do you think that's realistic that optogenetics could become like a mass market solution for human beings down the road,

or is that just unrealistic?

There's some push-and-extreme situations, right, where people are totally blind and they really want this because they don't want to stay.

Or, let's say, severe parkings that you uncontrollably, you know, you can't basically live your everyday life anymore.

Your life quality is so low that the cost of the risk is, you know, just outweighs the...

Later on, I think, at some point at will, I'm not sure how soon, once we have more, we could also...

We don't have to have it permanent, right?

We could also just introduce, like, we are doing right now with the...

We could introduce RNAs instead and have, like, a temporal...

Like, that it doesn't actually change the genetic code itself.

We could think about those type of solutions, but I think at some point we will be more comfortable with changing, you know, the genetic sequence.

And it won't pass to the next generation, right?

So how we do it in mice is we introduce a virus, we inject it in the specific brain region needed,

and it will, as I said, only be expressed in those neurons and those cell types.

So the risk is relatively low, and if you don't inject too much.

But as I said, what this won't fix would be the person has a stroke in that brain region.

You can't regenerate cells with this technology.

And with electric stimulation, you basically...

If I understand it right, you want to basically compensate for loss of brain tissue, basically.

You would then need to introduce stem cells that have these receptors, which is also being studied right now.

There's, like, spine mice that have neurons using optogenetics, and then they can walk again, basically.

Before I say anything else, I want to give the three new speakers on the stage.

Zock might be mispronouncing your name.

Carrie and Rhysam, all of you can take a turn.

Hi, I'm Sooch. I'm from Mexico, and I'm a genomic science student.

And I'm extremely interested in the brain.

I think it's, like, literally the most amazing machine in the universe.

Like, yeah, it's real amazing, and I would really love to channel my career towards neuroscience.

So this is a really interesting topic for me, and I was actually going to mention also early on the discussion of optogenomics,

but Catarina also mentioned it, so, well, it's not that necessary now, but thanks.

Carrie, if you want to say anything.

Hi, guys. Carrie here. Thanks for having me.

I also wanted to second what Catarina was talking about in optogenetics, and that's why I put my hand up.

She's pretty covered a lot. I think that one of the other things is just that ability with optogenetics to actually identify neuronal circuits.

So just as a means of gathering that sort of base data that we're going to need to be able to start actually manipulating these networks,

I think, you know, is incredibly important, and that might be something that takes a long time,

but I think with AI and other technologies and, you know, it could be, you know, more exponential than we expect.

Yeah, so that's what I wanted to add. Thank you.

And we sound. Yeah, I've been kind of, we had a good conversation the other night on the app in a different room talking about Hawkins theory

and how it relates to the stimulation and that, and I've been kind of pondering it over trying to, from the position of, you know,

let's say the theory is correct and how does that really end up connecting with neuroscience and, you know, what needs to happen next and BCIs.

But I won't dive down that rabbit hole. I do have some, you know, kind of deeper concerns with the problem at hand,

but I was just kind of building off of the conversation about optogenetics. You know, there's some serious limitations with that,

and in humans, in my opinion, you know, none of the transgenic techniques or anything like that that are real robust in animals are something to even think about in humans.

So we're down to thinking about, you know, viral and using viruses to express these options and neurons.

And, you know, it's interesting with how targeted that kind of thing could be. But, you know, right now we're very bad at these things when it comes to getting them across the brain barrier.

We're talking about a very invasive local injection that would need to happen, which is, you know, I mean, as an alternative to a full on neurosurgery that sounds like a good alternative,

but it's, you know, something that would need to happen. And, you know, to kind of point at the deeper problems here, you know, a lot of these virally expressed options,

they only last for a certain amount of time. So the question is, the real question is, even if you did get good at doing that and you could do it safely in humans,

what is it that you're going to do with it? Because you're going to, you're going to have a small time window.

It's not like you're going to be able to monitor and stimulate these neurons for life, right?

So we need to understand what it's going to take to open up plasticity mechanisms.

And it's going to need to be paired with therapies. We need to understand how movement is related to what cortical columns are processing and incorporate that into our therapies,

you know, combined with stimulation and maybe stimuli. But there's just, you know, there's a lot of big, deeper problems in between that being useful and even, you know, getting it to work in practice.

Anyone can go. Yeah, may I ask a question? I'm Dave here. If I understand you correctly, Katharina, that you already can achieve the food to blacks.

That means you can read the signal and yet you can stimulate the signal to the mice. So in this case, did it ever trigger you that a certain action or certain things that can be already stimulate from the universe to the object?

So are we reading the signal from the object or the subject itself or is it a signal that is broadcast from the universe? That is the first question.

The second question will be human is a more complicated creatures that we can relearn and re associate the whole experience or things when when this action is triggered, the association of the brain signal and metrics might be changed

and relocate to other location. Did you already having a study into that direction? I'm done speaking this year.

So if I understand, so the signals. So when it's okay, so there are different ways of studying the signals of brain in vivo when we use like calcium imaging or these

n-gram transgenic mice that those are fluorescent signals and we of course always check how the basal fluorescent signal is of the brain and tissue that doesn't express.

So that's how we basically make sure that we are recording the right signal when we record from cell cultures or from sliced tissue.

There's a bigger risk and electric record like recording of electric current or there we we have further decay to surround we we we that the signal is actually really really small.

So we need amplifiers and so we have to make sure that the noise is low that the signal is high compared to the noise.

And yeah, as I said, we have further decay to surround can't have like nowadays the amplifiers are pretty good at blending that excluding noise.

So yeah, and then we stimulate the cells with either optogenetically like actually just optogenetically nowadays or with and see how drugs do and things like that.

So and then those signals change based on what we do to the cells and then we do control experiments, of course, always we, for example, just inject or have like a virus that has like the promoter, but then not the the the option encoded in there,

but something else like let's say just GFP or something. And then we do the same procedure and then if we don't see a signal, then the the signal that we recorded in the actual experimental mice should be real.

So we always do a control experiments. We also just inject the vehicle, for example, and so on and so forth. So we make sure that we are actually recording what we attempt to record.

So I would say we use control experiments to make sure of that. And then the options, it depends. So transgenic mice have the off the the channel rotopsin or nphr whatever you express their whole life in there and the levels don't change.

So we like to use those for for experiments where we let's say come compare different groups of animals of treatments, or we want to compare different brain regions because if you inject the virus, you always have to make it's more complicated because you always have to make sure that you have the exact same amount of

virus. And you always inject it in the if it's a deep brain area and very small brain area that you hit the target and you always are very consistent with your injection. So that's why to make it easier, especially for lab technicians and students.

We prefer using transgenic mice that express it in the brain regions we wanted to express with the virus is different. We have to wait for a few weeks depending on the neuron type and if we want if they are long projections we have to wait longer.

But the waiting time is actually more the issue not the issue in mice and rats that we have that they become the active over time. They stay they stay in there for let's say months of experiments.

Of course the lifespan of a human is different and we would need to check but my brother for example is a neurosurgeon he uses the brain stimulation for Parkinson and and other type of disorders and those are really major brain surgery so just injecting a virus and injecting

an electro cell light sources nowadays we have very very small LEDs that are flexible and so on. I think would be a less major surgery than what we have right now so this would be an improvement and since we inject directly into the brain we don't have the blood

brain barrier issue. Yes I'm done speaking thank you.

I just had an idea that if anyone wants to let the current speaker know that they have something to say they could flash their microphone and then if you're speaking currently you can if you see someone flashing your microphone you could you could hand it to them directly.

So go ahead and Hosein. Thank you very much very interesting panel. So actually my question in fact my son also is a neuroscientist and he also works in optogenetics.

My question is that can we like a stimulated neuron with two separate basic resources one for example we stimulated with the photon that you know that create you know for optogenetics and another one you know for another for example source.

So the idea is that in a way like more or less this is regarding the question that we some brought that what happens when basically no longer that virus has any effect on the neuron and so on.

Maybe once you know two separate stimulation come to a neuron then after after why you can turn on turn off one and the other one actually continue stimulating it.

You know what I mean like like more like a conditional stimulation or something like that.

And if one of our geneticists in the room wants to take and wants to respond.

Well yeah so well you can also yeah you can express different type of absence.

But you mean then also then using let's say electric stimulation separately. Yeah.

And I think that was because currently the issue I think is that those electrodes when people move around a lot they change their position.

And then you could be ending up stimulating completely wrong wrong wrong brain regions and neurons.

Yeah. And I was just introducing what we do in animals to solve these because we had those issues.

And so that's what we use in animals.

So because we had those issues and we want we need to be very precise because if we want to know which pathway or which cell subtypes or how many let's say we want to know how many neurons

we need to make an eye blink movement.

So these types of questions we could answer with this technology to be very very precise.

And yeah but you can also use options that are inhibitory.

You can use options that have that are active with different types of wavelengths so you can distinguish between brain regions that you want to excite

and now so many options and then there's options with cold light so you have bioluminescence options.

So you could you could combine different types of options to distinguish brain regions cell types and so on and then distinguish it that way.

Let me go through one example so I clarify my question. So let's say for example I already have no link in my brain and it's optogenetic you know enabled one.

And so anytime for example I hear a specific sound you know that there is also a signal goes into my brain that makes me hungry you know.

So so basically that optogenetic signal is for making me hungry.

And then after a while I associate these two together.

So even if you don't basically trigger that that signal you know in another link anytime I hear that sound you know I get hungry.

So in a way now my brain you know the plasticity of my brain cause that I trigger on something which is not typically you know trigger this one.

Yes yeah sorry I didn't understand your question before yes we can definitely do that and people have been doing that in animals.

For example they re reassociated traumatic like bad memory with a reward system and then the bad memory then it wasn't a bad memory anymore.

It was a it turned into a rewarding memory with a combined with a signal so yeah we can definitely do that.

But as I said this won't this won't solve the problem of damaged tissue like once Alzheimer kicks in or or a lot of seizures over time make lesions in the brain severe depression make lesions in the brain.

And the loss of that tissue we can't with this system create new cells.

What we can do is facilitate the implementation of stem cells so that they will be integrated and also active in the system.

We can facilitate that with with options.

But so one of the things that I think we said was that it so with optogenetics you basically described a proof of concept for when you are able to decode the cortical column by by with optogenetics.

And then you're able to to visualize what the sequences of neuro firings mean and then transfer that to another animal.

So you could or you could just basically you could implant memory and that could be I guess a visual memory of something bad but then you could reshape it by recombining that memory to make it into a visual memory of something good.

And so this as a basic perfect concept that basically establishes that the neuro columns that that the course are the cortical columns are are when we decode these in human beings at a high resolution.

We're going to be able to do just that we're going to be able to grab high resolution images from the human neocortex in the cortical columns and transfer high high resolution images to other human beings to pull images out of one person's brain.

And simulate another person's brain when that would be basically the the the future generation like a next generation of this sort of proof of concept that you have now in a more primitive way with mice.

Basically, this proof of concept that you're doing with optogenetics establishes the basic idea.

And so I wonder if a couple things. One is, can you share some resources with all of us, Katarina, that's like maybe like paper or two that are really great for talking about just this exact thing of transferring the memory from from decoding the cortical column and transferring memory reshaping the memory.

And just a couple of resources, if you want to share, you could put a put on your Twitter or something if you want to.

Yeah, I can I can do that. I can do that later.

After the conversation, I'll put them up on my Twitter, some, you know, some papers have to have to find them. But I can put them up. I'm not sure if they are all open source. But if you direct message me or something, I can send the PDFs as an email then or something.

So, okay. So I also wanted to ask you another question. So there's another hypothesis. So it's not just a neural link that is, there's a lot of different new brain computer interfaces that are being developed.

So it's not just neural link. It's not just optogenetics. There's also something called your rock some microwave imaging. I wonder if anyone in the room has heard of that.

And you can tell us more about it. There's there's there's new there's a new thing called well there's there's an old there's it's not it's not new but it's sort of like what's happening is there's electrical there's EIT or electrical impedance tomography.

What's new about it is people are using deep learning with electrical impedance tomography to to get massively better results and massively massive improvements in image resolution with with deep learning combined with EIT.

So another thing is, I just saw a paper for laser Doppler holographic. And of course, you know, everyone knows that Mary Lou Jepsen is working in laser F nearest holographic.

The laser part of that means that these that the camera sensor is going to be is enable will enable the detection of single neurons at the firing speed of EEG or the firing speed of, you know, based basically the same the same rate that that the neurons are firing.

Just like what what what Neuralink is doing, but it's going to be able to you're going to be able to basically have this high resolution holographic image, which combines laser with functional interference spectroscopy or laser with Doppler.

And you're, you're doing holography, you're reversing the refraction of light so you can you're combining this with ultrasound to get greater depth. So you can get all the way to the thalamus all the way to the core of the brain through completely through the neocortex.

And, and then the idea is then that's just for the imaging part. So we have very precise imaging of down to the neuronal level at this at the speed of EEG.

But the idea here is that stimulation might happen with ultrasound.

So I kind of wanted to ask them, the neuroscientists in the room, what do you think about the plausibility of using ultrasound, you know, in combination with Neuralink or or any of these other technologies to be the the the tool that's going to allow us to target and stimulate individual neurons and individual

parts of of cortical columns and I'll pause there.

I'll jump in there. Like, I, it's not a lot. I haven't read a lot on the topic. I know that they were using ultrasound to disrupt the blood brain barrier and to directly target cancer cells and even plaques in Alzheimer's disease.

And I am not up to date on that. So I'm not quite sure where the current research is.

Thank you. Anyone else?

Michael, may I ask a question?

Yeah.

The example that you give that.

Can you still hear me because I'm getting a very bad signal here.

Yeah, I can hear you.

So if we be able to project a set of action on memory to another subject.

And in terms of reading device, as a researchers, you are reading the brain signal.

While if there is a projection is going on, do you have a device that detect that the current signal that I'm reading is originate from the subject itself or there is some other broadcast is going on.

So my question will be.

Do we have the device to detect that at this moment and then also the spectrum of energy if we're talking about the full spectrum of the frequency from sonic vibration and then to static EEG and then from static EEG and then they go higher to light.

And then is our reading devices are targeting into the correct spectrum of the original signal to the brain to trigger some action and or metrics of memory to trigger a movement or something.

So is the researchers aware that the full spectrum of the frequency that we are researching now they are limited to EEG and limited to light weight.

But do we look into the area of the full spectrum and the change reaction of the energy change form so that we can drill into the most foundation of the original signal was created.

I'm done speaking this year.

Okay, anybody want to go?

I don't know the answer.

Does anyone else want to answer it?

I didn't quite catch all of all of what you were saying.

Sorry, Dave, but are you talking about the actual frequency is in like what hurts?

Like how many hurts?

Like for example, I use low intensity repetitive transcranial magnetic stimulate stimulation, which is a form of noninvasive brain stimulation.

And we use that to modulate those neural pathways and you can use, for example, one hurts to inhibit a particular pathway or 10 hurts to excite that pathway.

Is that what you're talking about?

Sorry.

If you look at the energy transform change itself, starting from sonic, we have the object vibrate and then from the vibration it might create the magnetic hurts that you are currently working on on your reading device.

And then from the magnetic and then is slowly more intensified.

It transforms into another type of energy form.

So in terms of technology as a researchers, we only limited to the technology we're having right now, that for example, maybe you are reading from the magnetic field, point of view, you are reading from the frequency point of view.

But the full spectrum of energy transformation that if you don't, how to say, to master the whole change of the transformation, then we might only research on a certain area.

In a gap that we don't know that after this EEG from one hurts, it vibrates intensively, it transforms into another type of energy that can be measured also.

So are we reading at the root source of the original brain signal creating creation itself?

So if we can tap into the root source, then we, how to say, we can have more accurate data.

For example, if we can be detect is this brain signal is created by the subject itself.

That means the original subject itself or that is some other stimulation or broadcast that from external fact that is affecting the brain itself.

Do we have a device to read that at this moment? That was my second question.

Yes, we can record the signals of the cells with electrodes and so we can do it two ways.

We can record the electric signal and what we can do also is if the animals have either voltage sensitive dyes in the brain or in the brain cells or like a genetic encoded signal, for example, calcium signal or chloride signal,

then we can also visualize that activity so we can and then the frequency.

I actually did some work recently where a changed basically a memory from like I elicited the long term memory with high frequency stimulation.

And then I turned down this memory like I did compulsive behavior.

I generated compulsive behavior specific one to a specific movement and turned it into a compulsive behavior with high frequency stimulation.

And then I did low frequency stimulation that was known to introduce LTD like long term depression. And then I got that compulsive behavior back to a normal level with it.

So frequency matters for stimulation. Thank you.

That is amazing. Please share your research paper when that's out. I'm looking forward to it.

Yeah, thanks. I wrote it in a way that reviewers rejected it like I started the discussion with going back to when Freud was actually the first one to do this imprinting because for this stimulation what I did, I do the stimulation in the striatum

and the dorsal lateral striatum, which is known to be responsible for habit learning.

And I did it in the slides by injecting in the sensory motor cortex, the optogenetics tools and in the migtula to represent basically sensory motor signal from some sort of movement and then represent an emotional cue labeling with the migtula.

And I did this with a bunch of brain regions and the highest upstate I could generate was like I used the thalamic input because the striatum is the basal ganglia is where like a lot of inputs that generate then a movement or stopping a movement gets

like all that information gets integrated and then sends out what we basically in the end do. Do we do a movement or we don't.

So that's why it's a very cool model to test those information integration type of things and turning it and see and outread and behavior. So I did that and then I did it in vivo like when the mice were digging for example I stimulated and I only really needed

like 10 minutes a day. And after five days, if I did that every day for 10 minutes, they would like compulsively dig or they would compulsively groom or, you know, different behaviors but it wouldn't affect other behaviors.

So I started the whole thing with, you know, Freud that said this already that this would be important for a long to have a memory and is to have some sort of emotional cue attached to it.

You know, the reviewers didn't like my whole and then heavy and and so on. They didn't like it. I have to change my discussion everything.

That was wonderful. Yeah, thank you so much, Katarina. I want to explain to everyone here in the group why Katarina's answer also really answers Dave's question.

So Dave's question, the way I would rephrase it, the way I think about it, which is it's not it's not a better way of rephrasing it but it just helps me to understand it. To think of it, Dave's question is like what is the granularity of the measurement of the neuron, of the energy of the neuron?

What is the, are we doing like coarse grained measurements or, you know, basically getting like very precise changes in energy and frequency and amplitude and so on.

I want to say that there we can add more to what was being detected in terms of like amplitude and we can go even in more detail back to Katarina in a second but the reason why Katarina said it was very was a great answer was because basically by explaining that, you know, earlier

saying that basically if you can listen to the neuron to the degree that you can figure out if it's about to fire, like if the dendritic spike has happened but not the action potential, or if NDMA spikes have happened but not the dendritic spike.

And so if you can listen, that's a very fine grain level of detail. What is the amplitude of the dendritic spike? That's also, that'd be interesting to know.

And so what I think what Katarina was saying was that she successfully stimulated a neuron almost to the point that it was going to fire so that it would, you know, maybe that means that, you know, if it fires in a regular basis like a tonic frequency anyways,

it's going to fire just a little bit faster because it's even more ready to fire. So when the next round of signals comes through, that is going to be part of, you know, basically.

But then the other thing that Katarina said was that in terms of, you know, the granularity of the encoding, when you're talking about, you know, like, first of all, eliciting a long-term memory with high frequency stimulation

and then generating compulsive behavior with high frequency stimulation. So basically making this, like, stimulating this neuron so they can basically, or micro, you know, microcircuit or microcolumn or cluster,

cluster of cells that represents a neural circuit, whatever Katarina stimulated and you're welcome to give us more detail on that in a second. That caused the animal to have a high frequency response that what resulted in generated compulsive behavior.

And then Katarina reversed it by using LTD, long-term depression, to issue a low frequency stimulation, I mean, to use low frequency stimulation to basically calm down the animal's behavior, to slow down the high frequency firing of that neuron or that microcluster of neurons.

And this is extremely profound because it speaks to exactly what Dave is saying about the granularity of measuring and stimulating, or the coarse-grained measurement and coarse-grained stimulation of energy at the neuronal level and the microcircuit level.

So I want to go ahead and hand it back to Katarina if you want, but I just wanted to share the meaning of that with the group.

Yeah, you explained it really well. Thank you so much. I have a hard time sometimes explaining it because I'm doing the work, so sometimes you're stuck in the subject, so it's harder for you to explain it.

Yeah, that's exactly what we are seeing, that the frequency of the stimulation, which type of input is stimulating these brain cells.

And in the slides, at least, which doesn't always mean it's 100% also in vivo system the same way, but at least in the slides, we can see also that the location of where we stimulate the dendritic arbor makes also a difference.

So we need way less synapses to be stimulated on the distal areas of the dendrite in the striatum.

And then if you get closer to the soma, then you need more synapses activated and that's because the dendrite, the dendritic arbor, the shaft, like not the spines, the arbor itself, the shaft, they are thinner,

but the spine size is still the same, so the impact of those spines being activated is bigger, so strong inputs, of course, are represented by being on more distal areas of the dendrite.

So I don't think we have to, it's cool to know, like to that type of resolution, and it's known that these type of subcategories of mapping of the neuron itself, it's supposed to be even more important in human neurons.

There was one paper, I have to find it, where they somehow were able to have for a short amount of time to keep a few actual cortical human brain cells alive and record from them and do some electrophysiology on them.

I know that paper really well. Yeah, so the importance of the dendritic arbor and the localization and the integration of information is way more complicated and more precise, even in neurons from rats and mice that we usually record from.

I'm not sure how much we need of that to, let's say, to evoke a memory again in a person. I don't think we need to be that precise for just recall of a memory, but I'm not sure, I can say, but that's my thought.

Sam, did you want to talk about that paper? Yeah, so I'm kind of in limbo with that lab. I've rotated with the lab that did that work.

We're still working out, potentially, he may be my co-advisor, but nonetheless, I know the paper very well. So the basic idea with that.

By the way, did you want to just say the title of it for the group? Yeah, let me get the exact title here.

Yeah, just give me a second. That way, you guys can look at the paper if you want. It has a great graphical abstract that maybe will help if anyone's trying to follow along.

So maybe while we Sam is doing, I'll just reset the group real quick.

We're talking about neurophysics and nerve care, but specifically how Numenta and Neuralink overlap. The big idea from the new book from Numenta talks about how cortical columns may be used to represent what you're seeing and hearing and what you're touching,

and how all of them are in different areas of cortex, but they're all sort of linked together, and so they'll vote, and then that will be how your brain can represent objects to you.

But the thing is, cortical columns are in reach of Neuralink and other brain-computer interface technologies, which is why we're talking about the potential for decoding our thoughts and what we can see in here.

So go ahead and pass it back to Sam. Yeah, so the title of the paper is Enhanced Endritic Compartmentalization in Human Cortical Neurons, and I'll say the reason the lab was going after these kinds of things,

but what they're thinking about here is neurons being much more complicated than the models that are being used in AI currently, with some exception.

Some people are already working on incorporating dendrites, but they're much more powerful that we could think about neurons much more like computers and the brain much more like the internet than the traditional model, which is like thinking about the brain as a CPU and neurons as transistors.

And this is very compatible with what Jeff Hawkins has put forth, not just because the attention to dendritic computation, but this notion that there's a bunch of computers that are connected in the brain instead of the brain being a computer.

So the basic idea with this paper is they wanted to compare across rats and humans, and just so we don't lose anybody, so we get a little bit of context here.

What we're talking about is these neurons embedded in the neocortex, and the neocortex has many layers, so you could think of kind of sheets of different types of neurons that are kind of stacked up in six layers.

And then cells they're looking at are primal neurons, they're output neurons, so they have very large somata that are in the deeper layers in layer five, and then they have a big dendritic trunk that comes off of the soma, and it reaches up through the layers and it branches, it looks like a big tree.

So the dendrites just branch and the neurons that they were looking at in this study are called tough dendrites, so they have a really big arborization in layer one near the top of the cortex.

So the first thing they wanted to do was compare these exact types of neurons in rats and in humans, and they do this really heroic patch clamp technique in the rat, and when I say heroic, I mean if there's a dozen labs that can do these kinds of measurements, I would be surprised.

And they take a very, very tiny electrode and they put it in the top, in the dendritic tree, right where the trunk starts to really start to branch out in layer one, and they put one in the soma, and so they're measuring somatic voltage and dendritic voltage at the same time.

And what they then do is they can inject currents in the dendrite and measure how strong that influences the soma and so forth, you know, the other direction.

And what they find is that the rats have some attenuation, so they compare layer two, three cells, which are smaller cells, the soma start in the mice, layer two, three are merged together compared to a human.

So these cells that are somewhere in layer two, three in the mouse are more, you get any injection that you do in the dendrite you see at the soma.

And then if you go to a layer five cell, which is longer and bigger, it attenuates, so you can blast the dendrite, but you won't see activity at the soma.

Okay, so then, you know, that's kind of like the main thing that they're looking at here. There's other stuff going on in the paper, but, and then with humans, what they did is they have a relationship with MGH nearby here.

I actually work at the hospital there, and there's some neurosurgeon, Sidney Cash, who will, has a relationship with Mark when they have a surgery planned.

Sometimes they'll have to remove some human cortical tissue and the way to doing something else in a patient.

And when they do remove that tissue, they immediately put it into a cooler and they rush over, they grab it, and they kind of proceed from there how you normally would with a patch clap, a slice electrophysiology experiment and an animal with that tissue.

And so they were able to do the same exact experiments in the human neurons and the rat neurons. And it turns out that the human neurons are much more compartmentalized than the rat neurons.

And then, so the question is, why, you know, why is that? You know, what is the underlying mechanism? And so what they did is they built some biophysical models that then tried two things.

So, you know, the first thing that you notice when you compare the rat neuron to the human neuron is the human neuron is much bigger.

So do we have a bigger neuron with the same exact spatial channel densities that you see in a mouse neuron? Or do we have the same amount of channels and it's stretched?

And so they built two models that way and it turns out that the stretched version matches the data much better.

So the idea is we have larger neurons with a kind of stretch distribution of channels that creates more compartmentalization.

And then if you're trying to go, you know, bring this back around to what we were talking about, the motivations for the study, the idea then is that, you know, beyond having more neurons in the human brain,

that potentially the real underlying power of the human brain compared to a rodent is that each neuron is so much more compartmentalized and so much more powerful computationally than individual neurons in the rat.

So that's like the main thing that the paper is getting at. And there's lots of other cool stuff about the paper, but that was the point I think Katrina was trying to make.

I am a little bit curious how a stretched out larger neuron translates to being more compartmentalized.

So we've been kind of alluding to all these different dendritic spikes. There's, you know, three different types of dendritic spikes that people talk about, plateau potentials from calcium channels, dendritic sodium spikes,

which come through voltage gated sodium channels and they look a little more like what you see at the Soma, but a little broader, not as, you know, way more narrow than the calcium spikes.

And then there's these NMDA receptor spikes that happen really distally and they're more compartmentalized.

But the reason these things happen is there's a spatial relationship between where these channels are embedded in the membrane and the morphology of the dendrite and where the currents are coming,

where these synapses are located relative to these channels. This is like the physics underlying how these spikes are generated given inputs.

And so, you know, you could just kind of imagine a tree with a bunch of buds all over it. You know, there's your synapses and then you can just imagine ion channels kind of embedded all over

and then just imagine what would happen if you stretch something like that. And there was more space, more passive filtering in between each channel as you stretch it

and you can start to maybe imagine how there would be more localized activity in the dendrite that's isolated from the Soma.

In other words, there's computations happening out on the tree that are independent of what the neurons output is.

Yeah, and so maybe it would take a lot, maybe because it's larger, the tree would require more water and would the analogy work that the neuron would require more electricity to fire?

Yeah, so you would want, if you wanted a huge human neuron to be exactly, functionally identical to the mouse neuron, which it's not, you would have to add more channels in.

You would have to maintain the channel density that the mouse has. But that's, you know, indirectly through these biophysical simulations,

they're arguing that it's more likely that the channel densities are lower in human neurons.

Okay, I'll just let anyone else who wants to go next, go ahead.

Hi, thanks for letting me on. Is it okay for me to talk, just ask a couple of questions. I find what you've been talking about absolutely fascinating and it's great to hear.

Go ahead, please.

Okay, so you guys have got all the tools in the box, you know, and I just wondered if you've looked into what was I'm saying just then,

our brains aren't like, I'm an amateur by the way, but our brains are not like a mouse brain, isn't it?

Has it got lots of unexplained things, you know, going on like perception, maybe, and lots of other things, what kind of perceptions,

or does it run on? I mean, surely you could play it music and it'd do certain things, you know, but romantic music might, I don't know,

do you know what I mean? There's other ways of invoking behaviour in animals and people.

But what about the part of the brain that hasn't been damaged? So part of the brain that's always, we've always had but we don't use,

which is quite a lot, I mean, you probably know, but what would that have been used for in the past, maybe going back into our ancestors,

you know, like sort of a long time ago, prehistory and things like that, what resonant frequencies would we have been picking up?

Have you seen any of that regenerated, or have you found a way to regenerate that?

I suppose if you don't know what you're looking for, then you wouldn't, but anyway, just that sort of question, is that okay?

And thanks for letting me talk. I'm obviously an amateur, but I'm fascinated by what you've been talking about and it does resonate.

Thank you.

So real quickly, I don't think there's any part of the brain that's not being used.

If there was part of the brain that's not being used, it would be taken over very quickly by some other part of the brain.

You know, for example, if you lose your eyesight, the visual cortex is quickly taken over by the other sensory input areas and others of the brain.

Any part that you lose is going to be taken over because it's like a competition.

The brain is like, each area is in competition for resources with the other areas, and vision kind of seems to dominate.

But yeah, I don't think it's plausible that any part of your brain is not in use.

Sorry, one of them was, okay, so it's got knowledge. I mean, you have to learn something, so you might pick it up. You have to, you know, if it's inherited or you've seen it, it's easy to replicate it and hence vision.

But what about things that the brain can pick up other than vision or a sense, you know, direct sensory ones?

And I'm talking about particular perceptions and things like that, or, I don't know, ancestral links, you know, they, I mean, okay then, let's look ahead, telepathy, things like that.

If we've got, everything's running on RF, there's so many frequencies being blasted around us in everyday life, including the phone.

It's a part of the brain that's been stimulated more by RF and by high static electricity.

And I'll show you, I've got some triggers as well for you guys with regards to frequencies that we're running on at quite a distance as well, and you can pick it up using,

using, you just basically got the oscilloscope and lots of copper cable really, but I was, I got a patent on something and it was a byproduct of it.

I'd like to, I'd just like to talk something about it and show you what I've got.

And it is to do with radio waves and us picking them up. We don't know we're picking them up, but we are.

That's why you're listening to me, so thank you.

Yeah, it'd be interesting to hear what you have to say on that.

I know there's some really interesting research currently being done around healing after brain damage.

So, you know, it's not being, brain can heal a lot when it's in a natural environment, rather than in an over-stimulated environment, which, you know, I think from a,

like the point of view of you talking about ancestry or, you know, I know how long-term past there's a possibility that we're not, you know,

evolved yet enough to sort of be able to integrate all the stimulation that we currently integrate.

And if we put ourselves back into like a more natural homeostasis of what our body and our brain is used to, then we function a lot better.

I don't know if that sort of touches on what you're saying.

Yeah, we're getting there. Thank you for that.

Sorry, I don't know if you jump on the stage and ruin everything, but I've just got some questions.

So, I'm with everything with respect as well, I think.

I want to try to rephrase, because I'm getting the idea of what you're talking about,

but I want to kind of try to sort of rephrase it to reconnect it to the rest of the story so far.

And I mean, so, you know, I've been hearing from Clubhouse that human beings broadcast an electromagnetic field,

and there's a guy who has a new device that he's talking about that can detect a virus on your finger

by blasting the virus with an electromagnetic wave, and then that interfaces with the RNA.

And because his sensors, his electrodes are arranged in an array,

he's doing something like tomography where he's figuring out the sort of the letter arrangement of the RNA.

And so, because he's able to detect the sequence of RNA with his array,

then he can identify viruses like COVID-19, and so that's what his patent is about, and that's what he's going to do.

But he basically came to one of the past meetups that I did, and he said that electromagnetic waves can affect DNA, RNA, viruses, and bacteria

just the same way that they can affect antennas, which is pretty interesting.

But I think that what was also mentioned was emotion.

So, this is kind of what I want to ask, it sort of ties back to our group here tonight.

And so, Katerina mentioned that, so Katerina does optogenetics, so she's a geneticist.

Sorry, we've just pulled through for a second, there was some bits and bobs, and I picked up on our fluorescence,

the thing with manipulating, there was a nuclei with magnetic fields that I played with.

Can you see the picture?

Oh, we had to pull down the refresh to see your picture.

Okay, so your picture is, would you say that was?

It was just bits and bobs, but the point is, the way they reacted to AMF and magnetic fields that I put in,

I've got some other pictures, I'll show you the experiments if it helps.

The thing is, all those organisms don't have a skull?

They won't.

Like, whatever bacteria or viruses, they don't have a skull, and also those papers that came up,

that 5G makes something called a brain, that those don't, it was shown that those high-frequency,

they don't go through the skull at all.

It's really, really high, actually, to get signals deep into the brain.

So I'm quite doubtful of that being, if that would be the case, we would be actually very happy,

and need to do all those brain surgeries, just blast things, and people would improve, like Parkinson's.

There's TMS with very, very high magnetic fields.

You can simulate brain regions, and there's a bunch of studies like that at MIT,

and others where you change plasticity, depending also on the frequency,

but you need really, really powerful magnets, and being really, really on your skull directly,

and then you can change plasticity in the brain with those.

Yeah, that's what I'm talking about, and just to put that into context,

that's sort of, we can stimulate maybe two centimetres with really quite high magnetic fields.

So it's miles above what you would be exposed to.

But I think what Micah is mentioning, the gentleman who was bitching this,

I was in the same room, I think, and he was saying that he can sense those viruses.

Micah, correct me if I'm wrong, he was not necessarily talking right on the skin,

he was talking in the tissue, in the blood, perhaps, or something like that.

So what's separating them is maybe the tissue, not necessarily the bone, right?

So in that particular instance, I believe he was saying that you basically put saliva on your finger,

and then put your finger on the device, and you'd have to have confidence.

Oh, I see, yeah, makes sense.

He did speculate about some wireless version of it that he could do in the future with increased amplitudes.

Okay, that's, I don't know.

Yeah, to add to it further, our lab actually looks at low intensity RTMS as well,

so not just really high intensity magnetic fields,

and we have shown in an animal model that low intensity magnetic fields can still induce a plasticity response.

However, this is not sort of activated through your usual LPD pathways,

because it's subthreshold, but this is the research that we're looking at,

but it's still high relative to the usual magnetic fields that you would be exposed to.

So I have a really big question, and I really want to ask Katarina this.

It's about emotion in the brain, so Katarina's work is relevant.

She talked about how she was able to create a negative memory and transfer it to another animal,

but then to change that negative memory to a positive memory?

Oh, that wasn't me. I was citing a paper.

I mean, you know, like the taking of PM.

So in Joseph Leduslaw, we erased memory, but the trends and we induced optogenetically also memory

without actually having some sort of foot shock or anything.

But the transfer, I just shared it on Twitter, it was done by another.

The transfer of memory, like relabeling it was done by another study.

I did the compulsive, which is also like labeling a regular behavior with an anxiety cue basically

and turning it into a compulsion.

But yeah, the relabeling was done by a different, but still you can still.

All right.

Katarina, was it that the paper in the hippocampus?

Yeah.

The only thing I would say is we're like the hippocampus on plasticity than we are in the cortex.

Is the tool is being used in the process because I know that Kenneth is doing the AI self-programming.

So it's a tool of detection and also the manipulation before the projection also integrate with the AI.

Kenneth is providing at this moment for your research and testing job and all this.

No, we're not working collaboratively on it.

Our work is independent.

It's been a 10-year project and this year, so we've had a number of patents that have been approved

that underpin this technology, essentially the first patent was on a mathematical approach to

essentially making the output signal better quality than the input signal.

That might sound like magic, but it is an approach that is beyond interpolation.

It literally is based on the ability to do a very accurate guided directional progression through the data.

So using quaternion math, four-dimensional math, that we can scale logarithmically and using what's called op codes

to determine the direction that we're progressing through the data allows us to work with very precisely,

which we're expressing in our third patent that's pending,

a level of detail control that is quite significant in its value with regard to, for example,

ability to very accurately analyze petabytes of data by only touching or progressing through megabytes or gigabytes of that data.

So a very efficient way to process data, but perhaps more importantly, it maps to language, both human and machine.

It can be mapped to signs, symbols, and what we call meaning units.

And I mentioned earlier for those who weren't in the call or in the room initially,

we've mapped this approach to what we call a semantic periodic chart for computing

that allows us to use 256 elements that are semantic representations of meaning that can map in both directions,

both to natural language or programming language or to signs and symbols, but also to code.

And thus, so you think of it as being able to pick your methodology, whether you are more in your effort

benefited by expressing it in natural language or a programming language or a symbolic representation of what you mean.

They all map to the same definitive meaning to the machine.

And the benefit of that is that you can support different ways of thinking, different ways of expressing different languages,

different cultures, and within that those elements.

It has certain specific representations that are not present in programming languages such as emotion, fear, anger.

The things that we use to define our world are represented as elements that can be combined to the equivalent of compounds

and very complex constructs, including for AI algorithm production to the GUI in an application to processing the data.

They're all treated the same way, so a uniform way of treating all of these ways of expressing what humans mean

and what machines can understand and process efficiently.

I'm done talking.

I asked this question from the perspective of exploring the tools that scientists use and researchers using.

So for example, because the day in the other room, Kenneth, I'm not able to ask you this question,

that you are presenting the idea that we can immediately having a thought on the brain and captured as a programming logic

and programming processes going into the AI system and to be implemented.

Coming from a computer science background in a conventional way of programming,

after the programmer having their code source code is being programmed,

there is a QC process to identify any flow or any bugs or logic problem that presented on the code itself.

So after you automate the process, where is the evaluation come in when the subject having a thought of programming

and then being translated into the AI and that bugs our whole loop that is not identified when it goes into a complex system.

Will that create chaos in the whole ecosystem itself?

That's a great question, Dave. So again, our third patent is focused on something we call level of detail.

In other words, what that essentially allows for is representing, even in the case as you described where someone's expressed an idea,

we semantically then map it to what we call a meaning unit.

Now it can be represented not just the way the person expressed it, the way the machine understands it.

In technical terms and in code, which gives you the opportunity to examine it to sort of validate it to say,

oh, this is, first of all, if it's not syntactically or mathematically or based on a rule set that you can determine,

such as a programming convention, it does not get expressed in the system as a viable solution.

In other words, it doesn't create the error in the first place, you know, along those lines, whether it's mathematical or otherwise.

But it also gives you the ability to expose those representations for examination and validation.

And once you validate, in other words, basically say, yeah, this is what I want to happen.

And then it goes through a process of essentially, as I've heard described earlier.

I think Katarina was referring to, or maybe Micah, the competitive process that the brain does to determine,

okay, what is the most efficient way to implement this in a given programming language, let's say assembly code,

but also how to order the data, how to essentially utilize the resources that are available now.

And so it's seeking the optimal instruction and basically flagging the winner as the instruction that's going to be executed.

And then deleting it along with all the losers because it's stored a metric to establish the baseline going forward.

So it doesn't go through this process of iterating all the time.

So in other words, it's more than about automating code.

It is about essentially doing the process that every programmer should do.

The programmer should look for an optimal instruction because we applied value to the effort of creating code

and we're limited by constraints such as time, money, and so on.

We determine a winning solution for those imperatives versus what is the optimal.

So in other words, it is a way of self-optimizing.

Think about it as machine learning being applied to the fullness of what computing can do in a massively parallelized environment.

If that makes sense.

Well, then I see we are limited to program at the API level because I'm seeing the global trend now.

The programmer today also doesn't have a chance to touch on the source and the root.

So if the original design of the API itself having limitation and we need a certain execution to have a total system upgrade.

And if the API level is limited that then we forever look into a process that not be able to excel from that current limit.

If that applies to your system for the AI direct programming in terms of understanding.

Yes, so this is a good question because we have spent quite literally years addressing that particular problem,

which is how to provide an interface that not only supports as some would readily conclude that we're talking mostly about allowing people to create code.

But no, we've created an approach that allows us to take things like APIs, drivers, all the other legacy components of a compute environment and bring them into a universal methodology for packaging them

so that they become like the digital equivalent of a Lego brick so that you can essentially not just utilize them more efficiently,

but you can swap them in and out of memory so that they're not occupying resources that are limited and we're managing the RAM or the memory for zero fragmentation ever.

So you're gaining levels of efficiency that also accommodate things like what a particular system requires, whatever that system is via its API.

They all coexist in an environment as if they're like we refer to them as powers.

So what is a power? A power is the ability to do X and then a skill is ability to apply that power in a customized, efficient way.

So I don't want to take up too much of the room with that conversation.

I'm trying to answer the question that is put to me, but I think what I want to emphasize as the most relevant takeaway to this room is the ability to apply some of these concepts of how the brain works to how machines are instructed

has significant value particularly to the degree that you're able to inform the machine on a very granular level, right, that this is what I want to happen.

In fact, we refer to that as wantware versus software.

Software being something that programmers write and maybe is the outcome of some very specific AI algorithm, its process particularly around learning,

but basically creating this, I think of it as a scale on one side, you've got the ability to express an idea which is uniquely human, right?

On the other side of the scale, you've got the machine's ability to execute the idea.

What's in the middle predominantly today is programming languages that are very limited, not precise mechanism for understanding what was meant, what's the underlying meanings that support the idea.

Well, I mean, you can think about it this way, but you can also think about programming languages as the ways to sort of like think about your idea more, right?

Like when you have your idea in your brain, it's half baked, right?

It's just really, you know, the programming languages are in a way the languages, right, with which you describe your idea in a more formalized fashion, right?

And then quality control, what Dave was mentioning, it's also a way to further refine your idea.

Like it's very rarely, unless it's a very simple and really useless idea that you can really formalize it in your brain ahead of writing the code.

You think about the problem as you write in the code, right?

So it's an iterative process inherently, right?

It's not like you know everything that you want to code ahead of time, it never is that way.

I don't disagree with you and we're not trying to solve that problem of can you codify your idea in a language that can be compiled?

What we're trying to address is predominantly there is no semantic or meaning representation in that code sufficiently.

For example, if you write code, not only would I have a challenge of understanding what you're thinking was,

what your mind or your thoughts were, which led to your decision around a particular line of code,

but you sort of fast forward a year or two years or three years and you might even have trouble understanding what you were thinking.

So you've got to go through a process of figuring out what you or another author of code meant.

A semantic approach allows us to capture that through the process and basically say this is what I mean to happen.

Now represent that in code and by the way, pick your programming language,

whether it's assembly or C or Java or Perl, whatever it is.

It matters less than what the idea truly was on a granular level, if that makes sense.

So I was kind of listening to see if this would somehow connect back to how we might decode the brain.

I think that maybe in the future there's a path to using your semantic modular coding concept for decoding brain activity

and maybe encoding robot brains or something.

That's another topic for another day for sure.

I have actually a question about the neurons getting back a little bit to the topic.

Okay, what's the question?

Yeah, so the question was, the point was that the neurons in the brain, they're not like the neurons in the computer-based neural network.

They're more like sort of computers on the internet or in the disaggregated system, which is a fascinating idea.

I've heard before a few times that saying that essentially the neuron is just on and off switch or a thing with a very simple mathematical function

is just tremendous insult to the neuron.

There is a lot more going on there.

And to be honest, the reason why we run neural networks the way we run them right now on the computers is because they're just mapped.

What we run map better to the GPUs, to GPUs that we have.

Bill Daly, who is I think chief scientist of NVIDIA, he puts it right.

Getting the inspiration from the brain is a lot like getting inspiration from the birds to create the planes.

If you think about at first when the planes were created, those architects, what they started to do, they started doing those ridiculous designs

where they're flapping the wings of the plane.

They tried to essentially imitate the birds and how they fly.

And it was very stupid because it really never went anywhere.

We have to think about what our technology allows us to do.

At first it was prop-based and then the jet-based.

And we have to completely redesign how we believe about flying.

And really the inspiration from the birds is on a very basic physics level.

We didn't really take a lot of inspiration from the birds.

So I'm very skeptical about this idea that we have to have the inspiration from the brain to create the neural network basis.

And we don't do it right now really.

The reason why the neural networks exploded in computer science and they work right now is not because people came up with the design of the neural network.

It was known for decades before.

The first designs came up back in the 70s or something like that and it went nowhere.

The reason why right now it works is because the computer architecture allows us to have a necessary parallelism

and necessary sort of horsepower to feed them with enough data.

But that aside, my question to Vizem or Vizem,

is when you're saying that there is a lot more going on inside the neuron,

can you give a little bit of a taste of what that something could be?

Like what kind of processes are happening in the neuron so that it does something more to its inputs

so that the output is sort of more complex or more non-obvious, right?

Something that we don't assume for our computer architecture these days, if you understand what I'm asking.

I think if I can just give a short thing.

So the thing is, though, the problem is we waste a lot of resources with flying how we are flying,

with the design of our airplanes.

And the same is right now with AIs and how we do computing in an artificial way

is we just need way too many resources to do the computing we would need to do

to recreate something that's human-like. It would be outrageous amount of resources,

so there's no way to scale it that way.

And I think it's not really to imitate more what the brain is doing

in order to solve specific intelligent problems.

We can do that if we stay specific.

But the problem is we need to scale down the resources we need

so the energy efficiency.

And for that, as we discussed yesterday in another room,

for that we need to imitate more to what the brain is doing.

So the neuron, it's just like integrating a lot of information

and then giving an input base on the integration of that information.

So the integration is different brain inputs, so different information sources

is integrating on a chemical level and electric level,

and then it generates base physically an output base on that very quite complicated

information input which comes from different chemicals,

it comes from different ion levels, and it changes based on that stream of information input,

how it then generates the output of information.

I think to use a more granular way of doing it,

which has enabled us to scale computation to a level that we would need to.

Thank you.

If I may, Katerina, you are spot on, and I'll be brief in saying

that that's exactly the approach that we've taken.

We're not trying to fully mirror what the brain does.

First of all, we don't know.

There's so much we don't know, but what we were able to find

is an approach that allows us to, on a very granular level,

determine what is the best and most efficient way to apply processing

to a particular level of detail that we want to expose,

and that process is proving to be very, very efficient.

We estimate, we have a plan to build the equivalent of the most powerful supercomputer

in the world that now takes 400 racks to do it in less than 100.

I think we're going to go on a little bit off topic again.

I wanted to hear from, we Sam was probably going to answer that, I think, as well.

I think that's a little bit off topic, that's all I'm thinking.

Yeah, so, yeah, this is something I can talk,

I can go on and on about, and I won't,

but just kind of put it in perspective, I guess.

I'm not saying we need to do bottom up,

the only way to go is bottom up,

and I'm not saying the only way to go is top down,

I'm actually saying we need to be doing both at the same time,

and we need to meet in the middle,

and there's a lot of ways to think about that,

but if you look historically at what's happened,

you had something like the perceptron pops up,

Minsky points out that it can't do XOR,

they add layers, non-linearities,

then you move down the line another AI winter,

then you go into backprop, which is completely non-biological,

so you're totally throwing out the credit assignment problem,

but you're getting great performance out of these things,

which is great, which is good,

and we're learning a lot about the brain doing that,

I'm not saying that, but is the brain doing backprop?

I actually think it's a terrible question,

I don't think that's the way we should be thinking about it,

and I said this last night,

my apologies if anybody has heard me say this before,

but the bottom line is 90% of neural tissue is dendrites,

90% of synapses are on distal parts of the dendrite,

which means they, in vivo,

the data doesn't support that backpropagating action potentials

can reach those synapses,

so there's obviously learning rules that we don't know,

we don't understand,

and now I can play devil's advocate,

and I can say we are light years away in AI from intelligence

in my view,

and just look at something like continual learning,

for example,

and then you start to think about,

oh, well there's all these possible computations

that could be happening in the dendrite,

a single neuron could be gating signals,

we know that synapses cluster on a specific branches,

so things like algorithms that have worked to a certain extent

for continual learning,

such as gating units at the front of a network,

maybe all of that's happening locally in a single neuron,

and maybe that's one of the big conceptual things we're missing

between the AI neuron models that we're using now

and neurons that are more like computers,

and I could go on with other ideas and examples,

some of them are more speculative than others,

but my point would be that we have a lot to learn

from doing physics in the brain still,

there's a lot of work to be done.

Sounds great.

I'd love to hear more about emotion in the brain,

specifically how we could,

if Nementa's theory is correct,

how that applies to emotion in cortical columns

and how we might detect them and change them

with Neuralink's technology.

Anyone want to respond?

It's like, what are the neural correlates for emotion?

I know that there are some studies in optogenetics.

Well, yeah, they're different.

They are basically neural secrets representing emotions

based on what type of neurons

and what neurotransmitters are being released

and the amount of neurotransmitters

compared to other neurotransmitters

that are being released represent emotions.

It's not like we solved emotions.

We have like an idea that,

but even in the make the law there are,

next to each other, positive,

negative emotions and negative emotions.

So it's not like one brain region is responsible for an emotion

and not one neurotransmitter is responsible for just one emotion.

It always depends on gradients of concentrations

and dopamine is supposed to be a rewarding one

and then norepinephrine can be anxiety and panic attack

neural modulator,

but it can also enhance memory to a certain level

depending on the amount.

So it's quite fine tuning that needs to be done.

But if we, like with optogenetic what was done

is just reconnect neurons to a different circuitry

so that the circuitry was not,

this memory was not connected anymore to a negative circuitry,

let's say,

but it was then reconnected to a positive labeling circuitry.

So with electric stimulation,

I think it would be harder.

So you need to be more,

I think you would need to be more precise than that

either by using optogenetics.

Another way to do it would be to use maybe RNAs

and epigenetic tools,

so they are drugs.

So epigenetics nowadays is another way

of how long-term memory is stored

is with epigenetic mechanisms

and you can modulate the intensity of those memories

or the labeling,

maybe even the labeling of those memories.

There was shown in serotonin and dopaminergic circuitry

that epigenetic mechanisms are going on,

so you can change by targeting those epigenetic mechanisms.

The labeling of emotions are the intensity of a memory.

I have started to use those,

so what I did was in cancer research,

we have way more advanced drugs

because there was always way more funding in cancer.

If you look it up,

it's a huge amount of difference

between cancer research and mental health research.

So what I did,

I took a very specific small compound

from those novel cancer drugs

and injected them while a habit,

a compulsive memory was forming

and an anxious memory was forming,

which led to habitual avoidance.

When I injected that drug,

I could specifically erase the habitual avoidance of a fear,

but the reaction to actual threats stayed intact.

So I don't think just by,

just to summarize,

I don't think just by electric stimulation patterns,

you can modulate the emotions.

You would need to use other tools,

just like epigenetic tools or optogenetic tools

or RNA, like silencing or expressing more RNA.

But that needs to be tested more.

Wonderful, thank you.

While you were talking,

I invited some more people to the group

and of course anyone can do that as well.

Does anyone have anything they want to say?

Go ahead and just mute myself.

Yeah, I want to clarify

because there is some confusion that I'm thinking.

Is our brain working in an off manner

or in an analog manner?

And if it's in an analog manner,

I'm expecting the reading of the data

will be a branch of a histogram

going into the computer for analyzing.

So if we have the behavior of the spike and the low,

can we modify the analog signal into the middle

and then reproject to the subject itself

to present another behavior?

So that is my question, Dave.

So we have both, like the code, as I imagine it,

or as I understand it and I see it,

is you have the code, like the basic code,

which is the DNA.

Then you have analog data represented

and reflected in different ways in the brain.

You have them as electric signal

and then you have them as changes in gene expression

which represent mostly epigenetic mechanisms.

So you have, on the DNA are formed either chemicals around

which represent methylation

and then you have histone modulations

and you have folding of DNA

which by having analog information input,

you change how the DNA will be read

and then you change that

and this will be then leading into a different protein expression

and this will also lead into electric signaling in the brain.

So you have that in different ways.

In other cell types, you just have the epigenetic mechanism

but not the electric one.

So if you have enough knowledge, you can reflect both.

You can, as I said, induce or reduce epigenetic mechanism

on specific targets.

You can change the protein levels by RNA,

different type of different RNA expression

and then you can, by stimulating directly

those neurons in specific patterns

also reflect that electric signaling.

My answer would be yes, you just need to know

how to, you know, you need to know

how this analog information was stored and reflected

in order to project it.

Thank you.

I just wanted to welcome Dr. Olu and Abyss to the stage.

If you each want to take a minute to say something, you may.

Sure. Thank you for bringing me up.

I was just catching up with what's been said

and it's really interesting.

I think if I get what Katerina, if I'm saying that properly,

was saying is that you have basically different modes

or different levels at which you can manipulate the system.

And so depending on what the outcome effects that you want to get,

you can decide at what level you want to intervene,

which is something that I think about in our own work

where we do a lot of work with neuroimaging

and look at things that were at a systems network level

without taking into account how things might be changing

at a neuronal level.

And so you can have all these changes happening

at the neuronal level that you're not even aware of.

But if I'm trying to understand a phenomenon like,

in my case, a disorder,

that we understand it as a network dysfunction,

we can actually fix it at the network level

at the desired outcomes that we want.

This is all that I'm done speaking.

And Abyss, if you want to say something,

and then I'll recap the room real quick.

Yeah, sure. Thanks for inviting me up

and it's been a really great discussion.

I was actually on a listening mode for the most part,

but I do have some questions for Katerina.

Do you think optogenetics will be applicable to human subjects?

Because right now, I think, I'm not sure

if there's like a non-human primates experiment

using optogenetics.

And if you should choose between DRED and optogenetics,

which one would you prefer for, say,

subcortical structures?

Thank you.

So I know from the conference that I was in Boston,

not the last fall because of the shutdown,

but the year before.

And if people want to sign up for it,

scientists, it started that year,

like the optogenetic engineering and application conference.

It's really good because a lot of really great people

that invented those tools and keep adding on amazing technology to it.

It's amazing conferences and it's still quite small,

so you can directly talk a lot and ask a lot of questions

with amazing people.

And there, they showed that they were starting clinical trials,

but for the visual system, like for blind people,

they expressed CHR2 and NPHR in different levels

to basically reflect shades of intensity,

of black and white and even color.

So I know, but that's really easy

because you don't need to do deep brain injections.

So I think at some point when genetic treatments

will be applied more generally,

I think it will one day be a thing,

but I think it will still take time.

So for now, it's just an amazing research tool.

Oh, chemogenetics, right.

Chemogenetics, I don't like them too much.

We have them in the lab, but mostly in rats

because we know really well the mice genome

and when we do anything to the mice genome,

we know quite well what will happen.

In rats, it's a little bit different.

There are not too many genetic tools in rats

and it's quite expensive.

You need more amounts and so on.

But you don't have the temporal resolution

and you don't have because you have to apply dreads

in a systemic way.

The dreads is you use a chemical systemically

and you only express the receptors in a specific brain region

that you want with the virus.

But as I said, you use it systemically

so you don't have the precise temporal resolution

so you can't switch on and off

and you're on a millisecond level.

So the dreads will be in there for about 90 minutes.

The time of the onset is about 15 minutes.

So we use those when we just very bluntly want to...

We use the inhibitory ones to take out parts

of the prefrontal cortex.

Let's say the IEL, it doesn't really matter,

like specific prefrontal cortex brain region

and we just want to take it out while an animal is retrieving

a memory or while habituation is going on

and we just want to see what the whole brain region...

If the whole brain region is not online, what happens to it?

Then there's new, it's bioluminescence,

which is better because you...

Bioluminescence is that the brain itself,

you also co-express a protein

that produces cold light, either blue,

to activate, and you also have inhibitory ones,

which has the pro that you can visualize

what you are activating while you're doing the experiments

and you have to do histochemistry after the whole experiment

and see if you ended up in the right spot

and do immuno, but with the bioluminescence,

you have the light source expressed

and you will visualize it right away,

which has a lot of purpose.

I wanted to sort of refresh the room

because a lot of new people joined us,

including Dr. Olu and many others,

to sort of explain how we got...

a little bit about how we got here.

So the title of the room, Nementa and Neuralink,

came about because there's a lot of overlap between the two.

The new book, A Thousand Brains, I read it.

I also read about the...

basically, I'd say I read this in actually 2017,

so the main idea, which is about the cortical columns,

are organizing all the information that you're perceiving.

If you have a table, every object on the table

would be managed by a different cortical column.

But not only that,

if you have your phone, for example,

it's a great example because we're all on Clubhouse,

so if you have your phone, you're going to have

maybe a thousand cortical columns in the digital cortex

that are modeling your phone.

I confirmed with Jeff Hawkins, I asked him this,

so you'd also have a thousand cortical columns

in your audio cortex and your somatosensory cortex,

and he said, yes, exactly.

He's not saying that a neural column is multimodal,

but he's saying that these columns have to model

the phone in the digital cortex and the audio cortex

and then in the somatosensory cortex

and then vote on which representation is the most accurate

or the most correct, and then, I guess, arguably,

they're going to be modeled by fewer cortical columns after that,

but you're still going to have this sort of link

between the primary input cortices

for what you hear and what you see and what you feel.

But that is directly relevant to neural link

because neural link goes,

they're sticking 16 electrodes per each one of these strands,

which is going into the cortex,

so you're going to be able to get,

not only are they doing, like, you know,

there's 16 electrodes in one strand,

but they're putting in many hundreds of strands simultaneously,

so you end up with, you know, whatever the number is,

3,000 something electrodes in there at one time,

and if you are trying to figure out what fired and when,

you're going to want to have a lot of electrodes,

so you can do, you know, like, if you just have three electrodes,

the analogy is that if you just have three electrodes,

and imagine you have three microphones in a dark room,

and you're trying to figure out where a sound came from,

which the sound represents the spike in this case,

you might figure out which, whichever microphone was the loudest,

might give you an indication of where that sound came from,

but, you know, with a neuron, it can spike in multiple places,

and so if you get a sound in another place,

if you only had three microphones, you might think,

it's a different neuron, it could be the same one,

or if you get two sound spikes in the same,

or electrical spikes in the same area,

you might think it's just one neuron,

but it could be two because they're just close together,

so with many different electrodes,

you can get a high-resolution image,

and you're much more likely to figure out, you know,

if there's really two neurons that are close together,

or if one neuron is spiking in two different places,

and you can think about doing things like,

okay, well, can I detect if the neuron is about to fire,

if it's just like maybe like a dendritic spike,

or a DNA spike, you can increase your granularity

with increased resolution.

That's exciting, but basically I asked the folks at Nementa,

I said, do you think that we're going to be decoding?

I asked them yesterday on a clubhouse,

I talked to Subitai, and I said,

do you think we're going to be decoding,

decoding basically cortical columns,

the cortical columns that represent the cup in the visual cortex

and they represent the cup in the audio cortex

and represent the feel of the cup in the somatosensory cortex,

and he said yes, and the implications of that are profound

because basically that means, you know,

if an ear link is a device that's going to be enough

to potentially decode what we're seeing and hearing

and feeling, and potentially also what we're thinking

in the prefrontal cortex,

and also our emotions as Katarina was just explaining

right before Dr. Oler joined the room and many others.

So that sort of like recaps the thing,

and obviously the nerve gear is closer to brain-computer interfaces

and what Neuralink is doing and neurophysics.

We're talking about brain waves and oscillations

and brain activity and cortical column activity

and the networks of the brain,

but specifically the networks of cortical columns

and the network protocols in what is voting,

you know, when different cortical columns agree.

So these are all like related topics.

And so I'll go ahead and mute myself

and then let people say whatever they want.

Go ahead.

So I had a question about sort of the amount of real estate occupied

by something like the Neuralink electrode,

like how big a patch of cortex would be covered by their current setup?

So, okay, so each electrode is about the width of a hair,

but then depending on the density of neurons in a given area,

that each electrode could potentially detect

as many as a thousand neurons in a radius.

I think that's correct.

And it's like 140 micrometers or something like that,

but I may have to look that up,

but sometimes I forget.

Is it micrometers? It might be,

but it's like the distance from the electrode

to what it could potentially measure.

Katerina might have an answer for that, actually.

How many in Neuralink?

I'm not sure right now.

I have to look it up.

There's a lot of debate about how big a cortical column is,

which you talked about and Jeff Hawkins talks about.

And he kind of formalized it as like a cubic millimeter of tissue.

And I think I don't know any details about Neuralink,

but if you're talking about distances from electrodes,

then I think microns is pretty appropriate.

Yeah, but what the dimensions of their arrays,

of the microarrays, I don't know right now.

Do they have it published somewhere?

A thousand channels, but I don't know what the dimensions are.

When they came out with the first, I think it was two years ago,

when Neuralink first came out with all the details of it,

they did publish the information.

I'm just forgetting what it was offhand.

Yeah, one of the reasons why I ask is,

I sort of mentioned networks before,

and I think it's important to think about things

from a network perspective.

So I'm a psychiatrist,

and when looking at the literature on neuromodulation and psychiatry,

people were very focused on sort of what the target was of the electrode,

and where the target was connected to.

And they actually found that if you looked at the variability

in clinical response, it wasn't because of problems

with locating a particular DBS electrode

in a particular part of the subgenual singlet

or part of the striatum,

but it was actually the either structural connectivity

to where you place the electrode

or the functional connectivity that was really important.

And so I think in any of these strategies,

I think a good sort of mapping of all of the connectivity

and the network characteristics of the brain

are crucial to a brain target.

So that's the comment I wanted to make. Thanks.

Just a curious question.

If I come from the perspective of the ancient wisdom

that I always think they are much more advanced than us,

I'm not sure you're familiar with the Chinese acupuncture.

They have a whole diagram of the point

to put the needle into the body,

which I suspect is a needle of capturing metrics

and also the histogram of all this brain activity.

Are we at the current scientific now also capturing

the input data from a human being from that kind of diagram

that the ancient Chinese already have for their acupuncture point?

That's a really good question.

There may be studies, but they're not that I'm aware of.

Hang on. Someone's coming up.

Let me see.

So maybe someone has a response to that.

Hello, Hanson.

Hey, Mika. How are you doing?

Pretty good. Welcome.

I actually have a question.

Maybe really naive. You probably have just came in,

so you probably have covered it a long time ago,

but maybe I'll just ask my question

and then you can see if it is relevant.

So the electrodes that are planted,

I suppose it's implanted into the brain, is that correct?

The electrodes on the patch.

For Neurolink, yeah, they're implanted into the brain, yes.

Okay. So I suppose it's not exactly inserted into the neurons,

and if it is not, when the neuron fires,

as I understand it, the electric potential

transmits through the neurons or exons,

I'm just wondering if the electrode is away from the neuron,

how does it receive the signal?

Is it through, let's say, the coulomb, the static potential,

or is there any, is it the change,

the perturbation in the charge field

that the electrode is receiving?

Or is there a current going through the medium in the brain tissue

and the current going from the neuron to the electrode?

If it is, it seems a little bit strange

because that means the current is leaking from the neuron.

So how does the electrode detect the neuronal activity?

Thank you for asking the question.

Okay, I could answer that,

but we've got really capable people on the stage.

Anyone else want to take a stab at that?

I guess it's me.

So it's an electrode, right?

The idea with Neuralink is there's a microsurgery robot,

and the only thing that the microsurgery robot is doing

is trying to implant the electrodes in between blood vessels

to create the least amount of damage when you implant them.

And hopefully the electrodes end up near neurons.

I don't know if they're trying to avoid puncturing neurons.

Yeah, so that is one particular detail that I don't know,

but the idea is that they want it to be close enough to the neuron

to measure the actual spike.

And the actual spike, you have to be really close to measure that.

And I mean the electrical spike, the electricity.

So yeah, so the neuron is when it is leaking electricity

when the action potential fires.

The electrons are sort of, I guess it's like lightning in a way.

You have an action potential spike,

and a lightning strike in both situations,

there was a separation of positive and negative charges.

In the case of the body, the positive and negative charges

are inside the electrical gradient of a neuron,

and we're talking about the sodium ions and the potassium ions.

But in terms of the lightning strike,

of course it's positive and negative charges in the sky, right?

But in either case, the neuron has to reach equilibrium

in order to maintain its equilibrium,

because it's a dissipative system, it's a vortex.

And so it has to fire, and it has to fire off its electrical charge

along the axon body, but it is going to leak.

And so we're going to measure that.

I believe that's correct, but again,

we've got extremely qualified people up here who can correct me

if my understanding is not right.

So usually you record potentials, like field potentials,

and then you can analyze them by the hertz of field potentials you have,

and you will have depending on the neuron type

and depending on the stimulation type and so on,

you will have those field potentials in different frequencies,

which is expressed in hertz.

So yeah, field potentials usually.

So I'm confused though.

I don't know anything about the neural link,

but is it measuring LFPs or is it detecting spikes?

Well, from what I can understand,

that you can implement this simple machine learning algorithm

to determine the spikes.

But I would assume it's LFP, but I could be mistaken.

But I've seen like LFPs can essentially be broken down

to like a very simplistic spike sorting

and using some kind of like component analysis

that you can determine individual spikes.

I mean, if we're deconvolving on LFP signal into spikes,

that's going to be rough.

I mean, like traditionally with touch roads,

you're measuring spikes directly and then you do a spike sort from spikes.

Can you explain what is LFP, the acronym?

It's local field potential.

Local field potential.

They're transient electrical signals generated in the nervous system.

It's basically what electroencephalogram means,

even though EG refers to scalp electrodes,

but yeah, electrical signals generated in the nervous system

and other tissues.

I mean, they could be recording units, then you would know

that you would be able to analyze the exact spike

and inter-spike interval and so on.

But they specifically said they wanted to be close enough to a neuron

to measure an actual spike.

But I imagine for everything that they're not close to,

the thousand neurons that they're not close to talking about local field potentials.

That sounds more like a tetrod to me.

Yeah, I think it's more like a tetrod

because that would be the reason to have 16 electrodes in each hair

and put many of them in there.

It's using like a tetrod to triangulate the locations of spikes.

Yeah, so to narrow it down at volts, they measure volts.

It doesn't matter if it's field or a unit, it's involved.

If that answers your question, Tom.

If that's the case, I suppose it is measuring the potential,

the perturbation from the...

So basically, electrostatic potential from the perturbation

resulting from the spikes instead of the current

that is the iron, let's say the potassium

or the calcium iron, the current that goes along the neuron.

Is that correct?

Yeah, in order to measure currents, you need to be in the neuron,

which you can do in vivo.

I don't think that those errors are doing that.

You can, you know, petticell in vivo,

but I mostly do it in vitro, then you would measure directly currents

and then you can detangle, you know, the different receptors

that are contributing to those currents.

Or very near the exon,

because you do have current outside the iron current outside,

just immediately outside the neuron, correct?

If it is not, then I suppose it is measuring the potential difference,

just the electrical, I mean the electrostatic potential

caused by the spikes of the iron.

Well, the exon releases then, because of the spike,

a neurotransmitter is usually, I mean, you have electric...

you have electric sinuses, but those are not...

I don't know about the human brain, actually,

but those are not as common.

Usually a spike emits a neurotransmitter,

leads to neurotransmitter release,

which then goes to the next cell, activates receptors there,

that opens iron channels,

and it leads to the next action potential,

which is again, neurotransmitter release.

That's more often the case.

You have electric information transmission to get junctions mostly,

but those are not as common.

So basically, the conclusion is,

I suppose the electrode is far away from the neuron

that it cannot measure the current,

the electric potential that's caused by the excitation of the neuron,

and which caused the change in the iron concentration.

Is that correct?

Like, just...

Yeah, I mean, it's not just location, too.

If you want to measure currents in a cell, you want to...

There's a few ways to interact with the membrane,

but the simplest one to think about

is your pipette fluid is literally continuous with the neuron,

and then you can start to measure currents directly from the cell.

And I think maybe this is what you're driving at.

If you're measuring spikes relative to currents,

relative to synaptic currents,

that's a very low-dimensional signal,

like the actual current inputs into a given cell,

or even populations of cells

that's even higher-dimensional.

And, yeah, I couldn't imagine

that the device is measuring something like that.

So I'm pretty sure they're just...

Yeah, they're extracellular recordings,

and I think they can measure probably...

If they're similar to other electrodes,

about 100 microns from the electrode,

and put that into perspective,

the cell, the neuron, could be anywhere from 10 to 30 microns

in size itself.

Okay, so the conclusion is just the potential they're measuring,

the static potential.

I would...

I mean, at the root of it, they're measuring voltage, right?

Right, so I'm just wondering if the voltage...

Yeah, because I was wondering whether it's current,

measuring current directly, or the voltage

from the electrode's static potential.

So it is the voltage, right?

I kind of had a question

that will pull everyone in a different direction,

if you don't mind, but it's kind of...

I think it's a fun one.

And so it goes back to...

So I was at the grocery store, and I had this idea,

because I was listening to Catarina the other day

in another talk where we had some other great speakers.

And so the idea was...

So Catarina was just saying that earlier,

before many people came,

that she was...

she managed to stimulate a mouse

to basically...

she elicited long-term memory with high-frequency stimulation

and then generated a compulsive behavior

with high-frequency stimulation,

and the mouse had, like, impulsive behavior.

And then she used low-frequency stimulation

to induce LDT and long-term depression,

and that caused the animal's compulsive behavior to go away.

So that's an upcoming paper that she's working on.

It sounds really exciting.

What I was thinking about a lot was,

well, what is the role of high-frequency stimulation

and low-frequency stimulation?

I don't know if I should be saying stimulation,

but the idea is, like, you know, if someone has...

if someone is too impulsive and they interrupt too much,

if they have attention deficit disorder,

if they have less self-control,

you know, like, so I get...

Oh, yeah, so I think it was Catarina and maybe it was someone else,

but I think it was Catarina who was talking about

the inhibitory neuron system,

or the inhibitory neuron is basically

helping to fill a role of inhibition,

whereas, like, if we didn't have inhibition,

then we would just be, you know,

either we would be epileptic or we'd just be out of control

and we wouldn't be able to, you know...

So the idea is that inhibition is what gives a person choice.

It's what gives a person... not choice,

but the feeling of agency...

inhibition gives us agency.

It makes... instead of being like this, like,

floundering fish with the inhibitory networks,

we have a sense of control

that we might not have otherwise over ourselves.

And so people who have less self-control

who interrupt too much,

maybe their interneuron system is damaged

and, you know, and maybe people who are ADHD

have a dysfunctional interneuron system

or inhibitory system

and then maybe we could solve that

if we gave people low-frequency stimulation

to induce LTD in those areas that were really...

you know, they had too much stimulation.

So, like, imagine, like, you know,

there's too much stimulation in the part of your brain

and that causes reduced blood flow

in another part of your brain,

like the reduced blood flow that we see

in people who have ADHD, for example.

But, I mean, this could apply to a lot of different illnesses.

I don't want to just, like, stick it to ADD, but...

Go ahead.

Yeah, there's a lot of research being done in that direction.

And actually, those magnetic stimulation devices,

there are clinical trials going on,

especially in kids that have autism.

It is known that the inhibitory system is disrupted.

Also, in the prefrontal cortex.

And that's exactly what they are trying to do.

And what is interesting is that most likely,

a lot of research points towards that glia

in those brain structures,

that a malfunction in the glia

actually leads to a malfunction

in the inhibitory immune system,

which then links it to the immune system.

And there was a very cool paper from the lab.

I forgot the name right now, because it's really, like...

that found the immune system in the brain,

because, like, before we always said

the brain doesn't have its own immune system.

And they found that a dysregulation

of the immune system in the brain

can lead to such complicated behaviors

like social anxiety,

which then explains a lot about autism disorder.

So, yeah, it's a really cool thought.

And that's the work going on towards that direction.

The type of frequency versus low frequency stimulus,

are you talking about the electric stimulus

or some other stimulus?

I use optogenetic stimulation for that

to be more precise on, first of all,

which inputs I'm stimulating

and which neuron types I'm stimulating

and where and to have a temporal resolution,

because I needed to stimulate

those specific inputs onto the neurons

only when the mice were performing a specific test.

So the stimulant, did you say optimal, optimal,

or is it the photon?

The light, so we induced a gene

that produces a channel

that is usually found in algae.

The algae used this to detect blue light

so they know where they're led

and where they're screwed, basically.

And we used those receptors

and expressed them in neurons

by using genetic tools.

We can reduce them in viruses

and we can select in which neurons it will express

or which cell types it will express

and which it won't,

and by injecting it precisely into a brain region,

we can be precise with which brain region

and which neuron type should express those

and then we implant a light source.

You would usually have optic fiber cables in there

and then we hook it up to a laser

and then we can millisecond precisely stimulate

those neurons and also inhibit the neurons

in that precise way, so that made it possible

for me to just stimulate a specific input

onto that brain region and that neuron

while a specific behavior was going on.

Sophie.

I was just going to say it's really interesting

that the same paradigm also applies

in a less precise way with clinical RTMS

where 10 hertz high frequency stimulation

is supposed to increase activity

of the dorsolateral prefrontal cortex,

but if you stimulate at 1 hertz,

which is relatively low frequency,

it actually inhibits.

And what people do is they'll use

high frequency on the left hemisphere

and low frequency on the right hemisphere

because of somewhat dated ideas

there's some laterality in depression,

pathology such that you have under activity

of the left hemisphere and over activity

of the right hemisphere.

So it's really interesting to see the same

sort of pattern of high frequency, low frequency

stimulation of sort of being sort of recapitulated

at a larger scale.

Yeah, interestingly to add to that,

it can't be individualized particularly easily,

so we're all more complicated than we realize

and you can do a lot of work finding out

and tracking and mapping these inhibitory pathways

and excitatory pathways

because obviously if you stimulate with high frequency

and inhibitory pathway,

you're actually then inhibiting that part of the brain

even though you're using excitatory stimulation.

It's actually really, really complex,

but then it's also hard to be generalized

across a population.

And a lot of this stuff I think is quite outdated,

some of these ideas of stimulation parameters,

but as a general rule of thumb,

you've got 1 hertz is inhibitory

and 10 hertz is auditory.

Then there's extra layers I guess to add to that.

So the light, the frequency you're referring to

is the pulse frequency, is that right?

Yeah, exactly.

So the light, the blue light opens up positive ion channels,

which then depolarizes the cell

and then generates an action potential.

And then we have the opposite effect

if one puts on, that puts in the cell,

in the cell haloverid options with yellow light,

it activates a chloride pump,

so it hyperpholarizes the cell

which makes the cell be more inhibited.

There are also labs that do that

with the inhibitory neurons,

like they activate the inhibitory neurons

which activates the neurons,

but leads to more inhibition in the secretory.

And so on.

Yeah, I guess the color of the light

corresponds to the excitational energy

of the, let's say, calcium or potassium.

It's options, it's options are receptors

that actually get aid from algae and other organisms.

I would like to give Leonard and Jennifer

each a minute to speak, if you want to say anything.

Yeah, thanks, Micah.

I'm barbecuing right now, I just popped in

and came on stage to add some value later

versus just listening.

I want to maybe have you reset the room.

I've been listening for a couple of hours back and forth.

It is great to tie the mind to AI

and to what we're all collectively building,

specifically my company,

Machine Intelligence Neural Network Database.

We're more focused on decision theory

and probability theory and economics,

but I love the mental health side of it

and chemical neurons and my CTO

has worked on human-computer interaction

with most technical universities.

He's a senior-tenure professor there,

but I just want to add value and thanks, Micah,

but if we can, I'm just following the thread

and it looks like we're more on the more medical side,

but I think at the beginning of the room

we were more on the neural link side

and what are the probabilities and possibilities.

Sure, let me link it all together.

So Nementa has these really great ideas

about how cortical columns are helping to organize

the sensory modalities of information

in our brain.

I'm going to make this description a little bit shorter

than I did last time,

but basically the idea is that you have

cortical columns in your visual cortex

and your audio cortex,

the amount of sensory cortex

that are each representing what you see,

what you hear and what you feel

in terms of like they have to talk to each other

so that they can integrate this information

to make predictions about your phone

that you're holding right now,

how you're holding it, where your fingers are,

moving on the phone.

In addition to that,

you're going to have basically the prefrontal cortex

and other areas of your brain

are going to be creating representations of your ideas

and how your ideas are connecting to concepts

that are modeled with the visual cortex

and somatosensory cortex and so on.

And there's a process of voting

which reduces the number of cortical columns

that are going to be involved in this modeling,

but basically everything in the room with you right now,

every object on the table with you

is going to be basically modeled by cortical columns.

And so because it's true with a device like the Neuralink

and with its many electrodes that are sort of like,

I think of the reason you have this many electrodes,

ridiculous many electrodes,

is you're going to be doing basically triangulation,

sort of like a tetrod,

where you're going to try to figure out a high definition,

but much better than a tetrod,

you're going to try to figure out a high definition model

of what's firing and where it's firing

that goes beyond what you can do with a tetrod.

And so to bring that back to the medical topic right now,

so Neuralink has proposed

that they're going to be able to treat basically

every sort of mental illness.

Yeah, they came up with that.

And so the question is,

well, how when they're actually nitrous joined in,

I wonder if I should reset it one more time.

So invite to speak.

But yeah, so the idea is that Nementa is saying

that with the cortical columns,

we're modeling everything in the room.

So you've got the visual cortex,

you've got the audit cortex,

you've got the somatosensory cortex,

you've got the prefrontal cortex.

And there's cortical columns in each area

that are talking to each other

to predict anything that you're holding,

including your phone,

how you're rotating a coffee cup,

how you're rotating it,

where your fingers are on that phone,

what it looks like,

what it sounds like,

if you bang the phone on the coffee cup against something.

And then over time,

they're going to reduce their representation.

So they're sequence learning,

they're sparsity,

they're distributive representation.

These are the main ideas of Nementa.

But the big implication of that

is that with devices,

with these, I call them super tetrods,

the Neuralink device,

because you're going to do really advanced modeling

of what neuron fired and when,

and do advanced decoding of cortical columns.

And I asked Subitai from Neuralink yesterday in Clubhouse.

I said, can you basically, you know,

decode cortical columns

and figure out when cortical columns

are talking to other cortical columns,

when a vote has happened between cortical columns.

And also, he basically said yes,

this is possible.

And there's already examples,

when I pressed him on it further,

he said there's already examples of, you know,

if you can, if you hear something with the visual cortex,

it's going to,

it might cause you to think of some animal.

Like if it's a loud cat,

if you hear a loud cat,

you might, you might,

you might see in your imagination

some apuma or a cougar,

or some other like really angry cat,

you know, it could be in California,

or it might be a little bit different.

But the idea is that

you're going to be able to,

sort of like if you can deconstruct

the network protocols of cortical columns,

and you can decode the cortical columns with Neuralink,

then you can begin to pull,

you know, basically predict what images are happening.

And then we were talking to Catarina,

and Catarina was explaining,

well, she's already with mice,

she's already like decoding basic things

in micro-cordical columns,

you know, such as a fear response,

she's able to, with great precision,

to make a mouse less afraid,

or to, I'm sorry,

might be mixing that up with a different,

so that was, okay,

the story that Catarina was talking about

was about causing, increasing the,

it was, it was about,

that was about increasing the,

here it is, I think I took a note.

Basically, so there's a story

of transferring memories from one animal to another,

which is basically like in optogenetics,

which is an example of like

if you can capture a memory from one cord of calm,

you could transfer it to another,

but there's also, so that's been done,

and with Neuralink,

we'll be able to do that at a much higher resolution.

And then to get to basically,

oh, so in terms of the granularity with Neuralink,

or with optogenetics,

potentially, like what Catarina did was,

she listed the long-term memory

with high frequency stimulation,

generated compulsive behavior

with high frequency stimulation in a mouse.

And then, so maybe,

I don't know if that means like the animal's hand

was moving compulsively or something,

like its arm, but then with low frequency stimulation,

the long-term depression was induced,

and the animal's limb stopped moving.

And then, so then I've been thinking about this idea,

well, what if, for a couple of days, actually,

what if the inhibitory interneuron system

that Catarina was talking about yesterday,

what if that dysfunction is sort of like responsible

for things like attention deficit disorder,

and it turns out that was a good guess

because there's a lot of research in that area.

There is, you know,

Catarina brought up that the inhibitory neuron system

might be damaged in the case of Alzheimer's,

and then, no, was it Alzheimer's?

And then also, I think the one that came up

was autism and nervousness,

if that was another thing that came up,

which is, these are all very interesting things

that, so basically, talking about like,

okay, well, would a person gain more agency

if they had more low frequency stimulation,

basically inducing more long-term,

I'm sorry, more inhibition,

because the idea is that inhibition

might be the thing that gives us more agency,

and in some cases, if it's too much excitement,

then you're just, I guess, flopping around like a fish,

you don't have agency, you don't have self-control,

or not as much if you have less agency.

And so, bring this back to, yeah,

so bring this back to medicine,

it's like, okay, well, Neuralink proposed

that they're going to be able to treat any kind of dysfunction,

maybe they're talking about doing things,

like increasing the frequencies,

or decreasing frequencies,

to treat various illnesses,

and that might relate to autism and ADHD and others,

but that also sort of like,

what are the other things we can think about

in terms of treatments?

So immunology has been sort of in the background a little bit,

I meant, of this discussion,

Katerina brought immunology

because that's directly related to potential cause

for the glial cells being damaged,

which led to the dysfunction of the internal system,

so the immune system was compromised,

it disrupted the internal system,

and so inhibition was decreased,

people had, the subjects had more,

the glia was destroyed,

and the subjects had more social anxiety,

more autism, just sort of like rephrasing

what Katerina kind of said,

and so let's talk about immunology for a second also,

Katerina worked on cancer,

I know that CAR T cells and treatments,

or immunology treatments with cancer came up,

but that sort of recaps the room

for the way Leonard asked,

Nick and Carly to chance to speak as well.

Thanks so much, Meika.

Yeah, and thank you for all that background,

that was really helpful, I just dropped in,

so it's really helpful to get up to speed a little bit.

And I'm kind of coming in here from the angle of being curious

about neural link and where that is headed,

and also exploring this from the point of view

of actually autism as well,

which is wonderful that you just brought that up.

I'm really curious where we're at with respect to,

like of course building out the neuronal model

and being able to predict these things

and read and interpret is a huge project

in and of itself and very fascinating.

I'm incredibly curious about kind of the writing back mechanism

or the inducing certain states,

and the fact that you got to inhibition is exactly on my mind.

And here in this area, like for autism,

yes, nervousness, but in general,

like I'm curious the degree of like sensorial inhibition

or even amping up in certain cases

where there's hypoactivity of different inputs,

different sensory inputs,

if there had been any research in this area

or much talk around it.

And so I'm just excited to be here

and hear a little bit about,

like I'd love to hear what,

if there's a paper or anything you could point me to

that is already looking into that,

it sounds like this has been a topic.

So yeah, I don't know if I have too much to add necessarily

from my angle of the world.

I'm not directly involved in this quite so much.

My research with autism has been more in how to improve

like digital check communication channels

for those who are autistic.

But I'm very happy to be here

and it's a great topic.

So thank you.

And I will add quickly, I will have to step out

unfortunately too soon from this conversation as well.

I have a few other things going on tonight,

but yes, it will be here listening

for at least another 10 minutes or so

and very happy to be here.

Thank you.

Nick, you're next.

Yeah, I also jumped in kind of late

and apologies at first too.

I don't have to jump off really.

I'm going to give a seminar a week,

so I got to give a talk on Wednesday.

So I'm working on that.

I think that the cool thing,

obviously Merrillink has paved the way

at least for a lot of the innovations

for many of the tools within that space.

But the thing that I'm curious about

and that I don't know much about

is more of the learning process

that has to go on with the implantable devices themselves.

So for example, you brought up the example of the cat.

Like if you take a look at a cat

and how you can sort of map the associations

that occur within the cellular patterns

that give you that visual representation of the cat,

how that is going to occur

from one person to the next

is not necessarily going to be identically,

you know what I mean?

So when you start to,

let's say you put the Merrillink in

and you map the person's cellular firing patterns

that associate with them staring at a cat

and you take that exact same firing patterns

and you stimulate that in another person,

it would be hard-pressed to believe

that you're going to give the exact same thing

of the visual representation of a cat

and that other person,

because there's a whole host of factors

that have gone into whatever that person associates

with being a cat, you know what I mean?

And so I'm curious as to,

not only like the inter-individual differences

that are going to occur and sort of the ability

in order for each of these machines

to be widespread across in the public,

but also how each of the different perceptions

or whatever it might be

shapes over time within a given individual.

You know, like how you perceive a cat

when you're five years old and it's a slight fluffy thing

is going to be one certain neural firing pattern

within the cortex,

but if suddenly at age seven you got attacked by a cat

and now you have a fear response that's added on top of that,

is that going to be,

affect the neural link input or output, if you will,

from its, you know, the stimulus is the same,

but now the associations and the firing patterns

that are going on for the same stimulus

are very much different.

And so I'm curious if anyone knows anything about

the sort of learning process that has to go on

within the machine itself

and the adaptability to change over time.

That's a great question, Nick.

It makes me think of just the treatment of PTSD

in the space in general.

Yeah, and the reason I was thinking about it

sort of along the lines is like,

let's say for, you know, drug sensitivity,

you know, if you give a patient one drug,

you know, they are going to have a certain response

and you're giving a patient the same drug

under a different condition,

they're going to have a very much different response,

and that's what I'd known in the physiological world,

and it gives rise to many of the variances that we see,

but when it comes to implementing across people,

I think it creates sort of a big problem,

because all of a sudden you have this fantastic,

beautiful model that you've modeled on three people

with this neural link implantable device,

and all of a sudden you take that and you put it

into another person, and instead of seeing cats,

they're seeing, you know, zombies and goblins

running around, you know.

So it's an interesting thing.

I think I would agree, because, I don't know

if you know the work of Michael Laylee's last,

I took university.

I think he's one of the most advanced researchers

that is working on making specialized,

helping devices like for paralyzed people

to make, like, connection devices,

so you can basically, with your brain,

control all kinds of devices for paralyzed people,

or people that are not able to talk anymore and so on.

He's doing amazing work, but, yeah,

there is a training phase for each single person.

I would totally agree on that.

In mouse experiments, Katarina,

you know of transplants of memories

from one animal to another.

Is that it?

Yes, they are, like, fear memories and context.

There is, but I'm not sure how specific, like,

to a specific, like, let's say, a general idea of a cat,

I think, can be transmitted,

or a general visual cue of a cat,

I think, can be transmitted,

but not a specific cat combined with an emotional cue

can be very easily transmitted, I think.

If we're talking about those hippocampal studies,

I mean, it's a very, very vague kind of notion

of, like, a general fear that was transferred,

but, like, something super specific like that,

I don't think anything's close to that.

So I have a different take on that idea.

So Nemento is basically saying that you might have,

very specifically, the analogy of a coffee cup,

you might have the coffee cup

is basically modeled by a thousand different

cortical columns in the visual cortex

and 1000 different cortical columns in the somatosensory cortex

and a thousand different cortical columns in the audio cortex

and 1000 different cortical columns in the prefrontal cortex

and so on, and then the different areas have to vote.

So even if you could, what I think is,

like, even if you could stimulate,

If you could decode basically the pattern of a cup in the audio cortex and the pattern of a cup in the visual cortex,

and then you could take it over and stimulate another person's brain with those exact patterns with amazing detail.

And I mean like assuming those patterns each have temporal spatial characteristics,

like the pattern of broccoli has recognizable temporal spatial characteristics.

It has, you know, it has a texture, it has color, it has a crunch, it has recognizable characteristics.

That you could, I guess, if you put together a voxel, a four-dimensional voxel,

a temporal spatial pattern representing broccoli, and then you fed that to the brain via stimulation instead of through the mouth,

then maybe you could cause a column to replicate that sense of broccoli.

But the thing is, if that one column is disagreeing with all the other columns,

maybe that will never become a conscious representation of broccoli in the person's brain.

Maybe you'd have to stimulate many columns in order to convince the overall system to vote that, in fact, broccoli has been detected.

So that's my thought there.

So am I understanding correctly that we can start all over again and build a place called Tibet and then we have our own refugee redeploy again for reincarnation?

Could you rephrase that, sorry?

I mean with the current technology that you just explained just now,

am I understanding correctly that we can start again a new religion

and then having the reincarnation again and call it a refugee from Tibet

and then we can stimulate the whole memory into another individual?

Oh, well, there's a lot of, I mean, not tomorrow.

There's a lot of problems currently, but as we were talking about earlier,

trying to simulate the brain with electrodes could be very imprecise.

So that's why people are talking about stimulating the brain with other ways with optogenetics or maybe with ultrasound.

But whether we can get the kind of precision we need with electrodes, I think there's a lot of skepticism right now.

By the way, if Carl hasn't had a chance to speak and Carl wants to go on to anyone else, maybe just let us know if you have something burning you want to say.

Just flash your mic a lot and I'll try to get you.

So it looks like Abyss and Nick both want to go.

Carl, do you want to say something real quick or?

No, I'm just in June to conversation.

Thank you very much.

I keep on listening.

Okay, Abyss and then Nick please.

Yeah, so I have sort of like a loaded question because I have to leave like a pretty soon.

But it seems like this is a great opportunity to ask the question because we have like lots of accomplished neuroscientists here.

So I was kind of reminded of like the paper that was that was released by the Carmina group at Berkeley,

which actually demonstrated that there is a cortical stridal plasticity using your prosthetic techniques.

Like they actually put in tungsten electrodes to actually induce some kind of cortical plasticity between M1 and the striatum.

So do you think like some kind of functional plasticity is possible to detect using the rampant technology that we have right with Neuralink and Dementa and whatnot?

Like do you think it's possible to kind of go into a research route where such plasticity between different functional groups can be detected?

The other question that I have is that I think with the like I said with the rampant use of invasive techniques to read neural signals,

do you think we can actually make an dent on a neurovascular coupling?

Because that's also one of the things that's been left to us and non-invasive techniques like using F-nears and what?

F-nears with EEGs and they have, you know, different convolutional problems.

But with invasive techniques, do you think that we can remedy that situation?

And lastly, I don't know if anybody has read the new publication from BrainGate.

They actually implemented a wireless brain computer interface,

which kind of in my expectations, it precedes that of Neuralink and any other wireless technology that I've came across,

which like they use their own antenna design, which is like around 3.5 gigahertz or something.

And they have the signal capability of a wired brain computer interface because they were kind of like the pioneers in that whole area.

And they were able to record for 24 hours.

But yeah, so I'm just like to explain what the paper says, but if anybody has read it and if they did, like if they have a general opinion on that.

Thank you. I'm done speaking.

There are various approaches, like many labs are working on our wireless and the research and because having wires sticking out and you can basically do any social interaction tests and studies.

So there are many labs working on this and they make a lot of progress.

So the other question you asked about being specific to brain region connections and using electric stimulation would be very hard to stay very precise.

They for sure, I don't know the paper you mentioned, but there are many studies right now using, and I said optogenetics to detangle basically the role of specific connection between the cortical striatum and amygdala striatum and BTA and the place that comes.

I mean, this is being all being mapped out with optogenetics.

And I think that's the only way to be precise because you can't really control with electric stimulation.

You can't really control that just exactly this neurons that will project there will fire.

It will always affect infant neurons and other neurons around.

So to 100% say just this input was used or if this projection was used, you have to go to tools like optogenetics.

You have to inhibit the whole rest of the brain with TTX or something like that, which would be quite lethal if you would do that in vivo because you can do that in the slides.

That's how we do it in the slides that we inhibit the rest of the slides with TTX and then we just stimulate the specific connection.

So neurovascular coupling with EEG ephnears, I believe you're talking about the fact that ephnears is slower than EEG and I believe the solution is laser ephnears.

So laser ephnears is what open water is using. You need a faster camera to do it.

That's also laser doppler is something that is being used combining with holography, which is similar to what open water is doing, but it's a bigger machine, so laser doppler with holography or function average spectroscopy with EEG, but laser ephnears.

So it's basically much faster. It's as fast as EEG and that solves the neurovascular coupling issue, I believe, and I think Nick was next.

I was wanting to just speak to answering that question a little bit, but if we want to move on, we can.

Please, go ahead.

So I study pyramidal neurons in the auditory cortex, specifically the pyramidal neurons that project to this tritone, and one of the things from Jeff Hawkins' theory since we're talking about this,

in that context, that I think is significant as he's pointing out that each cortical column is a sensory motor circuit.

And when we were talking about this, I think it was last night, I think me and Sumitai agreed on this.

But my hot take on what resolution these BCIs should be focusing on right now, I would say it's interlaminar.

So if you could track and stimulate precisely at the precision of, say, layer one in the cortex versus deeper layers, which are more involved with motor action,

and we could kind of understand things relative to the level that I'm studying things, that's kind of a coarse grain thing,

but it seems like a reasonable thing for the current devices to do now is to kind of get a handle on what's happening across the laminar layers of the cortex.

And then I think it would be essential for these therapies. The other thing I study is the interneurons in layer one of the cortex that are controlling dendrites.

And I think reasonable like first strategy therapies that could work with these BCIs would be some sort of pairing with motor movement therapies paired with sensory stimulation and accurate interlaminar stimulation from these BCIs.

I feel like that's got to be a better place to start right now.

Alright, I appreciate it. Thanks, everyone.

Okay, I don't know if Nick wants to go next. By the way, so I think Jennifer was asking Katarina for some papers.

But I'll let this one out. Let me just be myself. Go ahead, Nick.

I was just going to say a few things. I mean, obviously, optogenetics is awesome.

It's very specific, but any of us that work with it every day know that it's far from a panacea of specificity.

And I think Katarina kind of mentioned it a little bit like it's very hard to necessarily optogenetically stimulate one specific cell,

unless you've obviously transfected the cell with a specific protein or you've genetically engineered the model in order to express it only in a specific set of cells.

And then on top of that, you have the synaptic connection. So unless you're willing to, like you said, add TTX or synapse isolate with like CNQX or something,

there's many off-target effects. So it is just more specific than general electrical stimulation, but it's certainly far from this elixir of the all-knowing answer.

Yeah.

We were talking about earlier, too. There's a lot of barriers to using it in human subjects right now.

There's a lot of the techniques that make it work in animals. They're just not options for human subjects.

Right. Like even in the papers that we publish with it, you know, unless you snap to be isolated, the cell that you're doing, you know,

to actually say whether or not that certain cell type that you're stimulating or inhibiting is the one that's expressing it rather than just receiving synaptic input from another one

that's expressing it within the field of light is oftentimes difficult.

You can use retrograde tracing that, you know, that will only, like, it won't be one cell, but if you want to be specific for a pathway and only if a pathway got activated,

you can do expressive that it retrogradually traces and then expresses in those neurons and then...

Right, right. I just meant for implementing it in humans.

Yeah.

You'd have to do the injection, right? In a person.

Yeah.

Yeah, we could do all this stuff.

And then it's going to bleach eventually, right? I mean, it's only going to... unless you know how to open a critical period of plasticity in that short window you have with it,

it's not like a chronic thing, right?

Well, it depends on how often you activate. I mean, a CHR2 is not really good.

There are newer ones that don't have the desensitization effect as bad as CHR2.

But yeah, they are new and improved optogenetic tools that don't have this effect and you can use lower stimulation, like lower light intensity,

because of course, if you use warm light, you will have... and a lot just by using that light, you will change to some extent excitability and damage to some extent the tissue, of course.

But there have been a lot of improvements made, but yeah, we are not there yet to do it in humans.

As I said there, the only clinical trial I know of is far-right enough stuff.

But if you want to be precise at some points and to really fix tissue again in a precise way, I think you have to one day go that route,

you know, those implants, that's it, my brother, he does deep brain stimulation in humans because he's a neurosurgeon,

and you have also over time that you have tissue damage from electric stimulation and then it doesn't work anymore after a while

and then you have to adjust the frequencies or the new implants and so on and so forth.

Like the electric stimulation, how we do right now in humans has also a lot of complications.

So there is some sort of push from professors and so on.

There are just hospitals that we should start thinking about using up to 2 neg in humans one day, but it's not like in the next few years,

but maybe in the next decade, I think.

Well, regardless, it was kind of a minor point just to say that oftentimes oxygen genetics is counted as the next answer

and while it is very innovative and it's cool, it certainly has its limitations as well.

So it certainly points us in the right direction, no doubt, but to have it be the answer is obviously maybe far-fetched.

But I guess the original point as well as I was going to say or what I was going to sort of propose is to ask to anyone that may know

or has any insights into is that oftentimes when we're sort of thinking about using these, like in our link type of devices or something like that

in order to elicit a behavior or response or a thought or a feeling, oftentimes kind of similar to what I asked before,

we assume that if you stimulate XYZ neurons, you're going to get X response from the patient or the human.

And while that's certainly the case on a Tuesday, if you take that and do it again on a Thursday, you can stimulate the exact same XYZ

and now you're going to get Y response from it.

And so what am I trying to say here? How do we sort of bridge the gap between assuming that the brain is only producing responses

in the absence of any sort of varying input?

Because obviously one of the big functions of the brain is to produce different motor actions or patterns or emotions or feelings.

But at the same time, and arguably more so, task heavy is the modulation factor.

It's overcoming the modulation modulatory inputs coming from every other part of the brain.

And so is there any sort of integrated way that the same sensor can monitor the inputs coming in in order to tailor the outputs that it's going to be using?

So I do have a kind of speculative answer about that.

So it goes back to the idea that you have many, many columns that are basically simultaneously rendering the same object, a cup.

So you may have thousands of columns rendering a cup in the visual cortex.

But they're voting on whether that cup is real and then eventually it takes less representation.

So the idea is that your brain always has a context that it's rendering.

It's not just the neural correlates of what happened right now.

It's basically the whole afternoon is being felt by the whole brain.

It's all in there. It was rendered. It's in the short-term memory.

It hasn't been deleted yet. The person hasn't gone to sleep yet, so it's there.

It's there in a temporal way.

So the brain can notice temporal patterns and so on and so forth.

And that's going to, basically, if you have something that's going to be a simulation that's going to be effective one time and not effective the next time,

maybe it's because it's so like, in the case of a very metaphor,

it would be like when catarina is stimulating a neuron that is just below threshold so that the next time that it's likely to fire a little faster because it was already ready to go.

And that is enough to trigger the memory.

It's like, so it's the same.

So if we take that micro analogy and make it bigger, if we get a neural column to behave the same way,

it's only going to become potentially conscious, you know, affect a person's conscious state or affect them in the way you want.

If it's in alignment with what all the other micro, with all the other cortical columns are also sort of like predicting as real.

On the other side of things, next Tuesday, when you get the person the same treatment, they respond completely differently because their brain is predicting something that is not in sync.

And so your question is, can we somehow stick our neural link electrodes all over the person's brain to figure out what they're focused on now so that we can design a treatment

to stimulate the brain that is actually in alignment with what their brain is modeling at that moment so that the brain actually picks up the stimulation that you're giving

and carries it the way you are hoping it will be carried instead of, you know, canceling it out.

Like, oh no, I don't want this stimulation right now.

It doesn't agree with the rest of my voting cortical columns, that's my thought.

Yeah, that was just, and I guess to be a little bit more specific, but it sounds like you're kind of going in that right direction.

It was, you know, let's say that you're stimulating one specific aspect of the prefrontal cortex and I'm just taking one random area.

And that area within the prefrontal cortex, whatever this synaptic space is, is bathing in, let's say, you know, 10 nanograms per deciliter of norepinephrine at that one time.

And if you give a 5-hertz stimulation, you're going to get a behavioral response out of the animal or the person.

Now put them in a different situation, and that same synaptic space now has 15 nanograms per deciliter of norepinephrine, which is a very, very, very easily doable change.

You know, you can see three, four full changes with these very quickly.

And you give that same 5-hertz stimulation to elicit the same response and suddenly you've triggered, you know, a grand mal seizure or something like that

because now you have so much extra endogenous normodulators that are within and acting upon those same synaptic spaces.

You know, so I guess the thing that would be kind of cool is if there's sort of a biofeedback mechanism in that it integrates a, what do I want to say?

Closed-loop therapy.

Exactly, you have negative feedback in the normal biological systems, and sometimes you have positive feedback.

And it's a very, very, very important mechanism in order to prevent those sort of things from happening.

And when you implant an artificial device, you know, how are you going to implement this sort of natural negative feedback mechanisms that are constantly varying?

And if we can figure that out, that would be amazingly fantastic, and maybe it's already there. I just don't know.

It's like a reinforcement learning stuff.

But then still your question about variability between humans is also fascinating, and I think very much still it feels like an open question.

That sort of variability, that addresses the temporal variability potentially among one, but when you expand to the many, it's still very, it seems scary and exciting.

I think I can inject a pretty strong constraint on that exact issue.

Because I was talking, we were talking to Simatahi yesterday about this, and I brought up the no free lunch theorem, which just, it's actually a very robust mathematical proof, but just kind of very kind of broad strokes here.

You're just saying no learning algorithm is better than another learning algorithm, or you could think about if you plotted, you know, how good the algorithm is doing on the y-axis against, you know, an axis of problems that the area under that curve would be the same for all learning algorithms.

So the Thousand Brains theory is saying implicitly that there's explicitly that the cortex is using the same algorithm that it's been repeating, it's repeating, and he's hypothesizing that there's reference frames that are being computed,

like grid cells, place cells, this kind of thing, and it's that this algorithm is able to abstract any type of information into this, what is, like a format.

So the way, the only way that theory works with the no free lunch theorem is that you're assuming the brain is putting things into a format, puts all problems into a format that it can solve the same way.

So it's kind of touching on what people were talking about with semantics, right? It reminds me of, there's a recent Andrew Sacks, McClellan, Ganguly paper about semantic development neural networks and what semantic structures would look like in, you know, neural networks.

And so in terms of this, this variability thing, maybe the, you know, if we're going to run with Hawkins being right, then we would know then that things need to be put into this, this kind of format which might make it robust to these like individual variations between people

and put different circuits throughout the brain. I think I might be able to tie some of this together if I can be allowed to speak.

Go ahead.

So we saw, the no free lunch thing, it reminds me of Wolfram's idea of computational equivalence of where, you know, you can, you can solve the same problems with different algorithms and things that some are more efficient than others.

Rubatite said yesterday that he thinks, you know, maybe the no free lunch theorem is true, but the brain is tuned to solve the exact kind of problems that people encounter and the type of stimuli that people experience.

And therefore it's maybe an optimal or more optimal choice of computational algorithm. And this idea that these algorithms are general, whether you're in the visual system or the auditory system or so forth.

To something that Jeff Hawkins points to is that there have been experiments apparently in which the visual system of a developing animal has been connected to what should be the auditory cortex and vice versa.

And each one just takes this the input that is given and it adapts to it.

And all this points to the brain having this general algorithm that it can adapt to whatever input you give it, which leads us back to where we were talking earlier about, or you guys were talking, I wasn't talking earlier about how do we interpret signals that come out of a neural

and how do we put the right signals that go into, through neural link to get the things to happen that we want to have happen in the brain. And I think what Elon Musk is counting on being a very smart guy is that the brain will do the work.

So we don't design the exact input pattern or the exact output pattern. For outputs, he points to the idea of connecting, say, through neural link or some prosthesis to a physical prosthesis, say someone has a prosthetic limb and they have to learn how to use it.

You know, people used to think you have to hook exactly the right nerves to the right nerves to get it to work. But basically, you just give the brain an input and it figures it out. And it gives him an ability to do an output and it figures it out and give it some sensory feedback.

And you can figure out how to move the machine. It can turn on motors and turn them off and interpret whatever sensory information you're feeding in through the prosthesis.

And this is also a good guess to the idea that we're talking about, about transferring memories from one person to another. Like, how could one person possibly take a signal out of the first person's neural link and put it into the brain of the second person's neural link through the second person's neural link

and how could they communicate? Where would you direct those signals and how would they ever interpret one another? Basically, I think Elon Musk's idea is that the brain just figures it out. The two brains are sending noise back to each other and then they may be getting some feedback in some other way.

This gets to the biofeedback idea. They can actually use words to communicate as well and figure out a way to communicate just because that's what the brain is made to do. And that's what I have to say.

So there's some research that backs that up. I don't know. DeCarrie, do you want to go first? I'm sorry.

I was just going to say that's a really great point and I agree.

So David Eagleman is another neuroscientist. He's talked a lot about research that backs that up and that includes pulling a rat's eyeball out of the eye socket and plugging into the audio cortex and the rat's brain just figured out that it began to see with its audio cortex.

The audio cortex began to behave like a visual cortex basically and see with that eyeball.

In another case, David Eagleman talks about using a tongue strip. The electrodes on the tongue strip are connected to a camera.

So someone who's blind can begin to see because their tongue is being stimulated. The grid of electrodes on the tongue represents the pixels on the camera and then the person's brain just figures it out.

So the idea is that you could just dump any kind of data into the brain and that could be stock data and your brain is going to figure it out and where you dump the data doesn't really matter that much.

But yeah, so let's pause there. I also wanted to add, I wanted to add Nicoleli's lab again. He made a brain-to-brain network.

So that brains of two or more animals can be networked together and be part of a single computational system to perform motor tests, which points kind of into that direction and he made a brain-to-brain interface,

allowing transmission of tactile and motor information between rats. So all of this, his lab work points into the direction that the brain most likely can figure those things out.

Wow.

Would you, would you think some research papers, some more research papers, Ketterina as well, the ones that Jennifer was asking about? I forgot what they were, but what you just shared.

Thanks, Micah. I think specifically, just to clarify on my side, the ones when you were talking about inhibition of sensory experiences with respect to autism, if there was anything specific to that, I'd be super curious. Thank you.

There's like such an abundance on autism and inhibitory research. I can, I can pull up, are you more interested in like the immune response related, autism related? I can, animals, there's like a lot of work. Are you, are you more interested in human work? I can check.

Yeah, I think we're...

Sorry, Ketterina, it doesn't mean to cut you off. Yeah, work is specific to humans with autism would be most interesting to me. And if there is kind of inhibition or excitation of certain sensory systems, any example of that is like a starting point, I can kind of run off of linked research papers and do my own follow-through.

Yeah, yeah, there are a bunch of where they use the magnetic, you know, the huge magnetic fields to change the system. And yeah, I will, I will pick some up and put them, post them on Twitter and then we can go from there.

Thank you so much. I hugely appreciate it. I'll, I'll send you a DM as well. I did send you on Micah around the same topic. So, but I'll, I'll pin you Ketterina. And this is probably me saying goodnight as well. So thank you so much, all of you. And it's been a pleasure and look forward to being in future rooms with you too.

Thank you.

Yeah, thanks. I will go.

Thank you.

Have a good night.

I also goodnight.

You too.

Bye bye.

Bye.

So, to play devil's advocate, and as Micah knows, I'm a neuroscientist by trade, so I, I'm obviously fully on board with everything that's going on the Neuralink, but to play devil's advocate in the statements about, you know, the specifics of the stimuli may not necessarily matter, because the brain can, let's say, sort them out into its own independent use.

Along those lines of thinking, do we necessarily then need to go towards the invasive brain approach or can we derive the majority of the functions through, let's say, some sort of systemic perturbations, you know, that are less invasive or through infusions of varying

molecules or whatever it is that you might think of.

Because either way you're either way you might be providing access into the brain, and whether or not it's a specific electrical stimulus, or if it's a sort of more of a biological stimulus that's coming from the periphery, which the brain is receiving, you know, can you take some sort of complex systemic

assumption that then the brain has to encode, you know, just as it would an electrical stimulation within the brain, and get a similar response if the brain ultimately in itself is going to sort out what the stimulus is any.

So I agree with that 100%. In fact, I don't know if I've written about this, but I've been thinking about, you know, it may turn out that the fastest way to simulate the brain into a virtual reality is through your eyes.

And that sounds like ridiculous, right, because we do that already. But your eyes are like this high bandwidth port into the brain. But besides that, yeah, so by the way, I wanted to mention because there's been there was a debate about

you know, stimulating with optogenetics being totally necessary. I think I heard Mary Lou Jepsen talk about simulating with with optoacoustic a sub millimeter precision, but there's a paper that I can mention. I'm footers not letting me post anything right now because I'm doing things too fast for Twitter.

But if you look up optoacoustic stimulation at sub millimeter spatial precision. Oh, let me, sorry. It isn't my turn. It's the thing I just posted. So opto, optoacoustic brain stimulation at sub millimeter spatial precision.

So this is like the emerging modality for neuromodulation, basically low intensity ultrasound. The regular ultrasound can't can't do anything below like two millimeters or something, but this has a diameter of 600 micrometers and generates omnidirectional sound wave locally at the fiber tip through

optoacoustic effect. But it's just an idea that in the future, perhaps we will be able to do stimulation direct stimulation with optoacoustics. But but but yeah, so like, I imagine yeah that we could we I had this dream once that they can I put something on my belts or my shoulder

and the computer connected AI controlled my my body. So it looked like I was upright in the cafeteria and allowed me and then induced me into a lucid dream so I could get up out of the cafeteria in a lucid dream and begin to walk walk out on the street and begin to play a computer game that

involved in that case angels falling from the sky with with axes and swords and medieval weapons and I had to fight them off somehow. But the idea was that basically that the input to to the lucid dream could could be any part of the body.

And I think that is that as long as you're connected to nervous system, as long as you know that the networking protocols of being able to stimulate one part of your nervous system and cause reactions in your brain. And I think that, you know, the examples of that.

So see what I was saying was that, you know, if you similar audio cortex, the visual cortex is also going to imagine the cat that you're hearing right the big cat.

I think you needed yourself.

Oh yeah, I mean, I was sorry, I'm like, and I was done.

Yeah, no, that was exactly the point. I was, you know, wondering is that, you know, perhaps we can get a majority of the way there before we even get into the brain, which is inherently going to be a more risky procedure.

Obviously, you know, it's going to open up a new frontier and certainly evolving, if nothing else, the different techniques in order to modulate and measure the brain activity. But perhaps if the stimulus isn't all that necessarily to be specific,

you know, we can get those stimuli from somewhere else that might be a little less risky or at least on the trial runs a little more generalizable to the public without having to worry necessarily about the innate risk of, you know, imminent danger right away.

So, what Katerina was saying, she's gone for the night, but what she was saying was that she basically, so, you know, our neurons are being, they have tonic and phasic firing patterns. They have cyclic rhythmic patterns all the time.

They're firing every so often, and whether you stimulate them or not. But the idea is that, so what she did was she stimulated a neuron just enough to get below its threshold, so that she was extremely granular,

so that it wouldn't fire because of her stimulation, but it would fire faster, it would peak faster, so it eventually produced the pattern that she was hoping to produce.

And I think perhaps we might decode the brains to a sufficient extent that we don't have to stimulate as much as we might think we need to stimulate now to reduce the kinds of effects that we want to.

We might be able to reduce it down to just basically just stimulating just enough to disrupt the rhythmic pattern, but not as much as, you know, it may not need to actually cause any neurons to fire, but just get them ready faster.

Yeah, most of the neurons, I mean, when you're manipulating them in general, I mean, they're not, but obviously they're extremely complicated, but they're not all that smart of things. I mean, they're pretty easy to entrain with different firing rates, you know,

we do that all the time with the rhythmic systems, is can these subset of cells and train the rhythm in order of words, in other words, like what is the minimum necessary amount of cells that you can stimulate within a repeated fashion that's in a different frequency than that of the normal biological frequency,

and can you entrain that rhythm so that it resets it, and you can kind of figure out the underpinnings of what is the minimum necessary circuit in order to take control of whatever function is that you're measuring and we do that all the time.

But yeah, it's a, you know, it's an interesting thought that, you know, perhaps we don't necessarily need to stimulate as much as we do, you know, you can, you can take three neurons and control 30 of them, something like that.

Wow, that's really cool.

So popcorn style, anyone wants to speak? Go ahead.

I just want to say thanks guys. I've got to jump out. I'm working from home today and I'm not getting a lot of work done.

I'm in Australia, so it's about one o'clock in the afternoon here. Yeah, thanks for the really interesting discussion and I look forward to being back and joining you all again.

Cheers.

Great, thank you chair.

I'm also going to jump off. I'm on that East Coast, so it's 1am here, but it's been a great discussion. Thanks for holding the room.

Great. I will start inviting some new people since people are jumping off. This is a six hour marathon. I expected people would be coming and going.

So let me do some more invites and if you haven't had a chance to speak yet, or if you're down below and you would like to say something, just raise your hand and we'll call you up.

Yeah, thanks Mike. I got to jump off too.

All right, all right. Have a great night.

I just want to say a sentence. I woke up five minutes ago. I'm underloading my brain, not loaded, but I just want to say for making relation between computer and brain, it's not necessary to using implants.

There are many other ways and we need to research on why neural link is important and the Elon Musk projects because when we start using 60 internet, the IoT will be joined to our life.

And this is so dangerous, but I'm a researcher. I'm working on it and I love it. But about neural link, I think it's not new. We know many projects before it.

The people using implants on brain that after three or five years, and I just want to say it's not necessary to be using implants. It was seen and I'm saying enough.

Awesome. Anyone want to reply? I'm still just inviting some folks.

There's a quick question, Mika. Since we are talking about the technology available and visible to the public right now, and Elon Musk seems to be a very multi-talented person from PayPal, from the whole AOL until today, he has been involved in so many projects.

So are these projects are government involved? Because if we involve such a high-tech project, normally the government and the military will be the first involved.

And when we get into the public, it will be like 10 times lower than the actual technology already available for them.

So what is your point of view of the current discussion and the technology that we are aware of right now? Are there a line with the level of the government project that is already running at the back now?

I am not sure what direction to take that question. Leonard, do you have thoughts on that?

So the question is about, given the implications of Neuralink and Dementa and the technology that we have now today, including the things that Elon Musk is working on, you wanted to know how the government and people around the world should react to that technology?

Is that right? Are we understanding the technology from the alignment of the actual technology that already reaches maturity?

Because normally when the public sector receiving the technology is already 10 times lower than the actual technology due to the secrecy of the CIA agent and the government sector that is reserving the knowledge and information as a weapon to not sure what hidden agenda is, I am not going to mention it.

So are you pretty sure that the current knowledge that is available to the public can be discussed is aligned with the level of the knowledge that may be the government or some agency that is hiding behind?

So the part that I am stuck on is how do I compare this to the knowledge that is hidden? The knowledge that some agency has. I don't know how to respond there.

I guess you kind of want to know. I am just not clear. When you said when secret government agencies are, I don't know what they are doing. I don't know what secret agencies are doing. So I am not sure how to respond.

I think we can look at from different perspectives. If we trace from the talent pool and the scientists that are available, are they a missing group of talented scientists that is hidden at the background, doesn't come to the public and do their speaking?

And then if we have the awareness that what we are knowing today for the public sector is slower than the actual technology 10 times for example, then we need to start to think of the perspective of what is already available and what we are doing research right now.

And we need to do the critical thinking 10 times faster ahead so to avoid to be the victim of the weaponized of the technology. That is my point of view. They fear I am not speaking.

I understand that part. So for example, at the first talk about Neuralink, Elon Musk slipped on stage and he started talking about the monkey. The monkey that they showed everybody two years later.

So he started talking about the monkey and everyone else shushed him like hey, we are not talking about that yet. But that happened two years ago. And then two years later, we see this video, a Neuralink video, and a monkey is controlling a cursor on a screen and is basically a replication of brain gate.

So brain gate is one of the goals that they are trying to do is create like a mass produced brain gate that you can leave in the brains of people who are paralyzed and they will be able to control cursors on screens and maybe a lot of other things.

You know, hands like artificial limbs and have more fine motor controls with those things to Neuralink. And this is like low hanging fruit for a device with Neuralink's capabilities. And so these things will be mass produced.

So my perspective is that whatever the public knows about, even like covering a lot of these stories, writing about them, if you are focused on it, what I know is usually a couple of years ahead of what the general public knows.

Because that can be very focused on certain things. But what I don't know is probably a couple of years ahead of what I do know. And that's where you get to like with the military, the secret scientists who are not reporting to the public,

maybe about two years ahead of the companies that are working on similar technology. There's lots of cool technology that DARPA worked on that the US Navy owns and that has leaked out after 12, 14 years of now going into the consumer.

And that's where people like me get to learn about it. But yeah, so I just expect that whatever is being worked on behind closed doors by independent scientists who are not talking to the public is probably some really cool stuff.

And some of the things that we are speculating about today, they may have made significant advances towards. I was just like realizing that basically with LaserDoppler that someone mentioned LaserDoppler as being as fast as EEG.

And that's when it really clicked for me that like for two years, Mary Lou Jepsen has been talking about laser F-nears. And the functional interference spectroscopy was laser the entire time.

I mean that it was as fast as the, Mary Lou Jepsen's technology is probably as fast as EEG and has been this entire time. But I don't think that she ever mentioned that. I was just like, oh my gosh, she's had these capabilities.

And I don't know if she's mentioned that, but I don't know why she would. But yeah, so that was just like, I don't know who that was. Someone just popped into the room for a second and someone wants to come up.

But yeah, so in terms of, you know, there may have been really interesting developments that have already happened that we're sort of like vaguely speculating about, but I don't know how we can know. Go ahead, Mark.

Are you saying that DARPA has solved many of the AI problems that we think we're dealing with on the cutting edge?

I don't know, but they may have some cool technology. I'm not sure what exactly they have, so obviously.

But I would say it's much more limited than there was like 30 years ago because of how the economics work and how much money the private sector now can put into use on this.

And when you listen to people who work in government, they complain all the time that they don't have access to the top talent.

And there is also people in AI talking on their relationship or how they get approached by DARPA and other agencies.

And I would say there should, of course, be some delay and some technologies that they have are not public.

But I would say two things. One, they become public much, much earlier than we've done, like 30, 40 years ago.

And there is not that much as there used to be. I know, Mika, what do you think about that?

Well, one of the major things is to slow down the spread of information.

Yeah, so Yanling Kun, one of the core contributors of deep learning, he was one of three who got a Nobel Prize with Jeffrey Hinton in Yashio Benjiro.

But he was saying that back in the, I think it was in the 80s, he came out with this open source.

I guess this was before open source. They wanted to release software, basically open source. Open source wasn't a thing yet, but they wanted to release, I think it may have been a neural network.

Yeah, it was like, it may have been the first CNN, but the thing is, this was before the internet.

So trying to release code in the 80s before the internet was really hard. It was really hard to share your code with other people.

And this came down to basically like a bandwidth constraint.

You can imagine the pre-internet world, or remember it like me, right?

So just because of that, there's so much information, so much research that I imagine that the U.S. government couldn't even accidentally distribute software in the 80s.

So that sort of led to, they did have the internet before everyone else, right? But it's like ARPAnet, so ARPA before DARPA.

But the thing is, the information travels so fast that if the government has an edge in terms of time, it's probably a lot less of an edge in innovation

than it used to be in the 80s because information gets around a lot faster.

And so I would expect there that the government could be two years ahead of us, ahead of, but maybe not 30 years.

So maybe 10 years is kind of a stretch. I think that whatever they're hiding is probably not more than two years more advanced.

Just a wild guess, but who knows?

Yeah, but that was your observation with the monkey and the mast, wasn't it? He was talking about that two years ago before it was officially released.

So yeah, that sounds about right.

I remember a good example. There was this hearing about the satellites that only the military have access to, hearing that they have extremely high bandwidth to send data back and forth, and they're really, really fast.

And they can do GPS precision, extremely precise GPS. You can find a reference to this online.

I think the US government was trying to stop the European people in Europe from deploying extremely high precision GPS because that was an advantage that they just wanted the US military to have or something like that.

But as soon as you get high precision GPS in Europe, well, okay, well then what's the advantage for the US military?

But that's like why we don't have high precision GPS in the United States is because they wanted this to be like technology that only, they wanted the military to have technologies that consumers didn't have.

And so, you know, military doesn't feel good about that if everyone has the same technology they do. But I'm kind of like, I want high precision GPS. How about you?

My main data point is how people who work in these agencies often complain about how they don't have access to top talent. That's a really, really important clue.

All right. Well, I mean, I think we're kind of at the, we're going way off topic here. So the topic, maybe I should recap the topic.

So, yeah, we're just talking about Neuralink and Numenta and the massive overlap between two. Numenta has given us this sort of map for how our cortical columns are creating reference frames that reference the objects.

And they could work together to represent everything we can see and hear within our conscious workspace, right?

And Numenta, so Numenta is going to be putting new sensors into human beings that are, you know, have vast capabilities. They have, you know, when you have 16 electrodes per tendron and you have, you know, 3,000 of these electrodes with all these tendrons.

Then you get to a situation where you can do things like, like very, like each electrode may be able to detect, create a map of neurons that are firing within a 1000 neuron radius or 140 micron radius.

And if you have, like, basically the way tetrodes work, you can, if you have multiple electrodes, you can triangulate. And if you have a lot of electrodes, you can get high definition triangulation of where the neurons are firing.

And it allows you to figure out if you get it firing in the same areas with two different neurons or just one neuron or firing in two different areas, is it just the same neuron or is it really two different neurons?

And this kind of, this is, this is something a lot of electrodes to do. It's sort of like a high resolution reconstruction or medical imaging.

Who's developing that? And what stage is?

That's Neuralink. That's Neuralink. That's the topic of the room today.

Okay, got it.

So the idea here is that when you combine basically the research of Nementa with research of Neuralink, potentially what we're talking about with Neuralink devices is decoding the cortical columns and decoding what people are seeing and hearing.

Neuroimagers will be able to know when cortical columns are voting on a representation and what the representation is.

So that means that, that, that means, and I, and by the way, the guy, one of the guys who works at the vice president of Nementa was here on Columbus yesterday.

And I asked him if, if the Neuralink device is something that will be decoding cortical columns and he, he, he affirmed it.

Yes, that is, that is something that is in the range of the device's capabilities.

If you put enough of them in the right places, this is possible. And so that means that we're moving into a world in which, you know, the courts may be able to, I don't know what the timeline is, but we're moving into a world where for real, the courts will be able to search your brain if they have a warrant to see if you committed a crime.

And they'll be able to perhaps trigger, you know, you're, you're, you're part of your cortex to trigger it.

Like, you know, if you trigger the audio cortex, you know, with the sound of a cat, a big cat, your visual cortex might imagine the visualization of a big cat running towards you really fast.

Like a, like a, you know, say a puma, right? This is a very dangerous cat. And so you, you might be able to basically pull up old memories, I don't know.

But, but, but it's an interesting, it's an interesting idea that what you can see and what you can hear and what you can feel, even what you're thinking, your thoughts themselves, your emotions, these can be decoded from, from Neural columns.

We had a guest earlier talking to us about explaining how emotions will be able to, how you'll be able to just, if you can decode a quarter, cortical columns, you can decode emotions.

She was working with optogenetics and mice, but yeah, so those are big, those are big things to think about in terms of like, well, what is the, what is the public's reaction going to be?

What is the, what is the public going to ask the government can do to do? What kind of laws does the government want to consider passing?

What is the real, like, you know, these devices are going to be mass produced for, initially, for, for the low hanging fruit of, of helping people who are immobile to have more mobility, right?

To, to, to give, to let someone control the cursor on a screen with thought or to control artificial limbs with thought.

And then, but then, you know, we've mastered, then we've mastered, distributed these devices, right?

Because you have people all over the world who have mental illnesses, they have autism and, and depression and post-traumatic stress disorder and, and attention deficit disorder.

And these devices can provide potential treatments by stimulating and, and also depressing.

They can, you know, like we had this speaker earlier was explaining how she was able to make an animal that she was treating basically have a, a, a, I should see if I can, if I can pull my notes up right here.

She was able to basically elicit a long term memory that involved high frequency stimulation from an animal.

And then she had another animal to generate the compulsive behavior with high frequency, high frequency stimulation that matched that frequency.

And then, so the animal then began to exhibit the, the compulsive behavior, right? Because, because it was based in this high frequency stimulation.

But then she's low frequency stimulation to, to induce LDT, which is a long term depression.

And what happened was the animals compulsive behavior went away.

So that sort of thing is like, has been demonstrated in a mouse.

What's also been demonstrated in a mouse is, is, you know, taking, scanning base, the basic primitive memories like fear and like a fear response to a certain thing.

And then transferring that with optogenetics to another mouse.

So that's an, that's an example of reading and writing to, to, to a brain.

And when we have the high definition of neurolink, we're going to have high definition reading and high definition writing capabilities in the future.

So maybe a high definition image will be able to be downloaded from one person and uploaded to another person.

So you can see what someone else saw.

And that's real.

I mean, they were pre-curches of this technology.

There are papers that we can point to.

And, and so, I mean, I do have a group that of course people can join.

The group is on Facebook has the same title as this talk.

So if you type the title of this talk into Facebook, you see a group with about 9,000 people, 9,900 people.

And, but, but yeah, we've got lots of papers.

And of course, if you, if you need more papers, you can ask and we'll get, we'll get them for you.

But yeah, so like what it, so, you know, so you're going to have a lot of people are going to want to talk about this.

You're going to have a lot of people going to want to ethics discussions, legal discussions, what laws do we want to pass.

And a lot of scientists are going to want to talk about this, like how is this, how soon is this coming?

What, what are the sort of like technology hurdles that and science hurdles that we need to do to get to this point.

And the thing that won't happen is it won't, won't be stopped.

And, and that's because it solves too many problems.

It solves too much human pain.

It solves too much disease.

And so when you have a lot of people who are in pain and they have diseases, their money is going to be at the forefront of their concerns.

So they're going to be paying into these expensive health care systems and health care systems are going to be looking to lower their costs.

And so the pain is going to drive money, which is going to drive this innovation, which is why this technology is coming, no matter how we feel about it.

But, but if we know that it's coming, then what do we do is, is, is we talk about it and we, yeah, so the world is going to change.

But, but the kind of thing that is being handed out is, is more dangerous than, than a scalpel.

It's more dangerous than a gun.

So it has to be, so regulation has to be considered in terms of, in terms of being realistic.

Yes, it does need to, it does need, it will like need to be, to exist because, because people need relief, people need medical relief, right?

But, but it's also a very serious tool that could be misused.

Yeah, that's come to my, come back to my point that the gap of technology available between the government's military and the public sector.

So the public is starting to have awareness of the tools can bring your, your, your mind and expose all privacy and everybody is naked and there's no privacy at all.

So how, how sure that we, we, we, we sure that the government is not already deployed and reading everybody's mind in the background for the past like 20 years or 30 years or 40 years already has been deployed.

And they come to the legal, legal policy that the public started to have awareness that we need to have some policy and legal way to protect ourselves.

So therefore, therefore I also searching, is there any device that can detect such activity reading other people's minds for their own objective already happening in the background of the society.

Let me give you some example.

In Malaysia, we have this culture of afternoon lunchtime at the big corporation like Malaysian airline. And we also having the CEO has been changed and he cannot accept this kind of a culture in the corporate world.

And when you think about reading people mind, you need the subject to be stationary. So the whole group of people in the corporate is having their head down and having their afternoon nap during lunchtime.

And then when they wake up that immediately execute what has been programmed.

So like looking at this kind of phenomena, you can like trace a bit that is something fishy going on in the background.

And then speaking of this year.

All right.

Where are we with the regulation? What do you see in regards to what is being proposed? If something is already been proposed or not, how that is coming?

Well, there's an I guess there's a sort of example in European legislation for artificial intelligence, which everyone might be have seen in the news recently.

I don't know how familiar everyone is, but I'm not that familiar with it. I briefly looked at it.

Yeah, so there's a proposal in Europe about, well, you know, maybe we should restrict some AI, some businesses from doing some things, some corporations from doing some things, people from doing some things.

Maybe the military should just have like the best of the best AI, but only for specific reasons, right? Only if we're looking for terrorists or something.

And when I hear stuff like that, I think, well, if you look at the history of all governments, like the history of all governments, there's no government that you can really trust.

But then someone else might, yeah, there's no corporation that you can really trust either.

So it's like a situation where like, well, I think we don't want to make one group a lot more powerful than everyone else, right?

I mean, I think so. I mean, that's my thought.

So the idea that only the military should have face recognition, for example, I don't know.

I don't know. There's a discussion that has to be had, but I'm not in Europe, so I mean, I don't know what will happen in the US at some point.

Let's see. Does anyone else want to respond to that one?

Oh, here we go. We got someone.

So welcome. No, that was completely incorrect.

Okay. Not last three years, not next three years.

It will never be, it cannot be used against a lot of people unless they are tied up in a like tortured table or something like that.

It cannot from far away. It's again, it's physics tells you cannot from far away record activities of the brain of people or change them.

It's physics laws. Even EEG, that's like where you record from the sculpt.

The resolution is so low that you cannot see it cannot at all. I mean, absolutely.

I mean, it's a laws of physics that is impossible.

What are your thoughts from EEG?

Are you referring to...

But for that, if you have implants in the brain, so how can you put implants in the brains of people against their will?

You have to arrest them, tie them up, fantasize them and like make a hole in their brain, etc.

So you will obviously know if something happens.

Okay. It sounds like you're referring to the idea that someone said earlier about the government already reading your mind 20 years ago.

I'm not suggesting that.

I'm just talking about the last thing that was said.

I may have not heard the last thing that was said. I may have zoomed out and I think I'm getting tired.

In terms of neural ink, this device that is in Nementa, the idea is that your brain will be creating many models of what you're experiencing

in many different parts of your cortex, but specifically in the cortical columns.

And then, you know, I asked Subitai from Nementa if a device like neural ink could be used to decode those cortical columns

and the voting process that they go through to recognize an object to affirm it.

And he said yes, that basically a device like neural ink could be used to decode cortical columns.

And now if your thoughts and feelings...

That's possible.

What do you say?

That's possible. We are far away from it, but you had all of them.

I think the things you said are impossible, possible, and I don't know when we can achieve that, but it's there possible.

Okay, but I mean, I didn't propose anything that was not possible.

I'm saying that there's a lot of papers we can point to to support this argument as well.

But the whole discussion has been about, it's basically...

I guess there's a lot of different ideas about how the brain is processing information,

but if the folks at Nementa are correct that cortical columns can be decoded

and that potentially we'll find the representations of what we're seeing and hearing and feeling in microcortical columns across the brain

and potentially that means that we'll be able to download those representations or predict what is there,

even if we, you know, which is a different way of getting at those representations.

But in the long term, I mean, if the same process is also happening outside this,

that's sort of the primary sensory cortices, but also like in the prefrontal cortex and the other areas of the brain,

the idea is that if you have an idea and the idea has properties, and that is also a reference frame,

it's the same sort of like mechanical operation as your brain tracking physical objects in your environment with your cortical columns,

which is what the Nementa book is about, a thousand brain theory, and the same idea applies to concepts.

So just like mapping everything physical with different cortical columns,

thousands of them all over the place, but in that your cortical columns are also tracking abstract ideas in a similar way.

And so you may have many different representations of the same idea,

and the cortical columns are helping to track the integrations of the different components.

So there's like, it's the structure of how that works is supposed to be similar to the way that you get a reference frame

out of the interaction, out of place cells and grid cells and entering a cortex.

And so if we can basically decode what cortical columns are doing in the sensory cortices, you know, with psychophysics,

and then potentially we can decode what the cortical columns are doing in the rest of the cortex,

and then we're listening to your thoughts at the level of cortical columns firing with enough sensors placed in enough areas.

And so that's what I'm saying.

It's like, well, okay, now we're decoding the brain on that level, which I don't think I'm disagreeing with you,

but in terms of this might be a long road to get to this point.

It might be decades of research, maybe.

But, you know, but, yeah, but if we can figure out the network protocols,

then we can figure out, you know, exactly the right amount of stimulation,

like this one optogenetics researcher came in earlier and she was saying that she delivered an amount of stimulation to one neuron

and that was sub-threshold, so that when that neuron was going to fire again in the next cycle,

it fired a little bit faster and that produced a pattern that she was hoping to generate,

which, you know, she was doing things like causing neurons to basically fire a lot faster

and introduce basically dysfunctional behaviors for the animal or then have them, you know, fire a lot slower

and make the dysfunctional behaviors go away because she was introducing long-term depression.

But she was, like, really just getting very granular and very, I guess,

doing coarse-grained stimulation to just produce just the right results, to change the rhythms of the brain

to create the effect of what she wanted to do.

And so I think that, I don't know exactly how we're going to, like,

I'm looking at, like, optocoustic brain simulations at several millimeter precision.

A neuroscientist came in and he was talking about,

well, what if we just stimulate outside the brain just to create the, you know,

because you can throw a stimulation at any part of the brain.

If it's like, you know, if you plug in a third eyeball into the audio cortex,

the brain is going to figure out how to see with that eyeball.

It's going to take whatever information you give it, wherever you give it,

and like a general learning algorithm, it's going to adapt and learn what that is

and learn the data stream and then pause because I'm just going to give someone else,

so I have a chance.

Now I'm coming from the wisdom of the ancient wisdom

because I have been studying all this ancient, you know,

Buddhism book, Quran, Muslim book.

In the Holy Quran, they are clearly stated that it's an angel doing all sort of thing.

That is one angel that is being assigned to record down human thought 24 times 7.

So now if you replace the angel as a satellite,

and according to the timing of the return of the Holy Quran,

when was it and maybe they already achieved the technology

that already be able to record all the human race thought to the technology

that has been missing or being brainwashed from the society that we lost touch of the technology.

So and this is really clearly stated on the Holy Quran

that maybe the scientists should also, you know, take a reference maybe

2,000 years ago, 5,000 years ago, that century,

that generation already achieved the way we want to achieve right now.

So if they are so daring to put into the Holy Quran

that it's an angel that recording 24 times 7,

what is your thought about this?

So I want to escape that,

because I want to escape that going into that direction

because it's too far off topic.

But let me see if we can invite some more people here

and we'll sort of move the conversation around a little bit.

Let me just change some things.

Thank you very much, Dave.

Let me actually, I'd love to hear more from Sohail for a little bit.

And then of course, who can we invite to this room?

I want to bring some more people up here.

Let me just invite everybody down there to come up and talk if you'd like.

Miha, I think what you described was, I don't know what to add it to.

It didn't sound like a question or something like that.

Yeah, I think that's still a dramatic point.

For example, auditory cortex can do the job of visual cortex.

Things like that.

Yeah, I mean, oops, I muted myself.

Yeah, there was an, so the researcher was David Eagleman

and he talked about how a rat's eyeball had been pulled out of the eye socket

and plugged into the audio cortex and the rat's brain figured it out

and began to see through the audio cortex.

Another experiment that has been done was connecting an electrode strip to a camera

and then connecting that to the person's back

and a smaller electrode strip was attached to a person's tongue.

In both cases, the subjects were blind

and they began to see a primitive image in their mind.

They began to see through the camera because, and it was very simple

because they just took the pixels of the camera

and they had that create the electrical stimulation on the electrode strip

which either went on their back or on their tongue

and their brain figured out the image based on the electrical stimulation.

So the idea is that you really could plug in any kind of information

to a part of your body that has enough nerves like your feet or your back, your tongue

or into the brain directly and your brain will figure it out

because it is essentially a general learning algorithm.

The neocortex is essentially self-similar across the entire thing.

I guess you could say it is divided into cortical columns

but the cortical columns are essentially repeating and others differences

but the argument from Nementa is that the similarities are broadly more common

than the differences.

So let me just pause.

Yeah, a cortical column is almost like a universal computing device, a computing machine, something like that.

In computer science we have a universal Turing machine.

I'm not saying a cortical column is a universal Turing machine

but it is a similar kind of machine.

It can adapt itself to very versatile so it can do multiple senses

and many different computations and calculations can be done in

a cortical column in a generic way and this is kind of in evolution, this is kind of a later stage.

So the sub-cortical areas of the brain were kind of almost like special papers,

computers or special papers devices.

Then a cortical column could kind of achieve a new technology in some sense

called the general papers computation but it's slower.

So that's what I...

Yeah, it's a bit slower so cortex and sub-cortical areas they work together in different time scales

and that's more or less like the fastest and slowest.

Like vaguely related to a fast and low system as well but that's a different issue so it's kind of...

I'm not saying like precise statements here but yeah.

But yeah, this is one of the fascinating things about cortex.

So cortical columns have been kind of identified or discovered by Mont Castle

in I think the 70s or I may be wrong about it.

Yeah, Vernon Mount Castle, yes.

And it was before 1950 I think, like 1940s maybe.

Oh, we lost you, let me bring you back up.

I don't know, I don't know.

I can say about a bit of history of that but I don't know what is useful to say.

I just don't want to start talking about any big purpose or something.

My apologies, this is leaded.

I think I moved some on to the stage by accident.

Yeah, you're fine.

So that was an accident.

So if you want, go ahead, keep going.

Yeah, and then so later the more qualities were, first of all they discovered that

so maybe it's good to know, it's a bit detailed, maybe it's good to know.

So when Mont Castle, where are known Mont Castle,

sent an electrode in across the thickness of cortex.

When the electrode was perpendicular,

he observed different things and when it was like slanted, I don't know what to write about.

Then the neurons had different qualities,

but when it was perpendicular to the surface of the cortex,

basically when he was going through the thickness of cortex,

he was seeing all neurons do the same thing and he was surprised

because it's very tiny, like how many millimeters?

It's like less than a millimeter or something like that.

It's like six layers.

I forgot how thick the cortex is, six layers,

but it's long enough so that you can see a variety of neural responses and functions.

But he was consistently seeing that across the depth of cortex,

across the thickness of cortex,

when he moves exactly perpendicular to the surface of the cortex,

he sees exact same responses.

So we kind of propose this idea that it's like,

imagine like a case that you cut it with a knife, particularly.

And there are layers, but each of these columns has a basic same functional unit

and those are going to work together to do this kind of computation.

So this idea was kind of, first it was not accepted,

well, because all new ideas are like that,

because it was a too big claim about the organization of the brain

because what if it was this once or twice, it happened accidentally.

But when other people saw the same thing, it was generally slowly accepted.

Mont Castle was one of the very, very important neuroscientists,

but people expected him to win the Nobel Prize.

I don't know, he didn't, maybe because adoption of his theory was slow.

So cortical columns and then there are,

so this is more about the organization of the brain initially,

but then later he suggests the more things like the hyper columns,

like multiple of these columns make bigger columns that are still closely related,

but not identical, and etc.

So they make their organization create kind of modules

and these different modules in different areas work together in a distributed process, etc.

So he developed these kind of theories with lots of rigorous experiments on mice, etc.

And so neuroscience was already there and they were doing this stuff like in the 70s and 80s.

Yeah, neuroscience has started to order,

but I'm just talking about the history of cortical columns.

So during time people tried to understand the cortical column,

what is that magical circuit that can do every calculation.

It was later on that it discovered that it's a universal circuit

that can do same circuit or same micro architecture,

can do many, many versatile functions across different brain regions,

not only different senses like vision and hearing,

but also many different things like calculating physical calculations,

like how objects in the physical world behave.

Basically most of the things that brain do, it was later gradually identified.

But it remained as kind of a secret and it's still almost a secret,

that not almost like it's very...

They have studied many, many different types of neural and many different connections that they have.

So they have a detailed map of how these are connected,

but there's still a lot more details to figure out to see how exactly it works.

So each of these columns has a similar connection to some other cortical area,

sorry, brain areas, like I just said, like plate cells, recently discovered.

Or thalamus, very...

Like very regularly they are all directly, all the time,

macrophoresis and macrophoresis information to thalamus, another area of the brain.

That kind of almost coordinates or manages the information processed by these cortical columns.

It's like a call center.

But these are all theories, like over time there are more functions found for thalamus, for example.

It's not just a call center, they're the coordinates, cortical columns.

And other things, but a new mentor, Jeff Hawkins, he had his own theory.

I think he's a really good neuroscientist in a rare case that a business man is also a neuroscientist.

And they invested in proper neuroscientific experiments to really, using a scientific approach,

unlike other companies, find out, like focus on what this...

Focus on one problem and find out what this cortical column does.

As a scientific question, not to produce, not to be worried too much about the application,

because if you want for each experiment, find application for that particular experiment,

you won't be able to find out how the cortical column works, for example.

So he invested this money there.

There are other businesses as well, like Allen Institute, like co-founder of Microsoft, Paul Allen.

Paul Allen is less famous than Bill Gates, but he was actually the...

Initially the brain of Microsoft was Paul Allen, but he later founded Allen Institute,

using the money that's obviously the operation.

So Allen Institute is one of the big companies.

Also Allen Institute invests in basic science in the sense that when they do a research,

or when they map the brain of miles, etc.,

for each project, they don't look for how much business value it brings to them immediately.

So they do basic science research, similar to academia a bit.

Numenta, because of that, Numenta and Allen Institute have been able to achieve some basic finding,

understanding similar to what is done all the time in academia.

But they are more famous because probably because they're companies,

that the companies generally get more publicity.

And yeah, this is a very recent finding by Numenta that, in line with their basic science research,

that Cortical Columns...they understood more things about Cortical Columns,

in a similar way that many, like thousands of...

Let's say like for Cortical Columns, maybe hundreds of years of science working on that area.

Numenta and Institute centers are other research centers alongside of them.

So they found interesting results about like how the location or place of the...

If I understand this correctly, at the level of Cortical Columns,

in that basic or versatile computing unit,

it kind of takes into account the location all the time.

And it's using optical recognition and other functions.

So they are kind of driving...Numenta is also driving the targets of science as well.

Neuralink is good in methods, for example, they are...

They found some, apparently some surgical techniques and stuff like that,

which also helps science as well, but they focus...

He's not driving science necessarily right now, but Numenta is.

So I was also fascinated by Cortical Columns and also Jeff Hopkins,

I think he's a real scientist that is driving this, using more or less good approach.

And that's why he is...

He's like teams or he's that are achieving understanding new things about Cortical Columns.

Sorry, I couldn't very well because it's early morning here, it's like six o'clock.

This has been great, yeah, thank you for diving deeper into Numenta and Neuralink.

I'm sure many people are interested in both companies, since that's the topic.

Leonard, are you up for chatting?

I'm inviting more people up, but...

Yeah, I just, I love what Sabal was saying and I'm still like, you know, cooking and trying to do that.

But I think Eric Weinstein is working on some of this work and I'm just kind of listening and taking it in.

And I think my only thought, just a similar conversation, is the bridge between the chemical research of the brain

or the human brain, you know, how we function versus a simulated mind.

I mean, my company, we've been working on machine intelligence through a network database.

The way we're clustering it is that sort of human-computer interaction and how we kind of focus on decision theory

and probability theory versus the earlier healthcare ramifications, which I think is just really vital

and what stimulates our processes going back historically all the way from human history.

So, you know, why are some people inclined to be X versus Y?

How are they able to overcome circumstances? Yada, yada.

And when you start just doping in that into true AI, outside of the computational side,

you know, artificial intelligence encompasses that psychological side of philosophy and questioning.

What is it to be human? What is it to exist, you know, whether you're human or not?

All these things as variables. And then what do we measure as this output?

You know, it's interesting because me and my girlfriend were watching a movie on Dan Brown's movie.

What am I thinking of? What was it? We just saw it yesterday and then with all the da Vinci code.

And so you cross over that with the original one, with religion, belief in society,

a little bit of science and so forth. So there's just a cross intersection there that fascinates me.

But my focus has been more on the narrow side of economics and decision probability theory,

human decision making, science and things, but indecision quality and so forth.

But when you could encode that, rather it's semantic neural language processing, you know,

if it's encoded binary by structure or syntax or semantics, if you really can get this machine to infer,

to really draw influence. So let's say if we get the hybrid of taking AlphaGo over at DeepMind

and then apply that to the human across the board, you know, are we really looking at a true AGI

and artificial general intelligence, you know, and are we really trying to get closer to that?

And what does that mean for us? So I'm still pondering and fascinated by both sides of it,

but it was great that I'm not sure if I think it's your call.

If I'm pronouncing your name correctly.

Oh, how do you pronounce your name?

So hey.

So hey, okay. I'm following you. So hey, so love to follow up with you at any time.

And for anyone in my profile, my emails there about work and stuff like that to drill down deeper,

and Michael knows this. But, you know, yeah, it's just a great room that you're doing.

I've been following you for a while because it's interesting that I think they're getting closer to,

I'll start just going off the grid here of when we start looking at edge computing and devices and the data,

the data upfeed and download feed, what can really be possible, you know, and so if we can see a world with that.

And as a side note for some of the room, for some of the guys here, girls and guys, excuse me,

you guys might like them, I didn't participate this last Sunday, but there's a sci-fi talk that Hollis,

Dr. Hollis has, as you said, Berkeley or somewhere up in Northern California,

every Sunday on the clubhouse, it's a sci-fi room, I'll see that, it's sci-fi talks or something.

And they kind of just go over these hybrid things of what science fiction and what will become science fact.

So a lot of this has led us to where we are now in this technology of at least imagining and visioning

and then looking at new models and new ways to do things versus the repetition of applying certain data models

to old axiom or paradigms, because there's always someone in a lab that's kind of reinventing the wheel,

so to speak, that's kind of how we've gotten to where I think we are in this world, right?

So I don't think we discover anything, in my opinion, we rediscover how to use electricity,

because I just keep always yielding back to my favorite innovator, which I just kind of love,

and my girlfriend will say I'm obsessed with him, but that's Nikolai Tesla.

And if anyone knows, and I'm not talking about Tesla, the car manufacturer, I'm talking about Nikolai Tesla,

who patented all of this and it's something that I've done already, but when you show down in this work,

you know, he's been able to connect this in the 1800s and we're just seeing a fruit from that tree.

So those are my few words.

Interesting. I think you mentioned decision-making is one of the few areas of the brain that is more understood.

I kind of well understood and are very good models that can describe how the brain circuits do decision-making,

but in certain contexts, in limited contexts, but because there are many, many things that the brain does

and we don't understand, but decision-making is one of the areas that is relatively more advanced

and more understood in detail. So I think that's why it's a very good area to target for.

Maybe we are in that stage that we can find applications.

So I think that's a good approach, in my opinion, from my knowledge of neuroscience.

It's a good area to try to find applications for and decision-making and it's one of those areas.

And about chemical, yeah, we, electrical activity is just one aspect of one of the dimensions of the brain activity

and chemical neurotransmitters also neuropeptides, kind of slower molecules that slower than neurotransmitters.

So genes, gene expressions, these are like three more layers that they are being studied, but these are not like integrators.

So they closely relate to electrical activity gene expression and other more complex molecules work together

to build the function of the neurons, but we are still far away from understanding them.

So yeah, there are complexities about chemical versus electrical activity as well,

but still more research needs to be there and there are already like hundreds of thousands of groups working on brain,

working so slowly and there's lack of funding because studying each small question requires like six months

or a year of work of some people in the lab to work on brains to find out answers.

So that's the main limiting factor of how much funding in science research we have.

Sorry, I digressed a bit, but yeah, it's great that we're using, I think, if for example find out how people make decisions

and how brain state of people are in learning, we can help in learning.

So already there is, I personally know about one application I haven't used, like for example QEG, you might actually use QEG, I don't know,

but QEG already is useful in that because you can see like at a very rough level,

but at least that rough map of activity of your brain at least tells you what the state of your brain is,

at least you'll know, I don't know if you're anxious in a certain sense or how focused you are,

or maybe it gives you some indicators, some just indicators of your decision.

It's like for example, like very, but that might be still useful for, very actually useful for people who are like students who want to learn,

or people who might think, why am I not learning this?

And then when they see the brain in the mirror of this QEG, like metaphoric mirror, not real mirror,

then they can see, oh, because kind of there is a bit of anxiety sitting there, so I'm just talking allegorically, not literally.

Okay, my brain is not dysfunctional, for example, it's not that I cannot learn,

that thing sitting there doesn't allow me to think so, let me sort it out,

and then they change the brain of the state of the brain a bit and maybe place practice,

and then they see they learn better, so they don't fall into trap of, oh, my brain is not working, I cannot learn,

I'm not, so I'm not intentional in something like this kind of trap or trap of health,

which is that people fall into these upgrades, benefits for people, they can be used even now.

So are you, Leon, are you kind of, so you know what, saying something, I just asked my question.

Oh, no.

Are you using something already, or like in your, in my research?

Go ahead, well, what you just touched upon, and I apologize for the multitasking,

but I took it across and I'm going to be on stage, be more devoted with other activities.

For education and stuff like that, I mean, can you elaborate how you think that can be used for education in more detail?

Yeah, 100%, I think what you're touching upon is like, well, first of all, and I was a former Marine,

I was in the United States Marine Corps, when you're talking about soldiers learning a new trade or new skill,

or dealing with, you know, PST, post-traumatic stress syndrome or PTSD, I guess I would say, how do they cope, right?

So sometimes it's just, it's interesting to me personally, as my experience, because I'm older than 42,

but it's like you'll see where it's reinforcement learning, if you know AI, machine learning, you're in that space,

that's all that's being reiterated to the brain, like we're just looping in a sense, I call it looping,

and you're just reiterating the same thought pattern, which are actually separate,

chemically and separate from neurons, but they're just sort of isolating about 80% of that repetitive pattern that says you can or cannot do X or Y or Z,

or you're still dramatically going through this. When that movement, you know, that memory is sort of in the cortex of the brain,

it's just still feeding that into people, which makes these individuals relive that situation, and even produce the chemicals, you know, within their brain to relive it.

I think it can be applied in educational efforts, 100%, I just believe that. You know, old school, we would go by, I'm calling old school versus old world,

you'd go by the discipline of just, you know, and I think even Tony Robbins and other, these are in the U.S., sorry, I'm speaking to the U.S.

Tony Robbins is a motivational speaker, and it's a lot of these people that used to say, you know, just think you'll believe it and just keep reiterating that,

and you know, you're able to kind of come out of your circumstances and change your life trajectory,

but a lot of this has been studied and researched that what occurs during these people going through this process for a couple of years,

or two or five years, joining programs such as Landmark or whatever, they join these programs and they're changing their chemical structure in their own brain,

because that neuroplasticity, which was spoke upon earlier about a phenol and a half a door cell, and I'm just going to touch upon that,

the neuro, you know, the plasticity of the human brain, that it can actually, even though it was hard-coded, it could actually remote itself,

almost like a computer or a simulation or an AI, right, because we all know that AI, in a sense, is everything software,

and if you just have a hardware model, you can, you know, send data to that hardware model to update it or upgrade it,

but the software is the fluidity part of it, or the plasticity part of it, that allows us to expand,

that's how we as humans can look at space, you know, my chairman is the former program director of NASA, Dr. Alexander Naraki,

and he, you know, and I'm pleased to always have him as a friend, but he helped program the International Space Station,

I mean, managed 1200 engineers, and I was asking him, why were you guys successful, why did you think you would be successful

of sitting the space station up there, because it's been about 20 years since it's been up there, 25 years,

and he's like, well, we knew if we just did the work, and, but at some point, you know, there was a belief,

and that goes even further, for them to be able to add to that, and then have it continuously work,

I mean, we as humanity have created some great feats, and have overcome a lot of, you know, troubles,

and I think we can continue to do so, as long as we're reminded of the science, and the knowledge, and the technology,

and the belief, and what we can do versus what we cannot do, so I was trying to finish up on that thread,

or touch up on that thread of the human plasticity of the brain, of being able to sort of chemically neuron switch,

and then over time, I think in some of the newer science in the room, it may be used as how you speak to this,

where I believe it kind of becomes a fixed mindset versus a growth mindset,

I think one of the psychologists at Stanford did a study on that for years,

and basically what she was pointing out was that it literally, there was a part of the brain that literally is like frozen,

like a cube of ice, which just shocked me, you know, 15 years ago, whatever.

It just really surprised me that our brains could be that hard and wired, but we do have the ability to know that sort of life.

So I've been listening to Leonard and Sohei, and David came back, welcome back David.

In terms of agency or choice, well both, let's talk about both, but, you know, so we have the feeling of agency,

and we were talking about how if you don't have, if you're interneural,

if your interneuron system is dysfunctional, if it's disrupted,

I guess if your glial cells are damaged and you have interneuron dysfunction,

and I guess your immune system could cause your glia to be destroyed,

and then your brain is not producing enough inhibition, and so your inhibition is off the rails.

In some examples, if you have too much excitement in one area, you may have a compulsivity disorder, right?

You may interrupt people too much, you may, I don't know, you may have too much stress, you know,

like maybe there's some link between PTSD or OCD in certain parts of the brain that are stimulating,

like basically overstimulated, and so if you could basically stimulate those parts of the brain with a low frequency stimulation,

and induce LTD, low, basically the idea is that you could make that behavior go away,

and you could, so basically by using, you know, either high frequency stimulation,

like 10 hertz or low frequency stimulation at 1 hertz to stimulate or inhibit different areas of the brain,

you could cause behavior improvements for people who are having a variety of mental issues,

including issues with self-agency or issues with feeling like they have choice,

and with devices like NeuroLink and perhaps also the open water technology we're talking about,

like potentially stimulation with acoustic, optoacoustic stimulation, you know,

which has sub-millimeter precision, then perhaps we could think about devices where you can really address

some of these issues that people have around the world in terms of their brain is just not firing

the way other people's brains are, and so they would have more treatment options, I think.

Pause there for a second. If you wanted to respond, Leonard, or anyone.

I did not send David down there. But if anyone wants to come up and talk, you just raise your hand,

and that was it. We've had some accidental movements today that no, anyway.

So people are welcome to come back up.

I have no idea what's happening there. That was weird.

So agency with inhibition. I mean, I could go deeper.

Decision theory. So there was a situation where I believe it was, I think it was 2015.

Scientists came and they said, we've figured it out. We figured out twice in the brain,

it's these neural circuits in the prefrontal cortex, right?

That's where it's happening, and we've got this computer model that proves it,

or not proves it, it demonstrates how it works.

And so it was these articles that neuroscientists identify brain circuit to control decision making at MIT,

and then there was something, oh, let me see if I can, I think I saved it.

Let me rotate to decision theory.

Oh, I just saved it so I could pull it up again.

This is weird. Oh, okay, so there's a paper, you can check it out, it's called Working Memory and Decision,

Making in the Frontal Parietal Circuit Model, right?

David's back. It's great. He's coming in and out. I'm assuming he's coming out.

I have a very bad connection, Michael, that's why I'm trying to listen in,

but the internet's not helping me out at the moment.

Okay, no problem. We understand.

So we didn't know what was happening, but thanks for saying that.

So Working Memory and Decision Making in the Frontal Parietal Circuit,

they created this really amazing, check out this article.

I should probably link this, but I understand that my Twitter is,

I think Twitter doesn't like me linking all these really nice research articles or something.

I don't know, but I'm just kidding. I'm using Twitter too much.

So they're like, you can't do it today. I'm like, okay, all right.

But I'll link the articles another time.

So the article is called Working Memory and Decision Making in the Frontal Parietal Circuit Model.

So if you look that up, you'll see the article.

And they created this basically nice computer models of how decision making works.

And I remember reading this a long time ago.

This is a 2017 paper. And if you look at the pictures below,

they sort of like explain how the decision is being diagrammed by Neurons firing.

It's really cool. And I was so convinced at the time.

But then I read this book called The Neurobasis of Free Will, a Criteria of Causation,

and he had a better idea. And I'm still convinced by his idea.

I still think it's a better idea.

But the idea is that, okay, so the Neocortex is sort of self-similar across the entire cortex.

So if you have decision making happening in one area of the cortex,

arguably it's happening everywhere, right?

Same mechanical process everywhere.

And so this guy who wrote The Neurobasis of Free Will, Criteria of Causation,

Peter, his last name is TSE, I should really just figure out,

I should remember how to pronounce the name, because I talk about his book all the time.

So the Neurobasis of Free Will is like, your Neurons are detecting information,

and the Neurons in coordination, they're detecting the basic roots of information.

And the cortical columns are taking these detections.

And so in some sense, Neurons and Neurocircuits are reacting to incoming information

by setting the criteria for what is necessary for them to react in the future.

So if you have a Neuron that, let's say that the idea is that

it's going to store a temporal memory.

It's going to store, like, okay, there's a song, and you're going to have the Neuron remember

when in time a certain beat happens, so it can predict that song when you hear it again.

And so some ideas are that we'll have spikes on the dendrite that will,

I think I'm saying it right, I think it's spikes on the dendrite that will represent the temporal memory.

And they're going to help the dendrite to resist firing, except in the case of a situation

where it recognizes a certain pattern, or maybe it works a different way.

But the idea is that a dendrite is going to store a long-term memory,

and then so it's going to look for two memories down below it.

I mean, not two memories, it's going to look for a pattern that's coming into it.

So it could be like two Neurons that are, it could be like the layer of Neurons

that's closer to the incoming sensory organ, so that could be your eyes.

So there's a V1 layer, and maybe we're talking about the V2 layer.

So the V2 layer has a dendrite in it, and it's listening to the V1 layer,

and it's looking for a specific temporal pattern.

So that means that it's going to say, okay, only if these, it's temporal,

and maybe temporal, and maybe there's a spatial dimension,

but if in case of piano, it's temporal, right?

So it's listening for a specific temporal pattern,

and maybe, oh, so maybe, so piano would be phasic, right?

Because it's not just temporal, and it's not spatial, but it's phasic.

There's something characteristic about the piano tune, right?

So anyway, so you'd have, you know, you have, at least one Neuron

would be listening for when that tune gets struck,

and so there'd be some sort of sparse distributed signal in the audio cortex

that, because we're talking about piano,

and then you'd have a dendrite that's listening for the tune to land at a certain point,

because if it doesn't land at that certain point, it's a different song.

You got to have a certain sort of, like, you know, temporal pattern to recognize a song,

even if you change the pitch of the song, right?

So your brain is doing this, somewhere in the brain,

this temporal recognition of patterns is happening, and maybe it's dendrite,

and then when that recognition happens, you see the neuron has to,

either the neuron itself has to change so that it's reacting to that pattern,

or the neural circuit has to change that neuron, so it's reacting to that pattern.

And that means that the basis of the decision-making is at the neural level or neuronal level,

because basically the idea is that a neuron is deciding, is detecting information,

and deciding whether to react based upon the criteria that it sets for when it's going to fire,

when it's going to react, and when you choose when you're going to react,

that is the microscopic version of a macroscopic thing.

And so it's also really interesting that, yeah, so maybe if you don't,

if you feel like you are too impulsive, like you're interrupting people too much,

it's a common example that we can all appreciate in the clubhouse,

as long as the person who comes in the room, and they won't pause themselves

until other people speak, and then they'll interrupt, right, it's fun, until it's not.

But the idea is, so maybe that person is having a failure or a difference in their interneuron system,

where it's overstimulated, and we are more inhibited,

and our inhibition gives us some additional agency,

and so how does that connect to our neurons?

So maybe our neurons are setting their own criteria,

and our neurosurgeons are setting their own criteria,

and inhibition has something to do with that as well, I'm curious.

And yeah, so let me just pause there, and I have more of it, I'll pause there.

I think an important aspect of what is, that was interesting,

an important aspect is lots of things are happening at the micro-architecture level,

in the sense that, yeah, individual neurons, each of them, more or less, they take some patterns,

so it's not like there's a piece of tissue, there's a piece of tissue that does the computation,

it's not like a mushy piece of tissue, but each individual neuron with their dendrites

and complex pattern of connection with other neurons can detect certain patterns,

so they distribute computation and calculations and pattern detections and predictions, as I said,

between themselves, and do detailed computations and pattern detections.

So lots of things are happening at the micro-circuits,

it means to understand this, so a circuit, a small piece of circuit, is microscopic, really.

Yeah, you said many other points, I just wanted to, um,

to clarify this part, because usually there is kind of misunderstanding that,

when, for example, signals are recorded in EEG or electrodes, like, not micro-electrodes,

but just broad metal electrodes like EEG, that level of details are not seen and lost,

so we need to see, like, each neuron can be seen like a person who is working at the material,

the organization of culprit. So each neuron has, there's lots of sophisticated and elaborate computations,

including what you described in a nice way.

Yeah, that's the point I wanted to emphasize.

Wow, thank you. Um, that was really fantastic,

because the microscopic picture of what you just said is sort of similar to the macroscopic picture that Nemento is describing,

and that's very, very interesting.

So, by the way, do you know of a paper that sort of encapsulates that?

I'll probably end up finding something, but if you think of a great one.

Sorry, which aspect? Because what you described, you described many things, actually.

Do you mean about the micro-circuit, or are you talking about that kind of inhibition thing,

or are you talking about the layers that you mentioned, or the activity,

because you touched on many aspects.

So, which you said specifically, about neuron-distributing calculations, or neuron-distributing pattern detection,

or neuron-distributing computations between themselves,

that specifically is something that, at the micro level,

that would be great to pull up in a research paper, because that sort of helps define a little bit more about what's happening

at the cortical column and micro-column level,

but also there's an analogy to large-scale things at Nemento,

the big picture things that Nemento is talking about.

So, if you can think of a paper that sort of like describes neuron-distributing calculations

and pattern detections and computations, that's, yeah.

Yeah, so basically, so this has been the main paradigm,

or kind of way of thinking in neuroscience since 40s, I would say, at least.

Even Raman Kahl, when he found out that the neurons,

like individual neurons have like elaborating complex branches and arboration and stuff like that,

I don't know if whether he postulated the functional distribution of functions across to it,

but I think even like before 20th century by Raman Kahl, I think it was probably thought of.

Oh, you were referring to sparse distribution.

Yeah, but in general, obviously to have a better understanding is better to see latest research,

like Nemento's paper actually would be the one that aims for that,

kind of culminates or like describes the kind of refined view that we have over the decades.

But I'm saying like this kind of micro view that the micro secrets are there has been like main,

actually in maybe 40s, like in the earliest neural networks,

even like artificial neural networks are based on that picture, yeah,

because in artificial neural networks there are neurons that they elaborate the weights

between the neurons are important that each weight can be more or less contributing to the computation.

So connections models that the roots go back to 40s,

which is actually almost precede the first computers.

So this has been around, but a good paper for that maybe Nemento,

or maybe there's like our books about like synaptic organization in the brain, et cetera.

So there are many things.

Yeah, I cannot like point to one.

No, I get it.

No, you're talking about, okay.

I just remember there was a book that was kind of emphasizing this on this kind of micro secret thing.

I forgot the name of the person later.

But as I said, maybe Nemento, because they try to give their work is kind of

try to focus on what is that this micro circuit in that column.

Maybe that's the level, the right level of things.

Hey, Micah, this is Leonard.

Hey, Leonard.

Yeah, can you send me or email me?

You did speak about earlier and I'm kind of cross referencing what you were asking earlier.

So I'll mention that paper about decision.

I think it was decision mapping or decision theory a little bit on circuitry.

I know it's an older paper.

But if you when you get a chance whenever just email me into my emails in the profile,

but yeah, I'd love to look at that paper because I think there's a cross correlation of what you just said,

what you alluded to and what so I was talking about.

And I won't pronounce your name, Sahel, correctly, because I'm always doing that with pronunciations.

But so my please, I beg your forgiveness, but it's a matter of multitasking, which I shouldn't be doing.

But it's a matter of the correlation, I think of circuitry and decisions and then the imprints of the human mind slash brain

or what we've kind of taking in sensory perceptions.

And so some of that I just see a cross vector there. That's all.

Hi. And so I just thought of something about the neural link and the detector in the brain.

So I think previously, all of you mentioned that the recording detector, the electrodes are recording the potential or voltage is a unit potential.

But are there any devices that detects the magnetic field in the brain? Because the current, if there is a current, there is always a magnetic field.

And so you measure the magnetic field, the strength or the vector potential of the magnetic field.

So I'm just wondering if in biology or especially in neuroscience, is the magnetic field a part of the detection or is it too weak to detect or some of the things.

Because I never heard people were mentioning magnetic field detection.

Yes, there is a method, MEG, I think magneto something, sorry, I might say the wrong...

Magneto encephalography.

Yeah, yeah, yeah, that's it. I didn't want to say something that is visual.

Yeah, that is actually very promising because it's a device that you can put on top of your head.

And it's quite a big device, it's very heavy and it's very expensive because it has to detect very, very subtle magnetic fluctuations in the magnetic field.

But this actually has been one of the promising ones that this can detect to a good precision, much, much better than like EEG for example.

Less resolution than electrodes putting their microelectroces and it means send inside the brain, but still good precision.

But the problem is that it's a big device, it's heavy, also it needs to be very close to the head, so it cannot be like used to read people's minds on the street or something like that.

But MEG is very promising, but it's quite still inexpensive.

So I suppose that precludes the possibility of just like the electrodes inserting into the brain.

So at least presently the MEG NITO in cephalography is not, cannot be, is not the MEG, the miniaturized enough to be able to insert it into the brain.

Can I correct something? So I think it can get as much as the precision that we can.

So it speaks, so one barrier is that it speaks.

But also there's another inherent limitation in the resolution of information that in principle it can get.

That's also a limitation, but in the non-invasive methods I think it's the highest resolution.

So among all the non-invasive instrumentation, this is the highest resolution.

Is that what you're saying?

Yeah, as far as I understand, yes.

I'm just speaking from a physics point of view, I don't know, because if you just put the electrode outside the brain,

and then if you have a distribution of charges inside the brain, basically close to compact region,

then you won't get the, if you just recall the potential, you won't get a unique solution for the distribution of charges.

And I suspect the, it's this, I haven't worked out the thing, but I suspect, I think, I suspect it's the same for the magnetic field as well.

So, but maybe it's better than EEG if it is outside.

So I think if you can get inside, just like the electrode, go inside, insert it into the brain,

you will get a better, I mean, if you insert the magnetic detector inside the brain,

it will get a better resolution than your own firing and currents.

So is that the case or you take it otherwise?

So let me see if I understood correctly.

So you're saying you use the, like, basically MEG, but in a smaller scale so that we can bring it inside,

so we can use it closer to neurons or inside the brain?

Yes, yes, yes, yes, that's what I'm saying.

I don't know if that's using, it might be used, but that, I think your physics intuition is correct that it's quite high resolution.

So the microelectrode that goes actually there, it has the highest resolution,

but I didn't get what you know, what was the cons, what was kind of disadvantages in your opinion.

The main disadvantage for electrodes is that it gets too narrow view, like you can see one neuron what it does, it's doing.

So the benefit of MEG is that it can map with relatively good resolution, still less than electrodes,

but from a broad range of population of neurons across the air region.

I didn't get what was your disadvantage you mentioned for MEG.

So basically what I'm saying is the MEG, the magnetic field,

basically you measure either the spin or the magnet, little magnet spin,

or the spin that generates the magnetic field in the brain or the current that's going through in the brain.

So basically, because current actually generates the magnetic field.

So the, you can, I mean, either, so there are two sources, right?

They are actually the same source, but one is macroscopic and one is microscopic.

So macroscopically you have current that's a neuron conducting the, there's a current going through a neuron,

then it will generate magnetic field, and then the others probably, I don't know about the biology,

so I am just saying in principle in physics.

So if there is a kind of a magnet, like spin, like the molecules,

the electrons going around the market, if they line up, then they will produce a magnetic field.

But most likely I think it's the current going in the brain that generates the magnetic field.

So basically, you can back out, detecting the magnetic field, you can back out the current density,

so the distribution of the current, location and strength of the current.

But I think in principle, if you just detect the magnetic field outside the brain,

and it will suffer the same shortfall of detecting the electric field outside the brain,

because basically you can have two distributions, let's say two distributions, two different distributions of electron,

but they charge, but generates the same electric field, which is measured by the EEG.

So that's why you have a, the further away you detect it, the lower the resolution would be,

and then the same thing happens with the magnetic field.

But of course, maybe the degree to which degree of the low resolution or the non-uniqueness of the solution is different.

But I don't know that the specifics, but in principle, you will have this non-uniqueness of the solutions

for the distribution of the charge and the distribution of the current.

So that's why, but that's why if you just measure from the outside, you won't get the highest resolution.

But maybe when you say the magnetic field detectors is high resolution,

probably compared to the EEG, they may have a high resolution,

but still in principle, you won't get the, you won't resolve the ambiguity in the current distribution.

So, but the thing is, this is, this will be different if your detectors inside the region, inside the region,

and then distributed as dense as possible inside the region, then you can back out the distribution of the charge

or the distribution of the magnetic field.

So that's why I'm asking about that.

So I want to comment because, so, so what's interesting about MEG is you don't need to attach any electrodes to the scalp,

because the magnetic field emerges from the brain and through the skull and the scalp without any distortion.

It's not like the skull is, is causing an issue here.

So.

I understand that.

But still.

I know.

Sorry.

No.

Go ahead.

I didn't mean to do a straw man on you, but sorry about that.

But I wanted to say that, that the MEG signal mostly reflects intracellular currents.

So what you see with MEG and what you see with electrodes is different types of activity.

For example, the radio sources that form the dipoles, the best dipoles for scalp EEG, these are not well detected by MEG.

Only currents that have a component tangential to the surface of a spherically symmetric conductor produce a magnetic field outside the scalp.

And so the spatial resolution of MEG is better than EEG, but that's because the magnetic fields are not scattered and distorted by the skull.

So that's why you put electrodes inside the skull and the resolution is massively better.

I don't think that would happen with MEG.

So in practice, MEG source localization is still not accurate because of the model that has assumptions that are overly simplified and are not adequate to represent the complexity of physics and physiology involved in the human brain under even ideal conditions.

The improved spatial resolution of MEG is insufficient to obtain information about the local circuits and the layered differential effects in the cortex or about neuronal spikes.

And the necessary requirements for revealing not only the locations, but also the mechanisms of neuronal operations.

And there's a further problem, which is that if you're thinking about the oscillations of the brain, the idea is that your brain has lots of different kinds of oscillations and these oscillations are changing all the time.

But from the big picture, these oscillations are sort of like interfering with each other so much that the brain, I may not be saying this correctly because I'm just learning about this now, but I guess it could be the case that if you have so many oscillations happening at the same time and it just looks like pink noise.

But the brain can, as a whole, seem to represent a pink noise state, but then bounce back into something that's just like the illusion of all the different oscillations cancelling each other out.

And so I mean, I don't know, like I think that the trying to use MEG to decode what's happening in the brain layer by layer and neuron by neuron might be possible, but I think that what needs to perhaps improve is basically a new computer model of a new mathematical model

and a predictive model of oscillatory changes in the brain.

And so you're decoding this really complex electromagnetic pattern from the complexity of something that just appears to be noise that has some structure to something like a three-dimensional Fourier transform where you're figuring out what are the frequencies in different layers.

And different regions because it's all going to mix together from the perspective of your sensors unless you have, like you can't use electric electrodes in an MEG machine because it's a magnetic machine.

So if you're sticking sensors inside the brain, they can't be magnetic if you're going to use MEG. And that's another sort of logistical issue.

Yeah, okay. I got a bit on the subject.

I just want to, well, I mean, I think what I was saying was a bit more specific. Basically, I tried to basically take this as a, just compute the distribution of the charges and the current as for how the current originate from the biological process.

I'm not concerned about that yet. I'm just talking about the distribution of the current.

So what I'm saying is that the, yeah, I agree that the magnetic field is not distorted much by the skull or the surface interface of the skull and the air.

But the reason for the lower resolution of the electric, I mean, to solve for the electric charge distribution inside the brain through EEG is not just the distortion of the electric field.

I mean, the, so long as we know the boundary condition or the interface condition, we can still resolve that issue.

But the more fundamental issue is that basically when you, the potential on the bounded region, a compact region, the boundary of the region or not is not uniquely determined by the distribution.

Well, actually, the other way around, not, you can have, the potential on the boundary is not uniquely back out a solution of charge distribution in the interior.

So that's why it's not about the change in the boundary, how the boundary changes the electric field, it's the outside, the nonuniqueliness of the solution.

So, so that's, that's the problem. And it's a fundamental mathematical problem. And then, so the same thing, actually, I had to work it out, but I would highly suspect the same thing happens for the magnetic field, the current.

So two distinct current distribution will give you the same magnetic field measured outside the, outside the region or on the boundary of the region.

So, so that's the, that's the problem. So that's why, how are you, that's why they will have a low resolution if you just measure the fields, be it electric or magnetic field outside the region.

And the only thing that you can, I mean, it's, personally, you can put some assumptions saying that, okay, so there's the possible distribution is the final set of them, and then you choose, choose from among these possible patterns, like a pattern matching pattern recognition exercise.

So, but fundamentally, if you don't have any pattern, no, don't have any a priori distribution of the, a priori idea of the distribution, then, then you won't be able to back out the distribution of the charges nor the current.

But the thing is, if you put this detection detectors inside in the interior of the, of the region, then you can, basically from a mathematical point of view, then you can solve for the distribution.

That's, that's what I'm saying.

I'm, I, okay, so I'll accept your physics based argument that we could get improved resolution of the whole brain, even if we couldn't, you know, do a four dimensional Fourier transform to figure out what part of the, of the brain that the holistic model was representing, like the individual

neurons or individual neurons. But I don't know, like, because we don't put electrodes on people with MEG, it's a big machine that surrounds your head. And so like getting closer to your brain means like removing your entire skull.

That's what I, I, I know what you're saying. It's invasive. I completely understand that. But I'm talking about, I'm looking into, say, the same doing the same thing as the neural link is doing inserting the electrode inside the brain.

So, so if, if technology allows, can we put also magnetic field detectors inside the brain as well?

Yeah, that's a different, I don't know. That's a different question than, because MEG is, is, is outside your, your skull. So that, so then we're not talking about MEG, but that's a different kind of magnetic field detection.

I don't know the answer. But, by the way, Leonard, I sent you, let's look that up. Let's research that. Hanson, let's find out. And maybe there's, maybe we'll discover something that already exists.

Leonard, I sent the, the email to you that you requested.

Thank you very much. My God, I got it by the way. I was just going to ask Hanson, but I don't want to exceed this thread, but you kind of covered it already, which was, um, yeah, if we're talking about electrical magnetic resonance or imaging and demonstration, and you've almost answered my own question.

Which was, there can't be invasive and noninvasive at the same time. These two will cancel each other out. So if you're looking at, I was going to ask if you're doing this in the lab versus the medical, mathematical models.

Have you been backing away and reading sort of the electromagnetic imaging and resolution from the subject and or subjects? And are you just modeling it that way? Or are you just, you know, putting forth the hypothesis, which is fine, because everyone's always thinking of what the model could be.

Because, um, I think at the end of the day, you're right, you've got to cancel one of the other out. It's neither connected to the skull or to the actual nose or the brains of the cells, or it isn't.

And what data are you reading and what's going to be your input output from that standpoint? And are you further distanced? Because earlier I was thinking quantum entanglement.

But listening more, it sounds like you're going down a path of either, you know, it's neither a physical connection, yet you're trying to, you know, do an AP testing model. And I'm just, I'm asking a question. I'm not making a statement.

No, I understand. So I'm not talking about the quantum mechanical effect yet. And I don't think we can detect the quantum mechanical effect at this stage with all the noise and all that.

Anyway, I'm talking about the classical, classical electrodynamic phenomena. I'm just trying to see, basically, I think the purpose of the electrodes sticking into the brain is to detect the charges or the charge distribution.

And then the current. So if you get the magnetic field in there, I mean, if you detect, you can detect the magnetic field, then you can deduce the current as well. So that's why I'm asking the question whether there's a magnetic field detector, and then whether you can put it inside the brain as well.

So there is, there are electromagnetic field detectors. They're about the size of a walkie-talkie and they're called EMF meters. I've never heard of anyone putting this inside the human brain. This might be a really good idea.

You should jump on developing a product that does that. You'll sell to millions of neuroscientists who will be very curious, I'm sure.

I mean, if one can miniaturize the magnetic field detector.

Yeah, I guess it's the biggest barrier for our understanding, for more understanding of the brain and also better interfaces is better resolution. Any method that can make that possible, any method that can improve that would advance this science of it also, the application, everything.

So you say good. And I think in principle, people in physics, especially the experimental physics, can help that. So I think the next breakthrough would be physics. So I think thinking about those ideas would be a great idea.

But MEG resolution is few millimeters, spatial resolution. Things smaller than a few millimeters cannot do so. But in terms of temporal resolution, it's milliseconds.

And another point is that MEG, in principle, cannot be more than MRI. Because MRI is also magnetic, but it's active, so it's resonant. So MRI, if MRI, they could achieve less smaller than a millimeter box size, but it requires high magnets.

Again, the idea of bringing these now closer to the surface, whether invasive or non-invasive, any method that can show us higher resolution of what happened there, or a broader view, but with higher resolution would make very, very big breakthroughs.

So many Nobel prizes are how you can produce and discover by physicists.

Yeah, I agree with you. And then the NMR machine, like you said, I just want to emphasize that it's different from the magnetic field detector, because NMR machine is actively putting a magnetic field from outside.

And I'm exciting the nucleus and all that. So it's different from recording the current, recording the magnetic field.

My argument was that, sorry, my argument was that the resolution cannot, MEG cannot be more than like MRI or...

That's not my only argument. Yes. I'm just trying to emphasize that point. I agree with what you're saying, because the NMR is outside, imposing an active magnetic field, which is actually very high magnetic field.

And then actively exciting the nucleus inside the body. So that's different from the source. And actually you get the radio wave, not the magnetic field, you're not detecting the magnetic field.

So it's different from the detecting the magnetic field generated by the current inside the brain.

Actually, I want to pose another question to Sokai, or is Sohai, and how the people who may know about this.

The cable field for neural signal transmission is just more about the, it is just the electric potential, basically the capacitance and then resistance and all that.

It doesn't include any inductance, which is basically the magnetic part of it.

And I'm just wondering, is it because the magnet effect is so small, or the theory doesn't just ignore that factor?

So, first of all, I think we should call it not cable theory, but conductance-based modeling. Cable theory is a subset of that, but the conductance-based modeling, which means, like, Hodgkin-Hossey and so forth.

Yeah, but yeah, the electrical field, I've seen a lot of studies, and actually the electric fields were related to my, like, there was a project that was working, modeling them as well.

But magnetic, I haven't seen, I haven't, like, looked for papers in that, but I think there is, there might be a kind of special constant, or time constant, that's different.

I don't know, that's an interesting question, and I'll look it up.

But I doubt that they haven't looked at it. Let's look it up before saying it.

So, I did find, I was just going to say, I did find a project while y'all were talking, I looked it up.

And so, there was the National Science Foundation awarded some researchers money to basically create a wireless neuro-sensing diagnostic system that's basically, they call it an EMF biosensors that would be implanted neuro-sensors.

And the goal was to detect brainwaves, and they were awarded something like $800,000 to do this research.

The goal was to, the goal of the project, this was in 2013, December 2013.

The goal was to develop a fully passive intracranial sensing system.

This novel electronic brain-machine interface uses an implantable miniaturized neuro-sensor based upon microwaves scattering, backscattering, miniature textile antenna, and RF circuits on flexible polymer substrate.

It has unique properties of only minor heating, thus minimizing image injury and trauma to the brain.

And so, I'm going to post this on Twitter a little bit later.

But yeah, if you just look up, if you type in physiological studies of brain signals using a wireless neuro-sensing dash diagnostic system, you'll get it.

Now, actually, this sounds a lot like furoxa microwave imaging.

Because here's what they said, they basically said that this novel electronic brain-machine interface uses an implantable miniaturized neuro-sensor based upon microwaves backscattering.

So, if you type into your browser, you type in furoxa, it's spelled F-U-R-A-X-A dot com.

You'll see this is called furoxa microwave imaging.

And I did connect the idea of furoxa microwave imaging being potentially connected to, you know, basically EMF.

But it is, right?

Yeah, so you can check out those two things if you want.

Are you going to put it on your Twitter?

Yes, I will. Let me see if I can do that right now.

So, furoxa microwave imaging is really exciting.

So, I often mention, like, you know, Neuralink and I'll talk about open water and EIT plus deep learning.

But furoxa is the other one.

Furoxa is the other major sort of like new medical imaging technology coming along.

And so, it's really interesting that, like, I didn't, like, when I looked this up and I was like, microwave technology from an EMS sensor?

Oh, furoxa, that is a major brain-computer interface technology.

So, let me just tweet this one.

Yeah, so I did that one. Go ahead.

I guess I'll just read it, but I guess if you have a quick answer, then I'll just ask the microwave.

Is it saying that using some kind of a fiber going into the brain and then generate basically a wave guy and then generate or shoot microwave from inside the brain?

Yeah, I haven't, I don't know.

Okay, never mind.

But let me know when you find out.

The first time I read the paper, I just, I read the paper and I was like, I don't understand anything.

That was years ago.

So, I guess I have to come and go because of my work study.

Yeah, I gotta go.

I also have to go.

I just thought of my question.

Wonderful.

Leonard, the room is yours if you want to take it over because we're all leaving.

No, I know, I know my, let's do this again.

I love it.

It's 12 for me at night.

I need to go as well, but I think we should maybe schedule more of this for him in the future because I love the thought process of where are we going with all of this.

He touched upon something else and I think, but I don't want to keep the room going longer.

I think by the nature of microwave, it's going to be external, but in a very local distance within like a two feet perimeter.

But that's just my thought.

I'll read the paper when you put it on Twitter or something when I find it myself.

But yeah, thanks for hosting.

I think we should do this again.

And, you know, thank you guys for coming in everyone.

And I'll post additional links in the Facebook group, which is called the Facebook group has the same title as this chat.

So if you look on Facebook and you type the same, the same chat you're in right now, Neuralink, Numenta, Neurophysics, Nerve Gear, Happy Hour, that's the name of the Facebook group.

And I'll post links there too.

And I'll send some links.

I'll email some links to Leonard.

Good night everybody.

Good night.

Bye.

Bye.

Good night.

Thank you.