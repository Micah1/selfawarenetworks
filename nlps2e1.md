Neural Lace Podcast Season 2 Episode 1 Neurohackers

Thursday August 2nd 2018
On today’s podcast Micah Blumberg, Organizer of NeurotechSF part of NeurotechX micah@vrma.io neurohaxor.com selfawarenetworks.com

Jean Rintoul of MindsEyeBiomedical.com jean@mindseyebiomedical.com

Presented by NeurotechSF
part of NeurotechX.com

Listen here: https://www.youtube.com/watch?v=aexQwTpOwYc

Audio Transcription by OpenAI's Whisper

Hi, everybody. This is Micah Bloomberg. I'm an organizer for Narotech SF, part of the

Narotech X, and I'm here with Jean... How do you say your last name?

Rintel. Okay, Jean Rintel of Mind's Eye Biomedical. And we're creating a podcast today called

the Naralyze Podcast Season 2, and we might just shorten the name to Narohackers, maybe

just for simplicity. So this is Thursday, August 2nd, 2018, and we decided to just record

our voices while we have slides, so you're going to be able to see on YouTube our slides.

And so this is... That's Jean to the left, and that's me. That's Micah to the right.

So for the Narotech Meetups that I organized, and also San Francisco Virtual Reality Meetups,

I started a project that I'm just calling Narohaxer. You can join... To join this project

online, you can visit narohaxer.com, and then you'll be invited to our Discord. You can

also join our Facebook group, and the link to that is self-awarenetworks.com. The group

is called Self-Aware Networks, Computational Biology, Naralyze, because I guess the sort

of like maybe dream of Narohackers is Naralyze, or the next generation brain-computer interface,

you know, of many Narohackers, and Neuroscientists. Narohaxer is a project that combines tech

and novel ways to create new technologies and new applications, to create new kinds

of biofeedback data visualizations and new kinds of neurofeedback experiences, next generation

nerve correlation for biomedical research, and new user interfaces that predict user

choices, emotions, and life dislikes, and social experiences in AR and VR. So, this

project, Narohaxer, has overarching objectives combined, brain-computer interface or brain-machine

interface devices with WebXR and XR devices. XR means augmented and virtual reality, and

also, you know, heart rate tracking, eye tracking, deep learning, AI, and 3D point-clad neural

networks. A project exists in part because we know that many AR, VR devices are coming

to the market with built-in biosensors, and let's go to the next slide. I believe the

next generation of mainstream brain-computer interface devices could be an XR device,

like AR or VR glasses, that incorporate biosensors, such as EEG, ECG, or EMG, and also eye tracking.

But there's potentially lots of new sensor technologies coming to the market that could

be integrated into these wearable BCI XR devices, like open water by Mary Lou Jepsen, open NIRS,

open electrical impedance tomography, open acoustical impedance tomography, few rocks

of microwave imaging, aquaverts, AI wearable, and there's lots of work being done to combine

deep learning neural networks or other neural networking technologies with brain-computer

interfaces. So that's one of the things that our group is focused on. The VR and AR devices

today may be considered a new type of brain-computer interface already because they do collect

data from the user, like the user's head and hand position or orientation, and some of these

headsets include microphones and other sensors. Increasing though, they'll include sensors

traditionally known for biofeedback, such as EEG, EMG, ECG, et cetera, and eye tracking.

Machine learning, which includes deep learning neural networking and computational neuroscience,

can augment BCI data with data visualization, the prediction of emotion intention, and even

image recognition of what the brain sees from a low cost. There's a reference. You can look

up low-cost EEG data from... Just follow this link right here that you see because they

get all the slides. It's kind of cool. The future of XR devices, I already said this,

will integrate biosensors. NERBAL is an example of this. They're integrating EEG with HC5

and their technology is based on a company called wearable sensors. MTEC is a company

that's creating an EMG muscle face tracking faceplate. NEES Interaxon is a company that

is creating a faceplate that tracks brain and heart data. LUXID is a company that tracks

EEG and eye tracking. They're creating a VR headset that does both. Eye fluence combines

eye tracking with AR devices and AI. If you predict these are intention, and now they're

a Google company, I'm sorry, they're an alphabet company. MicroDOS VR is a company that I'm

a brand ambassador for and we combine procedurally generated eye candy and virtual reality via

Unreal Engine with the NEES faceplate with its EEG and ECG sensors to create next generation

or feedback, to create a level of presence and immersion in VR. This is a company that

I'm an advisor for. I advise them on machine learning and new technology. They're creating

an all day wearable technology that it uses next generation EEG sensors that have improved

noise cancellation, that they have massively improved noise cancellation. They get a huge

like a billion times improvement in sensitivity and 100% noise cancellation. There's some

huge numbers like that. With machine learning that we send the data up to the cloud and

it returns to the API in real time, a prediction of whether the user likes or dislikes. The

initial product is called the decision maker. It's really cool. People don't have to wait

for these products to arrive. None of these products that I mentioned are currently available,

but they're coming. With 3D models and 3D printed parts, neural hackers can combine

existing VCI sensors with VR headsets. This is a picture of the open VCI and the HSE Vive.

Both of these are easy to acquire. On July 29th, the neural hacks project made significant

progress. We combined Meetup, Neurotech SF and SFVR at Noisebridge. We connected the

Brainduino EEG with WebXR. We produced a video showing actual voltages. You can find the

link inside the Discord, which is at Neurohacker.com. This is the last slide is a picture of the

Brainduino EEG board on the upper left. To the right of that is the Oculus Go. Below

that to the left is the concept picture of WebVR. To the right of that is a picture of

the actual in that last picture on the right. That's WebVR on the left. That's our neural

signals coming in on the right. You can find the video over that. Let's go ahead and go

to the slides that Gene brought. Gene, I first met Gene. Before I met Gene, I saw this talk

from – I was at Noisebridge and people said, you have to watch this talk from the Creative

Communications and Chaos Communications Congress on open electrical impedance tomography.

That was a talk that Gene was giving in Germany about this new technology that has enormous

promise to democratize research because it's low cost. It's do-it-yourself and it's open

source. Let's go ahead and put the slides up and then present on this device. Go ahead,

Gene. If you want to advance the slide, just tap the screen.

Just tap the screen?

Yeah, or swipe.

Okay, great.

I'll just –

I'll try to ask a question as soon as I think about it.

Yeah, thanks, Michael. Looting.

Shoot. It should be preloaded. I downloaded it for offline viewing, but come on.

Okay.

There we go.

Oh, yeah.

Whoops.

Hmm.

Hmm.

Yeah. Basically, electrical impedance tomography is an interesting technology. It's sort

of – nobody really has heard of it before. It's relatively new. I became interested

in it a while ago when I was looking into biomedical imaging and what techniques we do have, what

we can do, and why we can't easily hack with many of them. For some reason, my slides seem

to be a little bit –

I doubt what it is. Let me see if I can get them. Sorry. I'll keep talking, but –

Yeah, so I'll just start –

They seem to be working like this.

Okay, that'll do.

Okay.

So basically, it was like, wouldn't it be awesome if biomedical imaging was really easily

accessible? It would be great. We could do imaging preemptively. It would be wonderful

for healthcare diagnostics. If it was so easily accessible that a lot of us could add to it

and prove it, basically have democratized imaging, imagine how fast healthcare would

move forward. It would just be great. So that was like a beginning motivation. If you look

at the U.S. on healthcare, the operating theater hasn't actually changed much in 30 years,

which is a funny thing to say because we've had all these other amazing technological advances

like the Internet, but still the operating theater is a little bit slow behind the 8-ball.

So we also spend a lot on healthcare, yet the change, the innovation is really hard

to see. So the question is, how do you create innovation in this really interesting place?

We want to learn more about the body, the nervous system, and how we can interact with

technology. So I'm going to go through an introduction to what we've got now, which

is the MRI, the CAT scanner, the ultrasound, and the EEG. They've all got different pros

and cons.

There's several more, too.

Oh, absolutely.

Yeah.

All right, keep going.

Yeah. And they've got different kinds of resolution and different kinds of tissues that they've

got looking at.

Should we try to look bigger or would you prefer like this?

That's fine.

Okay.

Yeah, they need lots of maintenance CAT scans. You use X-rays to do those. They're actually

ionizing radiation, and they cost a few million dollars. Again, you're probably not going

to have one on your desk at home and fiddle about with it. An MRI is absolutely beautiful,

amazing physics. And there's also functional MRI and diffusion tensor imaging.

And there's all these variants.

Versions of different new improvements on MRI.

There are. And they all get different things. And it's like, wow, that's awesome, but it's

$3 million and patient. I saw this article the other day where there's like patients

in Wisconsin, and they're getting charged $1,600 per MRI scan.

That's not for the machine. This is just for a single scan. And I was like, oh, it's interesting.

That's suddenly not something that you would just have to fiddle about with, is it?

Yeah. What's the cost of MRI? Is it something like $1,000 per scan?

It depends on the hospital. These guys must go on to my charging $1,600.

Yeah. I thought that was kind of normal, like that price range.

Yeah. So that's a lot. Just for the scan.

Yeah.

And in other countries, it can be much cheaper. So it depends on the medical system. So you've

also got ultrasound. It's a bit grainy, but it's pretty awesome. It's definitely cheaper.

And EEG, which is a bit different again, it gets different stuff, which I'll talk about

soon, but it's cheap. And there is an open source EEG, which is open BCI. All of these

different imaging modalities have different applications that they would be best at,

ranging. You could have tuberculosis where you just need a static image, or maybe if

you need something that has fast time resolution, like fetal heart rate or deep brain stimulation,

that would be a different kind of imaging again.

So a lot of the motivation here is, can we find something that's more useful than EEG?

Yeah.

EEG, I think.

So what's wrong with EEG?

Yeah.

Let's talk about it.

Yeah. So EEG is great. And we should use it for all the information that we can possibly

get out of it. But it's limited. It measures info that's mostly just at the cortex. That's

like at the edge of the brain.

Yeah. Just on the surface, basically the surface of the brain.

Yeah. That's not the whole, that's not, that's not like, getting...

And it's mostly frequency information. Not very good at spatial.

That's right. That's right. So it has really poor spatial specificity. So that's a problem.

And interestingly, because of what it's measuring, it's actually measuring these things called

post-synaptic potentials. That's like the signal between the synapses and it's not actually

the signal generated by the cell or the output signal, which is not as an action potential.

In neuroscience labs, there are really things like patch clamp electrophysiology where you're

all about measuring these action potentials. And they tell you a lot about the actual neuron

firing time and timing and the chemical processes.

We can't do that with EEG.

No. You can't do that with EEG. But you do get these post-synaptic potentials, which

aren't the same, but it's still something. But it's not the actual biochemical, like,

oh, the cell, this output thing, right? And the other weird thing about it is you only

get perpendicular signals.

Yeah. It's like if you had a closed football stadium that was closed to the sky and you

were on top of the roof of that stadium and you tried to record the audio coming from

the roof, which may be just a roar of indecipherable sound. And you tried to predict what was going

on inside that stadium from that sort of like bad sound outside the stadium. That's like

what you're doing with EEG.

Exactly.

It's really massively limited in terms of its capabilities.

Yeah. And the perpendicular signals is an interesting thing because the potential difference

that you're measuring with EEG polarizes the cell. So that means you're only getting this

information from cells oriented a certain way, which means that the information you're

getting isn't a true representation of the information transfer that's going down below.

Right.

And if we really wanted to create the best BCI, we want to know in depth what is really

happening.

Yeah, yeah. That's right.

We need that information.

We need to go, the best BCI has to go way beyond EEG.

Right.

Totally not disputing that. Absolutely.

Yeah, but EEG is great for what it is and we've done, you know, there's great research

going on. So anyway, that brings me to this talk, which is about this other little-known

imaging modality called EIT or...

Yeah, which has electrical impedance tomography.

That's fine.

Yeah.

Which has massive improvements for do-it-yourself spatial recognition.

Yeah, it's got a few different improvements. It's a different kind of technique than EEG.

Though it still uses electricity, what it does is it sends very small AC currents through

at different angles. And based on the material that's in its way, the currents will actually

take slightly different paths.

Yeah.

So...

If they're trying to pass through fat or through water, it's going to pass through different

speeds.

Yes. Yeah. Or take a different route around the cells.

And so you have like this sort of like sensors around a bowl. So if you look at the left

lower picture that we can zoom in here, you have this bowl with all these electrodes around

it and these vegetables in there. And one at a time, these clips which have electrodes

that can transmit and receive signals will fire and all the rest of them will record

when they receive the signal. And then the next one fires and then the rest of them act

as receivers. And through the firing and receiving of those signals, you can plot out

what's in the dish between the electrodes. Or you can plot out where... You know, you

can plot out the... How would you describe that?

The locations of the vegetables.

Yes. I would call it the tomographic reconstruction. Yeah. Of the image.

Of the image. Yeah.

Which is kind of exciting because here you're getting spatial information, not just from

the edge where, say, the cortex might be of the brain, you're getting information from

the middle as well, you know, maybe the amygdala or emotional centers. But right now, we've

got it in a tank. I've actually got it set up here in a tank as well just for the hell

of it.

Yeah, you've got the whole thing down.

Okay. So, yeah. So currently, it's been used for things like lung volume, muscle and fat

mass, hemorrhage detection. And there's also some research work for depth of anesthesia

via action potentials, which is all sort of interesting and why I would do what's good

about it, what's bad. I mean, there's a few things that are quite obviously bad. Nobody

knows about it. It's bad. Maybe we're trying to do something about that.

Spatial resolution is limited somewhat by the number of electrodes. And there's some

assumptions made in the reconstruction algorithm, which would be better if they weren't made.

The advantage is it's super non-invasive, non-ionizing, making it safe. It's compact

and expensive. There's better source localization than EEG, good for functional imaging. So,

although the spatial resolution might not have wowed you as much as an MRI, this is where

I want to just remind people about what the first MRI scan looks like.

Okay.

You see on the top left, that's a thorax or that's like the lung cavity. And you can

sort of see it's not as amazing as the scan underneath it, which is what we have today.

So an early EIT scan I've got on the top right with a question mark saying, what is

the future? I don't know. But here's an open EIT project. So, yeah, it's open source. You

can go to GitHub. It's under the mysterious name of open EIT. And you can sort of see

everything just sitting there. And hopefully the readme is pretty easy to install and stuff.

To sort of like go through a little bit how you get an image back from like sending these

cards through. So you send signals through from different angles. And they'll be stronger

or weaker based on what material they go through. And when you add them all up together, you

kind of get a rough image. And that's the most basic way to do a reconstruction.

You can also do 3D reconstructions. There's some people that you're aware of that are

combining multiple layers or multiple X, Y, and Z rings.

Yeah. Yeah. So you can do it any which way. I'm just showing like 2D examples because

they're actually the easiest. You can absolutely do it as you mentioned in 3D. And you create

what you need to do is you need to know where the electrodes are in your 3D space.

Yeah. You need to calibrate their locations. Yeah, exactly. I think that's where the ARVR

part. Yeah. One aspect of where... Yeah. One idea that we're hoping to get help

with is we want to create an AR application. So right now you can use AR to measure the

distance between two points that you select. Like you can measure the length of your iPad

or the length of a table. But you could potentially create an application that would recognize

with artificial intelligence, recognize your electrodes, and plot their position in space.

So if you have the position of your space, this would be important because if you're

putting... Everyone's skull is a different size. So if you're putting an EEG cap of open

electrical impedance sensors like a... Could be like a beanie cap of sensors on someone's

head, it's going to be... Every sensor is going to be in a different place every time.

So if you could run a phone around their head with augmented reality and machine learning,

you could recognize the positions of... You could classify the sensors, recognize the...

Do object segmentation where you recognize all the pixels that belong to those sensors,

and then plot their exact XYZ position in space so that you have that in the computer

so that you can take the data from sending an electrical signal. When you're doing...

Oh, electrical biggest demography, you're sending the electrical signal and the rest

of the sensors are capturing it. So you'd be able to plot which sensor received which

signal and its exact position in space, and from that you could derive the contents of

someone's head in a spatial way and create a 3D point cloud of the data you're capturing

from inside their skull. So we really want to work with people to make this augmented

reality application. Keep going. Yeah. Because right now you're putting your sensors around

a ball just to solve the problem of knowing the original sensor position so you can calibrate

what's in the ball. Yeah. So this is what the devices look like now. There's two. There's

one. On the left is an 8 electrode version, little PCB with a little flex electrode array

to do a reconstruction. I think most of the images... I've got a few images of that one

working and I've got this 32 electrode one working now which can get much higher resolution

than the 8 electrode one. If you've got a few questions about safety, you might be worried

about injecting small currents through your head because of TDCS and TACS. So TACS is

in the upper milliamps. We're putting up our cast, by the way. So it's up at milliamps

and then you've got... When you do EEG, you actually send currents through the head as

well to measure whether or not the electrodes are making contact and that's in the region

of nanoamps. And in between that is microamps and at the lower end of the microamp range

is where EIT fits in. And it's actually within the established guidelines for safety, safety

on humans which is... So we're not going to cause burns like TDCS is going to cause?

No. Not even close. Not even close. No way. That's good news. Yeah. So that's nice. So

this is what the opening IT dashboard looks like. So it's sort of configurable. Lots of

buttons there. So this is the EIT electrode version and you can see here as you move this

cup on the top left around anti-clockwise you can see the reconstructed image is moving

around and clockwise too. Just like go from left to right and to the next level. And again

that's with the EIT electrode one. This is the 32 electrode one and you can see you can

get much higher resolution with that one. There's two cups sort of placed in there and

you can see their shapes reconstructed in the bowl which is sort of interesting. I think

we actually... I don't know if we can... Yeah you guys can. We won't get sound though. Maybe

I have to hit play first. I don't know. Yeah. Let's see. Present on this device. Slow down.

Here we go. It got it working earlier. There we go. So this is just a demonstration of

the... Is this actually pulling from YouTube? Yes. So it's not actually on this device.

So that might be a problem. Well I mean it is connected to the web so let's see. Oh no

no no it's fine. Yeah. So this is with that little EIT electrode version. So they can't

hear that so just go ahead and talk over it. Yeah. So you can see I'm moving the shut

glass around and clockwise in the bowl and it's doing the reconstruction in real time

and you can basically see that on the screen. So that's basically what I wanted to show

with that one. I don't know how do I... Do you want to go back? Swipe left or swipe right?

So obviously the resolution leaves some wanting but we've got this 32 electrode one now which

improves that a bit. This is the other super cool thing that it can do actually. So obviously

we have different materials and they're changing and I had all the time like blood flow for

instance. How do we differentiate those changes? Okay. Yeah. How do you differentiate blood

flow? Okay. So every different material has a dielectric spectrum so if you run different

frequencies through it you can actually recreate a spectrum and just to sort of show that concretely

I did it with an apple and a sweet potato and then on the bottom right here you can

see this oops you see the spectrums. So that's on the right side of the image. You can make

sure maybe you can zoom in. Oh cool. Yeah. So you see this sweet potato has a spectrum

that's notably different from the apple which is notably different from the water. Why that's

cool is if you can differentiate them visually like that you can differentiate them with

machine learning even better. Okay. And I did something sort of very basic on the left

differentiating the different objects just based on their dielectric properties what's

inside them. Yeah. Say you wanted to do things like tumors, lung volume, you were searching

for a coin inside your stomach. Are you starting to write your own machine learning programs?

Not just yet. Okay. But I think it's ripe for machine learning. Yeah. So yeah. So I

think that's like an interesting aspect of the data and also I'll just go back. So this

is like it makes it hard to visualize as well because whenever you're doing a visualization

you're actually only visualizing a single frequency. Yeah. So if you can just visualize

it like put it into machine learning because it's so high dimensional you can kind of get

all that information can yield in one human's just like that's a lot of information for

a human to pass or see or whatever. Yeah. So it's interesting because you could point

a camera at the original data if you're looking at a dish of vegetables and stuff and train

it on looking at the sensor output and the original data and train it that way and then

just feed it new data. The new data could be like from inside human being or something

and see what it can predict using the previous training data on vegetables and stuff. It's

one possible way to train it. Yeah. Yeah. Yeah. Or you could sort of train it on sort

of tumor samples as well or something like that depending on what you want to look for.

Yeah. Yeah. So in the in papers the maximum spatial and temporal resolution is 200 micrometers

and less than 2 milliseconds. 200 micrometers. So that means you can really focus on a small

microbiology. You could identify cancer cells. Yeah. Yeah. So they're easier to identify

because they actually attract a lot of blood. So they have a lot of blood around them. Do

you know of anyone doing that? I think some there's papers where people have been trying.

The skull is a problem in this paper here. Yeah. What they do is they stick it onto the

outside of the rat's brain. So they go through the skull. Yeah. And they stick this little

flat array and it's called an intracranial surface array. It wraps around the rat's

brain. Yeah. Okay. Exactly. And that's what they're doing the reconstruction with. Yeah.

So that's the sort of limit of what people have got to with it so far. But I think also

just tapping will send you to the next slide. Yeah. So medical imaging. That was something

that like I talked just talked about neuroscience option. But if you remember there's actually

four billion people that don't have any access to any kind of imaging and that's interesting.

So I saw this project a couple of years ago at an NVIDIA conference where a researcher

was pointing the camera at someone's eyeball and then using deep learning. It might like

a really focused in zoom of their eyeball and then focusing deep learning on locating

lesions in their eye which would help diagnose a whole bunch of images. I mean a whole bunch

of illnesses. And so there's all sorts of applications that we're seeing where deep

learning is helping to create small objects that you can be taking to the poorest places

in the world. And so deep learning combined with medical imaging can help really make

active medical imaging for medical diagnosis really cheap and really available everywhere

in the world. So doctors without borders or especially I don't know if anyone from doctors

without borders contacted you. No, not yet. But I'll be interested in talking to them.

But yeah. So just imagine if like do you remember the cell phone towers how I think in the developing

world they kind of leapfrogged all the infrastructure that we've put in place for our phone lines

and they just went straight to cell phones. Yeah, they never built out telephone lines.

So what if these 4 billion people that don't have an MRI because it's ridiculously awesome

but expensive. What if there was like a cool thing that you could do where you have a cheap

kind of biomedical imaging and then you use the best machine learning on it and you can

do just as well. Yeah, just as well or even better or better MRI and so maybe the developing

world never sort of like never roll out these expensive MRI magnets. Maybe not. I just I

mean I don't know. It's just like it's a good it's an interesting idea. Yeah, these things

happen sometimes. Is the world ripe for that kind of thing. This technology could potentially

change the world for the better. Yeah, that's an idea. That would be nice. Yeah. But like

so I did this. This is this graph again of like applications. At what point are we even

useful? I'm just trying to be like super realistic. Okay. I think in a conservative estimate.

And you know, I don't think you need that much resolution. So like here's a easy one.

It would be water on the lungs or otherwise known as pulmonary edema. If you look at this

diagram you can actually see on the top right a graph that has a blue and a red one from

each lung. You can see one lung has a much lower air volume than the other lung. There

you go. That's your information right there. Bam. You can get more subtle on it. But that's

basically what you need for pulmonary edema. Okay. Here's like something else that it can

get if you put it on your chest. So this is like the breathing and expansion and and contraction

of the lungs. So you can sort of see this lower frequency signal and then you see it

like a like a faster periodic signal, which is the blood flow through the valve, the heart

valve. So again, this is different from ECG. What it's measuring is the change in the dielectric

materials. So you've got a change in materials going on to give you the heart rate and you've

got a change in materials going on to give you breathing because it's like getting filled

up with air and then emptying out the air and then blood is pumping through the lung.

You've got lots of different materials changing. Oops. Yeah. So, so this is the 32 electrode

system, which is just sort of like getting there. I showed you an early picture and obviously

GitHub opening IT is a good place to go. Thinking about like next steps and directions and how

do we make this thing even better? How do we really make this an MRI but cheaper and portable

and ready to go? Okay. So here's an idea. Is it even possible to like equal the imaging

of MRI? It would be different. It gets different info. Different info. Yeah. And as you can

see on the bottom right is what happens when you get 8, 16 and 32 electrodes. Okay. Looks

like like an exponential increase in definition. Yeah. Yeah. So you get this increase in those.

Yeah. Exactly definition. You could also combine it with other imaging modalities. So you don't

want to, I don't want to eliminate MRI if you've got one hanging around and you should

use it. Microwave is up and coming. That's interesting. I hear there's a few different

companies working on optical as well. Yeah. Like Infrared. Yeah. Infrared and Infrared

is another kind of imaging but it has pros and cons also. And it's one of its cons I

believe is the depth with which it can image. Okay. But it might be able to get good information

again at the surface and sound. Acoustical. Acoustical. So I guess the point here is why

not combine the modalities. We can do math. We can use machine learning on this information

and we can get more info out of it. I saw someone, one company was combining TDCS with

EEG and I thought that you couldn't do that because I tried to do that and like the TDCS

messes with your electrical currents and then so EEG is trying to measure electrical currents

and so I thought you, somehow they pulled it off. So I guess it's possible I guess to

combine anything if you understand it. If you understand it. Yeah. Yeah. Just as long

as it sort of doesn't violate any sort of rules of physics in any way. Yeah. I mean,

so electrical and penis tomography, do you take turn between the different sensors? So

I guess you could take turn with capturing your EEG signals or whatever other kinds of,

you know, if you needed to. Yeah, you could. Yeah. Yeah. You could absolutely, you could

absolutely actually, you could definitely do that. People have in fact. Oh, they have

come back. Yeah. Yeah. There's no problem doing that at all. Okay. So totally doable.

So doable. It's done. Okay. Yeah. So how do you get to a better spatial reconstruction

here? Right now, the reconstruction assumes a resistor network. Now the body isn't a perfect

resistor network. It has like capacitive elements. Every cell has a capacitive elements. So

perhaps we shouldn't make all these assumptions and maybe we should just use like that these

test tanks and create some big neural network by gathering data from them and basically

recreate the solution from just from information and data instead of making all these assumptions

in the physics. That's one idea. Right now, you can improve the time resolution. You can

detect ion activation and nerve signaling. That's what's really cool about it. So at

a slower time resolution, you're measuring things like blood flow changes like I showed

you with that heart rate plot. Obviously, you can do the blood flow in other parts of

the body like the brain as well. But at faster time periods, going back to this EEG question,

you can actually measure action potentials. Wow. And that's kind of like the golden, the

golden goal in a lot of ways. For nerve care. Yeah. I'll just go back. But obviously, you

know, there's a signal to noise problem. Do you know about neural dust? Yeah. The tiny

little MEMS implants. Yes. You inject like magnetic dust and then you can control it

with electromagnetic waves outside the person's brain. You can stimulate those and they cause

nerves to fire. It's just super cool. Yeah. So there's a lot of very cool technologies

going. I think I seem to sort of be really interested in these non-invasive ones. Right.

And partly, I think you can make a good consumer product out of that. Yeah. Not a base of nerve

stimulation. Yeah. So that's what sort of attracts me to VR is because, you know, your

eyes are like this high man with gateway into your brain. Oh, they are. And so if you put,

if you're basically, when you're wearing VR glasses, you're creating massive stimulation

of the nervous system by immersing somebody into some alternate world that you can control

the shapes, you know, and so that's just, that is, you know, that's one gateway into

writing to the brain. So we're talking about mostly reading from the brain, but, you know,

keeping, there's so many different ways to stimulate the brain and neural dust is one

way that there's, you know, people, what do you, what do you say in terms of like what

technologies you're excited about for stimulating the brain? Right functionality. That's your

next slide. And point it back. Yeah. Same wave length. Well, same part. So does, so does,

I'm just sort of, I'm just like pointing at the image of the wave. Yeah. And yeah, funny

you mentioned, right? Functionality. Yeah. Yeah. So there's like, you could do it with

neural dust. But that again, that's an implant that requires yeah, injected, injecting dust

into your nerves. And very inefficient. And that's fine. I'm happy for other people to

test that. However, so many great wireless technologies that why shouldn't we take advantage

of those first? And I think the real neural lace nerve gear product is going to have a

wireless transmission into our brains. Yeah, which I, like, I'm pretty sure. Yeah. Like,

I think you can do it. There's actually been work to prove you can do it. Like, like, just

in cell in 2017, this paper came out, which is on this slide. It uses beat frequencies.

So it sends through two signals that have slightly different frequencies. Binaural beats.

It's sort of like binaural beats, but a little bit different. Is it sending an electrical

beat? Yeah. Okay. So it's electrical simulation. Yep, it is. And it creates a large amplitude

at one particular location. Okay. And then it was shown in this cell paper written by

this guy called Neil Grossman, that that the neurons in that area actually trigger coherently

with the the beat frequency. Okay. So they start firing at the same frequency of the

beat. Yeah. Which is what we've seen with other kinds of binaural beats stimulation.

Or if you like expose someone to audio or visual binaural beats stimulation, eventually

their neurons will begin to oscillate at the same frequency of this pattern you're sending

into the brain. Entrainment. Brainwave entrainment. Yeah. And so they showed in this paper that

it's what you're saying is that they've put, they've used electrical brainwave entrainment

to induce a very local response from a neuron to start pulsing at the frequency they were

sending it. Yeah. And I mean, that's even better. That's almost better than neural

dust, really. Because neural dust allows you to make the neuron fire, but this allows

you to make the neural fire with a frequency. So that's, yeah. So it just depends what like

what you want to do. Like there's all these cool things like deep brain stimulation for

Parkinson's disease. Yeah. And that's been shown to work, but you have to like stick

this sort of thing into, yeah, into your brain, which is great. If you have a tremor and you

need that. Yeah. Yeah. If you want to have like, if you want to install something into

your brain, yeah, that's one. But this is not invasive. You might want to just try something

out for a day. And they use what for this image shows us they use four electrodes for

stimulating one neuron. Yes, that's correct. Yeah. I think they've got a common mode rejection

scheme going on there. So like it's a way to control the amplitude of the signal more

exactly. Let me have to like, yeah, go ahead. Yeah. So I think that the writing field is

open. I'm not trying to say that, you know, you have to do with the beat frequency either.

There's other these electrodes work for that or would need a different kind of you could

use any kind of electrode to, to do this with. So there's other techniques as well, like

phased array and beam forming techniques that are often known in reader. Yeah. And I just

want to put that out there that, you know, the, you know, the, the, the Air Force has

all these great techniques. And they should still work. Okay. And I believe there's a

few patterns out on the phased array with current forming through the brain already.

Okay. Just to put that one out there. So people have got that working. Okay. So that's very

interesting. We can basically use all of these, these techniques and control in a very localized

and specific way. And that's really sort of like interesting in terms of bioelectronic

medicine, where we're going. And I just want to sort of like, I went to Edinburgh a while

ago and I was like looking at this like surgeons hospital. And like surgeons once use saws and

knives. And if you got an infection, they cut your arm off. Yeah. And that was, you're

saying they once did that. They once did that. And now we have this like micro keyhole surgery

thing. Yeah. And that's definitely way better than chopping your arm off. Yeah. If you have

an infection on your finger or something. Yeah. Now imagine, I mean, limbs still, still

getting abutated, but not for, you know, just infections and stuff. We definitely have seen

this progression towards using smaller and smaller, less invasive technologies for surgery.

Yeah. Yeah, absolutely. So, yeah, so there's, yeah, so these less invasive technologies,

like I think we could just go further. And I'm like, I think, you know, electronics or

this current AC stimulation is one way. There should be other microwave methods that you

mentioned for access. Yeah. So that's interesting. Are you looking into that now? Not right now.

I don't have enough time to do all the methods or anything. So I'm just like trying to get

something really nice going for EIT. But I think there's other methods that are good

as well. Yeah. And then there's other things as well, like acoustical methods. There's,

so you don't want to sort of limit what method you're using. And I think you can even combine

the two. So I may have a question. Yes. I studied, I studied a lot of neuroscience and

computational biology, but I've no where like, I don't know if I probably understand

like a small percentage of it, even after 12 years of studying. So, yeah, I mean, so

I want to ask the question. And it's like, so do you know that like the real difference

between, so I guess, so MRI is using tomography, but diffusion tensor imaging is using tractography.

And do you know like the essential differences between, and then the open water is using

holography. And I think these are all like related concepts. And I'm like, what are like

the distinctions between tomography, holography and tractography? I don't know. It's really

hard. I don't know. I can try. So tomography is any kind of image reconstruction. And tractography

is looking at the trails of the ions. So I think in diffusion tensor imaging, what like

let's have a look.

Oh, we can actually put up on this one if you want. Okay. That way people can see. So

let me go here. And so we're going to Google diffusion tensor imaging. If you want, you

may use this keyboard. Okay. And then which one did you click on? Oh, I don't know. It's

okay. Do you want to use this one instead of yours? Okay. Is that way people can see

what you're doing?

By the tracking. And so I know what, well, I don't know exactly. Yeah. Yeah. I don't

know exactly what open water is doing. But so there's, there's holography, which is taking

advantage of diffraction. And I believe phase conjugation, which is like an optics concept

where you can sort of do cancellation by sending light at different angles. I'm not a super

optics expert. But you can look up phase conjugation, it will go through the theory of how you do

this light canceling to recreate the scatter. Right. Basically, it's like,

you're creating a scatter. It's deconvolving a scatter with any light technique. I feel

like it's a similar concept to tomography. But oh, yeah. Oh, I mean, it's all a type

of tomography. It's all a type of tomography. Like MRI is a different kind of tomography

than EIT because it's actually using RF. And then diffusion tensor imaging. I don't

actually know what the difference is off the top of my head of my tractography. It's like

measuring that the myelin sheath paths. Yeah. Yeah. Like I can, I guess somehow I read that

the magnet is like when you release, when the magnet gets released, then the water molecules

will travel along axons. The water molecules or the blood will travel along axons or something.

And then that, so the radio signals that are received from the X camera position, the Y

camera position, and the Z camera position will track, will be able to identify the positions

of the, of the, the moving, like the ionic. They'll be able to identify the actual fiber

tracks in the brain of the glial cells because they're, that's why we have these really beautiful

images of like, this is, actually that's too small, but let's see if there's another one

in here. A bigger one. So that's MRI. And then we get, we get, so this is the, these

are like the glial cells that are inside the core of the brain and the neurons, the most

of the gray matter neurons would be on the outside of the brain. And the glial cells

would be able, it's able to map the, the different ion tracks that are along the axons of the

glial cells because the blood is moving along them when the magnet, when the, when the MRI

magnet releases, the radio frequencies blast into the X camera, the Y camera, and Z position

camera. And from that you can sort of, when you have, you can combine the different cameras

together with the X, Y, and Z positions and recreate the actual position in space of each

of those, those axon locations. And so you end up with these really beautiful images.

That diffusion tensor imaging is known for. And to me it's all, like you said, it's all

under the umbrella of tomography, but it's just, I guess it's, I guess I was, yeah, I

wasn't really clear on that until you said that, but yeah, that makes a lot of sense.

Let's go back to the, let's go back to the Google. I think that might be the last page.

So let's go to here and we'll go to, like, images. And so you can see this is another,

so these are, these images are starting to get really, really advanced. And so one thing

I wanted to show you was sort of like maybe one of the things that inspired me was this

research by Jack Launt that, do you know Jack Launt's work?

Yeah, that's right. So he uses MRI and diffusion tensor imaging. And so what he does is he,

let's go to the, the Launt lab. So he uses MRI, it's something that I want to do with

Open EIT because what he did was he created like a semantic map of where, so he had people

watching a movie. So you sit in his lab, he's sitting in his MRI machine, you're watching

a movie. And then he's also monitoring the brainwamps and his machine is making correlations

between not brain waves, but blood flow with the MRI. So he's, his machine is making correlations

between the images that you're seeing in the computer and your blood flow. And so from

that he's able to sort of like create a semantic map of which areas of your brain are lighting

up when you're seeing a certain scene in a movie. And so he found out that you could,

you could say, well, in, in, you know, one region of, of, of your brain, like maybe the

side of your head, you would see, you would see like a dog or a cat, like maybe in this

area you would see like a dog or a cat. And then maybe in some other area you would see

like a house or a building. And so then if you saw a motorcycle, the motorcycle would

be oddly closer and spatially to the house and farther away from the dog. And you would

think, oh yeah, I guess a motorcycle would be more likely to be in a garage or in near

a house than it would be near a cat or a dog. And so you, you, you not only think of it

as being in a different spatial like place and when you imagine it, but actually it is

in a different spatial place in your MRI scan and your blood flow, like spatially. So I

would like to sort of like, so the whole idea for the Neuralace podcast, because we're in

season two, was the idea that we could sort of extend the basic concepts that Jack Gallant

was exhibiting with MRI and an AI to sort of like say, what if we took like the self-driving

car, all the centers of the self-driving car, are figuring out like the concepts of, of

the world around that car, like they're figuring out, this is, you have like, you see, it has

to identify, okay, that's a car up ahead, and that's a car door, and the car door is

open, and that's a cat, and that's a dog, and the dog is moving at this velocity, and that

car is moving at this.

I think they saw some recons, they have a video.

Yeah, do you want to, do you want to look it up?

I don't, um, oops, Gallant video, what do you call it, recreation. And yeah, ah, here

we go.

Oh, you want to, yes, play that.

I think it's this one. So this thing is like, just so inspiring for what we, like, will

eventually be able to handle it.

No, I sort of skipped ahead, but.

Anyway.

Yeah, so the user was presented with a clip on the left, and on the right, that's the

image reconstruction from, from Brain with Activities, as you can see, but that, but

the computer was, was trained on both the original images of, um, the, um, of the person's

medical imaging, and, uh, so they canceled, I go back. So the, so the individual, so the

individual, so the computer was trained on both the movie combined with, um, medical

imaging.

Um, and then what happened is you only, on the right, you only, you don't present the

computer with the, with the, with the images in the clips, you only presenting the trained

computer on the images of the blood flow. And so from the blood flow alone, it's predicting

the images that the person is now seeing. Um, and that is really, uh, significant, um,

because we played again.

I know.

Yeah. So, so, so what that means is, I mean, that we can recreate what we're seeing from

our brain activity, which makes a little sense that we can.

Yeah.

And the question is, so I found out, I found out that, um, and basically a neural network

can function as a, um, as a digital signal processor that you can, I've seen this, I've

seen this, somebody showed me like some work that's under NDA, but you can basically take

a movie and plug it into, um, a neural network and the neural network can reconstruct, um,

the movie just the way, but it's just basically signal processing. It's the same sort of signal

processing we've been doing since the 80s and cameras, you know, camera, video cameras,

they can capture stuff. And so basically neural networks are just, um, sort of, um, encoding

the DSP, the filters.

Yeah.

Then there are no neural networks are DSP. So that makes sense that if, if our brains

are neural networks and they are, if even our, even our eyes aren't contained neural

networks, right? Um, they are naturally going to be able to take the incoming stream of

data and recreate that inside and also learn from that stream of data. And that's basically

like, so at that point, you know, our eyes are basically video cameras. And this is basically

the Jack Claw's work is showing us that we can, we can download images from our minds

because we are sort of creating video, a point cloud of volumetric video with our brains

that we're then learning from. And so like, I tell people, when you look around in the

world, you're not like, like the explanation is that what you're seeing is an actual point

cloud of video that your mind is reconstructing. It's, it's not like video, it is video.

Yeah.

So wouldn't that be cool if there was a way that we could, um, for me, I think that the

whole speech thing, it's, it's, it's nice, but it's not as high bandwidth as speaking

to you and say video. What if I could communicate with you by like, I could just like, like

stick this thing on the back of my head where my visual cortex is. And then this nice hologram

would appear between us of the thing that I'm imagining.

You're sort of outputting your, what you're seeing. So you can sort of describe what you're

saying.

And, and then like what would be even better is like, you can do it too. And then we can

both morph the thing in the middle, like the thing that we're both visualizing. It's

like a high bandwidth conversation.

Yeah. It's like a, like instead of just this time series data that's coming out through

our mouths, you just have like, like thousands, like so many more dimensions if you can sort

of like speak in images together.

You know, I can imagine some diplomats would want to use that to figure out the best way

to, to not only communicate with the other country, but to find, you know, peace between

the two countries, figure out where, you know, figure out how to best, you know, solve the

problems between both countries, the sort of thing. Like there's all sorts of different

possible applications there.

It's really exciting. Yeah. So that's in, in, in, and I think that combining VR with,

I don't know if we should go back to, I want to show you the self-driving car, self-driving

car, machine learning video. I want to see a video of that. So let's go to, yeah. So

basically you're, I don't know if I can, here we go. Technology behind the self-driving

car.

So this was an NVIDIA video from, let's see. I'm pretty sure, I don't know what this

is. Yeah. So basically the computer is predicting, I don't know what that music is. Who is doing

that music?

It's background music.

It's background music. Okay. So yes, the computer is, is trying to take video of the world and,

and recognize objects in the world, recognize houses and, and predict the velocity of the

different objects and figure out if the car can move forward. But if you could take the

sort of self-driving car that's creating concepts of the world around it, and let me see if

I can find a better one. But let's see. Simulating, self-driving. If you can sort of take, this

is an advertisement.

What if home security was different?

Anyway. So if you could sort of like take the concept of the self-driving car and you

could figure out the, all the line, all the, the object segmentation of an object. You

say, well, this is, this is a cat. And this, these are all the pixels that belong to the

cat and you're creating a 3D point cloud. And you're doing the same thing to the medical

imaging of someone's mind while they're looking at this cat. So you're, you're creating an

object segmentation of their brain wave activity with multiple modalities. You're saying this

brain wave activity is strongly associated with this cat that the person is seeing. And

at some point you're mapping out all the objects in the world around a person and correlating

with all of their brain wave activities. You're, you're directly pinpointing which brain wave

activity belongs to which object in the world, in every single object in the world, and all

of their brain wave activity.

Yeah.

If you could sort of like map that out.

So if you can train that, like in a way, so, so, I mean, part of the problem is I don't

want to spend my life lying down in an MRI.

Right.

I don't want to go into it for an hour, but like, to get this kind of volume of data,

there's a couple of different directions. There's the noninvasive directions, which

are harder in many respects because you have the skull to get through.

Yeah.

And obviously that means the signal is not as big. So you need a more sensitive way to

sort of get that information out. But we've been doing all this cool radar stuff for a

while now. It's possible, but it's tough. So we might start with low resolution, noninvasively,

and move to higher resolution as we refine our techniques.

Yeah.

The other way is like you mentioned, like with the neural dust, um, electrocortical rays.

I want to, I want to try to point sensors up the nut, the nasal canal. See what we can,

because you can almost reach the bottom of the thalamus that goes underneath the blood

brain barrier.

Yeah.

And capture those signals. And basically that would be very valuable because, you know,

most of your incoming senses go through the thalamus first, before they go to the rest

of the brain, and then they come back to the thalamus after they pass through the neocortex.

It's a big feedback loop, your brain.

Interesting.

Now, I, I agree with you that the nasal cavity is a hot spot.

Yeah. And also your eyes, your eyes are like a massive two way port in and out of your

brain.

Yeah.

So there's massive like research that's valuable for eye tracking and maybe like, you know,

just really maybe trying to like put electrodes near the eye or on the eye that can measure

the, um, electrical activity from the neurons there.

Um, and that might prove to be very valuable at some point, very valuable research to find

out what's going on in the brain by studying the eyes, um, with multiple modalities of

sensors.

Um, and in addition to every other place, and then at some point we, so if we can predict,

you know, what a brain, you know, what, which, what kinds of brain activity represent objects

in the world, and we can stimulate the brain to produce those same kinds of patterns.

Will we see, um, the like, well, if you can have it, if you can have the, the, if you

can identify perfectly the brain wave pattern of a cat and you can stimulate the brain perfectly

to reproduce that pattern, would you be able to make someone see a cat where there is no

cat?

And if you could do that,

If you could do that, then you would have like augmented reality and virtual reality

without having to wear glasses because you'd be stimulating the brain directly.

I agree.

I agree.

Yeah.

I think it's possible.

And I think it's really exciting and it's amazing, but it brings up all these interesting

questions about control because what you experience, um, what was that saying, neurons

that fire together, wire together. So if you create this like little feedback loop where

you can both read and write, um, you will be changing your brain over time. Um, what

will we become? We will just become part of the network.

Um,

Your brain becomes something you can read and write to like a special kind of hard drive.

You can kind of program it.

You could program it. You could download and you could also download information from it.

Yes.

You could download audio and video and maybe even people like courts may use this to download

like proof that you are guilty or not guilty of some crime that you've been accused of.

Yeah.

Um, that's possible.

Yeah. I mean, it's kind of awesome and scary. Like, um, I think the example today is Facebook.

Yeah.

So Facebook, I think is awesome because it does connect us together. Um, like I keep in

contact with some friends from Australia via Facebook, but who I would otherwise keep

in contact with.

Yeah. Um, and the recording seems to, the screen recording seems to have ended, but

this one's still going. I wonder when it stopped. But anyway, yeah, that's okay. This one's

still going.

Okay.

Yeah.

Should I start it again? Or we don't need the screen recording.

Oh, yeah. I don't know that we need the screen.

Okay.

Keep talking.

Yeah.

Um, yeah.

So we didn't lose anything.

Oh, yeah.

So there's Facebook, which is great.

Yeah.

I think it's awesome because it connects us, except it's privately owned and, um, to,

for it to exist, it gives us advertising, which is kind of taking up our mind space.

I don't want my attention sort of taken up with something that isn't aligned.

Okay.

There's an alignment issue of advertising is not aligned with how I wish to, to use

my mental space.

Okay.

And then there's also a control issue with Facebook. Um, is the government tracking all

of our social interactions with it? Um, are they going to like hack down in some sort of

fascist regime? If you, I don't know, right?

Yeah.

Kind of conversation with, so if there is a fascist over there and you know, you're just

friends and stuff.

So, yeah. So if we perfect neural lace, there is the potential that some of the governments

in the world, some of the fascist authoritarian regimens will, um, use this to spy on people's

minds. Like, um, I wonder if we could just maybe turn that down.

So how do we ensure freedom and have technology move forward?

Yeah.

Cool.

All right.

Yeah.

So, um, what are we saying?

Um, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so, so,

so, overall, I think Facebook is awesome because it connects us.

Yeah.

I don't think we are aligned with advertisers, that's a problem, and I also think, uh, the

government and the various fascist regimes, could potentially abuse it.

Could you abuse it?

Yeah.

And I, that's the same question with PCI.

Well, even with, even with EEG and eye tracking, those two technologies, you, there's, there's

We've already seen really scary machines

that can do lie detecting that with amazing abilities

to detect the nuanced expressions of your lies

that you're giving.

There's some really amazing research.

And these are old technologies, eye tracking and EEG or not

new.

So at the point when we're combining AI and self-driving

cars and open electrical impedance tomography,

and we're predicting the images and maybe also the sounds

that people are seeing, and we're

able to pull those out of someone's mind,

that's way beyond lie detecting, right?

You're peering into somebody's mind, you know?

To me, some people say the word privacy is dead.

And then as technology moves forward,

it seems like we are more and more transparent.

And more of our lives have moved to the computer.

Sometimes Facebook advertising is really untarget.

And it's like they psychically knew

that I was interested in a pair of shoes or something.

I don't know.

And that's both exciting, that it can know you,

and scary, that it can abuse you.

Yeah, could.

So yeah, so there's this, how do we maintain identity

and freedom in this world?

Should we just stop development?

And that's a major concern.

I mean, major technology companies

like Microsoft have recently been

in the news asking for regulation of facial recognition

so that people are not abusing that.

Because the potential for someone to, you know,

in a city like San Francisco to rent an office and a skyscraper

and point a camera at the street

and start doing facial recognition of everyone

on the street is extremely easy.

They pointed out in that article that if your facial recognition

is only verifying, it only has a confidence of 80%.

You're going to make massive mistakes.

But you can still only trust the images

that have a facial recognition of above 98% if you want to.

So it's not like you're stuck with an 80% facial recognition.

And you definitely don't have to use

Amazon's or Microsoft's face recognition.

You can design your own.

And so the potential for not only for international spies,

but also just for corporate spies

to sort of like track individuals everywhere on the planet

is just massive and like you said, easy.

So tracking is also connecting, you know, like a negative way.

So it's also could be beautiful.

Advertisers could be helping you get to the product.

It could help, you know, like imagine

you're wearing augmented reality

and you want to find a store and I don't know.

I'm just saying like, please finish your thought.

How can I help you?

How can it be positive?

Yeah, because it seems like it comes down

to a central question of alignment of goals.

Like I don't necessarily hate an advertiser who's

trying to help me in ways that I truly need.

But I don't really want my attentional resources taken

by advertising I don't want.

I mean, it could be that they intercept someone

who's on a most wanted list and that person

doesn't intercept you.

You wouldn't really know that happened if that happened.

That could be one positive use of facial recognition

in the public space if they're able to locate someone who

could harm you.

But you wouldn't know about it, I guess.

And but I mean, what would be a positive if let's say

that you lost someone and you lost someone

like your kid in a big city, you don't know where they are.

If that person could be located sort of automatically,

that would be valuable to you as a parent if that was the case.

I mean, there's so there's.

Say you're a kid, you're an angsty teenager,

and you want to night out.

Let's say you want to speak from your parents.

And your parents are able to track you down.

And then you would feel very upset about that.

Yeah, it's like two sides of it.

At some point you might start wearing a mask.

Right, so yeah.

The teenagers will be wearing masks.

To escape their parents.

To escape the saying.

That sounds like some sort of, yeah, that could happen.

And then at some point, you know,

they're able to identify people just by the pattern

of your walking.

Yeah, like everyone has a unique pattern of walking,

which is crazy to think about.

Yeah, yeah, gate recognition.

Yeah.

So at some point, maybe teenagers

will be wearing something that interferes with their gate,

like a little bump on their shoe.

Right.

So they force them to knock or gate.

Yeah.

Fooling machine learning is going

to be like this continual, you know, technology

to fool it will be this continual arms race, I guess.

Yeah, yeah.

So I guess we should keep going.

But be mindful of the security arms race

and be responsible about how.

So this technology could also lead

to building robot minds.

Yes.

We understand more fundamentally how the brain is working

and how like our 3D neural network of our brains

are not only doing digital signal processing

and learning from these 3D point clouds of video

and massive feedback loops.

We can also create machines that are doing

the exact same sort of things.

We can understand.

If we can understand our brains.

Then we can create machines that do the same sorts of processes

that our brains do.

And so those machines would be able to all the same sort

of calculations, all the same sort of computational

biology that our brains do.

And they'd be able to see and hear and speak at human level.

And maybe above.

And even have self-concepts.

Yeah.

So that's where I started my group, Self-Waring Networks.

So it's basically from the research into neural ace,

we're going to be able to build self-aware robots

and maybe above human level robots.

And drones that are basically smarter than pigs

or maybe smarter than humans at some point.

And that's a whole another sort of a can of worms.

Absolutely.

Yeah, all these ideas seem very futuristic, Dick.

But Ale and.

Barreling towards us like a speed train.

Like a speed train.

And we have no idea.

We're just challenging.

By the time we finish this podcast, the first sentient.

But by the time we finish this episode of this podcast,

the first sentient artificial intelligence

could be waking up somewhere in the world.

Someone may have already built it.

That being said, if humans in our old school biological selves,

I think if we want to remain relevant or even competitive,

it's an arms race in some respects.

Of who can build the best conscious sentient creature.

And an entity.

That includes supercomputers and everything.

And I would argue that the best way

is that we have to evolve with machines, not without them.

So we can do it a lot with biology

and we'll continue to do more.

But by using machines to understand biology more,

I think we're going to meld.

Well, with the development of nerve gear,

we can begin to expand our minds into external minds.

I call it the extender brains.

And I keep telling people, and this is a product

I'm going to build, the first product is nerve gear

and relays, the second product is artificial cortex

or extender brain, and the third product

is artificial brains or artificial life.

And I don't know, the research for all three is related.

But it means that we can grow with machines

by a person who could decide to connect their mind to an AI.

And there's a number of different sci-fi books

and sci-fi movies that have animations,

like Sword Art Online, that have explored what this concept would

be like and sort of command narrative

and people in these worlds where these technologies exist.

But I think we don't really know how it's going to play out.

We don't.

Some of those books are a little bit dystopian.

And I mean, it could go in all directions.

But I do think that we need to sort of be paying

as much attention to BCI as we do to just AI.

And I feel like AI is currently like gets a lot of the attention

and BCI gets a lot less.

And that's assuming that we want humans to move

into this new world as well.

Yeah.

Yeah.

I think we should maybe cap it off there.

Yeah.

This is everything else you wanted to say or add.

You want to add any, mention any links you want people to visit?

Oh, yeah.

Yeah, it would be awesome if you want to sign up

to the mailing list.

I'm planning on doing a crowdfunding campaign

in a couple of months for the Open EIT project.

You're selling like kids in your crowdfunding campaign?

Yeah.

OK.

You want to be Indiegogo or something?

I'm thinking of CrowdSupply.

OK.

Because CrowdSupply is like an open source

hardware advocate right now.

So where can people go?

So if you go to MindsEyeBioMedical.com,

just sign up to the mailing list.

And you'll get notifications when the campaign goes live.

And yeah, that would be great.

Perfect.

Anything else?

That's it.

That was lovely talking about BCI with you.

Likewise.

Thank you very much, everybody.

And we're going to end it there.