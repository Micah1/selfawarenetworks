a0258z

I am in a valid sense able to describe myself accurately as a motion picture rendering, a volumetric video constructed from frequencies in a 3D grid that is my brain displaying a computed rendering to its own 3D grid. I am a phase field, a tomographic rendering of frequencies in the 3D Neural Network of the Brain!

I have all these different neural arrays going off in my head like flashbulbs, with oscillating ripple's of excitation and inhibition that magnify memories from the exit terminal of a single neuron to the whole brain's network.

3D images, moving in 6 degrees of freedom, telescoping and then cycling about the brain like a movie. It's like reality ripples through us, or that the walls and the mind become one because all life experience is inside the tomodeck of the mind. Not Holography Tomography, not Holodeck, Tomodeck. The mind is a tomodeck.

Imagine these arrays are 3D, meaning signals can go up down left right, so the array can represent reality tomographically through its tempo spatial configuration of action potentials triggering shifts in  memory states across the brain.

*The threshold in the synapse & in the dendrite , is in some sense the bias that is modified  when additional synapses or spines grow, or are removed, and the bias is modified when synapses are inhibited or excited because of agonists or neurotransmitters of some type.

What I mean is that imagine that a phase took up a cube of space, and that they were stacked like legos, in 3D, like a construction set of phase patterns as legos. then you see your 3D world is spatially 3D rendered in your mind, from your perspective as the observer is implied by the rendering.

I like to think that there is not really any you in the human mind, but then again, who am I to think that existence of identity might be just implied and not real in a tangible way.

The brain's oscillatory theta waves may serve as a natural back prop like mechanism for improving the accuracy of nodes, phases.

"potassium channels (VGKCs) open during action potentials and speed repolarization, thus shortening action potential duration"
Potassium Channel Types at Motor-Axon Nodes of Ranvier
https://www.jneurosci.org/content/42/25/4957
1. This supports the synaptic unreliability article & flow of information in the brain.
2. It dives deeper into Parkinson's disease research.

Evidence for Indirect (wireless) signaling reference
"#eNeuro: Researchers at The Robert Larner, M.D. College of Medicine at The University of Vermont revealed in rats that theta coherence within & between the dorsal HC, ventral HC & mPFC changed as a function of learning & cognitive demand & independent of degree of direct synaptic connectivity."
"Dynamic θ Frequency Coordination within and between the Prefrontal Cortex-Hippocampus Circuit during Learning of a Spatial Avoidance Task" https://www.eneuro.org/content/9/2/ENEURO.0414-21.2022

Essentially this reference shows that Theta Oscillations are driving coordinated activity between neurons that are not directly connected by synaptic arrangements, supporting the theory I am putting forward.

It's interesting to think about the Theta phase oscillation as being constructed by a Taylor series polynomial of inhibitory brainwave patterns, with the ability to sort of wirelessly modify patterns across the network over some area of brain tissue. This describes a mechanism for how patterns embedded in our brainwaves might drive coordinated multi-cellular activity at the single neuron level, across the brain without direct connections between activated neurons. Matching up with my other ideas about large scale oscillations driving interfaces between smaller scale neural oscillations in different hemispheres.

# Quantum Teleportation via Oscillation (Did I write this down elsewhere in my notes)
This is also key to the idea of why I think Quantum Teleportation works, in that I think a larger oscillation is carrying a signal between two smaller oscillations that are not directly connected.

Maybe, the major goal for a deep neural network is to achieve compressed & accurate representations, gradient descent helps with the accuracy part, but at least we can be open minded & unbiased towards finding new methods to improve accuracy & sparsity in machine learning.

# "You Don’t Understand Neural Networks Until You Understand the Universal Approximation Theorem" 
https://medium.com/analytics-vidhya/you-dont-understand-neural-networks-until-you-understand-the-universal-approximation-theorem-85b3e7677126

After reading this I considered how the Universal Approximation Theorem kind of made that first layer of neurons seem like the bumps on a record. Like a 1970's recording player kinda record. Right like to represent a music pattern, each note is a bump of a different size, or something like that.

I searched online and I could not find any comparison of Universal Approximation Theorem to Music Records, or CD-roms. But there is significant writing about neural networks as signal processors. https://www.allaboutcircuits.com/technical-articles/neural-network-signal-processing-validation-in-neural-network-design/

I think I have argued somewhere in my notes, more than once, that the human brain's neural network is infact a signal processor among other things. Signal processing is a key component of Neural Oscillatory Tomography, you have to process the signals in order to build up associations or detect coincidences that lead to pattern representations that the network has learned, and pattern recollections that appear in your rendered mind when you are thinking about a memory, or considering the future (a rendered memory-prediction, that a sequence of neural arrays is processing & rendering for other neural arrays)

"Pin Point Impression 3D Needle Art Sculpture Toy" (search for images in the search engine or try this link) https://duckduckgo.com/?q=Pin+Point+Impression+3D+Needle+Art+Sculpture+Toy&iax=images&ia=images 

In that the position of each needle could be understood as the value that a particular neuron took on during the training process when it is learning representations.

ON this topic here is an interesting article about using "Recurrent Neural Nets for Audio Classification" https://towardsdatascience.com/recurrent-neural-nets-for-audio-classification-81cb62327990

In typical deep neural networks we might be talking about high dimensional representations, it's not the value of that one node, it's the pattern embedded across the high dimensional abstract space of all the weights & bias's & active connections in network that represent that learned pattern.

# "Learning in High Dimension Always Amounts to Extrapolation"
https://arxiv.org/abs/2110.09485

This short series on Backpropagation Calculus (3Blue1Brown) can be helpful to understanding the intuition behind backpropagation calculus, and why adjusting the weights, bias, and activations of nodes improves the accuracy of a representation in the neural network. 
https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4

# Action Potential Path Tracing as Taylor Series Polynomials for Neural Oscillatory Tomography 

This new idea I have is about action potential path tracing, each phase interval represents a vector that is being modified like a taylor series polynomial function developing a non-linear pattern in a dynamic linear sequence. So that the waves of inhibition that follow the action potential sequence cause a next level of adjustment to the phase oscillating pattern from individual cell level to aggregate brain waves at greater scales. Brainwaves, such as the theta brainwave, can perhaps be thought of as wireless high amplitude low frequency phase shifts applied to large numbers of neurons in an oscillating cell assembly (or cortical column) at some sequence of time intervals, and they sort of revolve around the timing of the decay of the action potential, so each interaction of inhibitory brainwave bursts following each action potential in a sequence of action potential bursts is intersecting with the previous & the next brainwave, and this process I am suggesting is like Taylor Series Polynomials, in that their effect on one another is one that improves the resolution & accuracy of whatever is being modeled at that moment by that oscillating cell assembly. 

The article that I linked above on "Universal Approximation Theorem" talks about Taylor Series Polynomial Functions having a sort of granularity limitation on pattern representation. So I thought maybe the action potential high phasic firing sequence, which sets off waves of inhibitory low phasic firing sequences, could be construed as a sort of Taylor Series calculation for modifying the curve of a learned vector pattern, represented by a phase change, and modified by subsequent phases changes in other Action potential events. So that as information travels through the nervous system, at each step, as a signal passes from one neuron to the next, the estimated representation of that pattern grows quantitatively, towards what could be some approximation of infinity at a certain point, if it goes on long enough. 

So when it comes to the very excitatory neuron based cerebellum, which isn't going to be that useful for complex information patterns, but is perhaps going to be useful for highly coordinated movement when it comes to skiing, walking, running, snowboarding, riding an electric unicycle, that excitatory network in the cerebellum is going to be useful precisely because the ability to improve spatial pattern representation is going to be accomplished with a Taylor Series like Polynomial Functioning consisting of activated excitatory neurons looping in the cerebellum. So that explains why the cerebellum might be good for spatial navigation and coordinated movement.


