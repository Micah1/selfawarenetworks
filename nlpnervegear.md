The NerveGear Show

Nov 2, 2017

original audio/video here https://youtu.be/p9yTPBrrES4

Audio Transcription by OpenAI's Whisper

Hi everyone. This is Micah here and I'm here with Sarah who's with Virtual Bites.

Hey! Virtual Bites!

Yeah, let us know if the audio is better this time because we're going to try to re-record the nerve gear show that we did yesterday.

So I hope the audio is better this time.

So Sarah here is a neuroscientist. Yes.

Where did you study neuroscience?

I was studying at the Andersen Institute in the Netherlands. I was studying research into psychedelics.

And then I took it because it's really hard to actually give people psychedelics.

Getting their approval takes years and years. And I opened an immersive VR lab in my department.

And I was researching how virtual reality can affect our brain, can affect our perception of our body and our movement.

And now I'm still doing this with Virtual Bites. We're an education and research and art collective.

And we really want to push forward these ideas about what this technology can do for our brains.

The good sides and also the possible dangers that we should be considering.

And we come out with two demos every month of just these wacky ideas that we have of what this technology can do.

And you can see them on our Patreon page under the Virtual Bites.

Great.

So yeah, so Virtual Bites, they're doing research by building actual VR apps now, tying it into their neuroscience research.

And so Virtual Bites was actually at the first SFVR 360 in September.

And again, in October, we had, because we're doing monthly events.

And so you're actually going to be able to see Sarah speak, by the way, at the SFVR November event, which is going to be at the Microsoft Reactor.

This is the SFVR logo, one of them. We have many different versions of this logo.

I'll show you another one real quick. Let me grab the camera.

So this is our other logo.

And so, gosh, I really hope people can hear us. Okay.

I don't see that like a feed, but okay, so I wanted to like, we got, we talked a lot of neuroscience last time.

So the Nerve Gear podcast was sort of like created about like, you know, like I really wanted to say, how could we accelerate the point in time in which we could do AR and VR without a headset and with just by sending signals directly into the brain.

And so we had, this time we got a bunch of brain images here. So it's a really complex problem.

But in general, I'm really interested in building next generation brain computer interfaces and I have been studying neuroscience for 12 years.

And I have a, had a neurofeedback business. So we were combining in 2012, we were combining EEG, taking EEG signals, which as you noted are very noisy, consumer grade EEG is very noisy, it's very bad, but we're enlightened sound effects to the biometric signals it was picking up.

And that would include signals that are mistakenly muscle signals, like if you move your eye and stuff like that.

EEG sensors, the job is to eliminate as much of that noise outside the spectrum of EEG as possible. So you're not picking up ECG or EMG or...

Maybe we should just remind people that at least from our perspective, one of the biggest problems of EEG is it doesn't begin with pick up all of the brain waves, right?

It just picks up brain waves coming from pyramid cells that are synchronized in your cortex. So we're losing a bunch of data even just to begin with.

That's right. Yeah. For example, EEG sensors are usually measuring something in between zero and 200 Hertz, but brain waves go up much higher than that.

So there's a neuroscientist who, I got his picture this time. Let me grab this real quick and show you this.

This book is The Neurobasis of Free Will by Peter C. Criterial Causation is a subtitle. And that book is really great because it's sort of like...

And we also got the Unintelligence book here. You lost me. We also got this. This is Unintelligence by Jeff Hawkins.

Let's see. We also got Networks of the Brain by Orloff Swarnes. Let's see that one right there.

So what I was going to say was... So go into Jeff Hawkins' book real quick, and then we'll go back to Peter C's book.

So Jeff Hawkins' book, and you've read his book, he talks about how grids of cells are complex enough to measure...

Like a 3D grid of cells is complex enough to measure a really complex sequence of inputs from any sensory apparatus.

It's going to be a very general learning algorithm that stores...

So you can have incoming inputs from eyes or ears, from your nose, and it can store unique patterns with amazing complexity.

And the patterns that it stores are based on patterns that came in, and those become predictions for what...

Predictions, yes.

Predictions. So if you get a partial pattern that comes in, it triggers that same network that responded to a larger pattern to re-trigger the...

This is really the unique person, and I guess this whole predictive coding framework that is starting to gain a lot of access in neuroscience.

I consider Jeff Hawkins to be the father of it, because he's the one who started to look at his brain as a prediction machine.

And that's really the first time where people have started to realize that these top-down predictions are really, really important for the brain.

It's not just what's coming into the sentences at any given time, and it's also what's coming down from higher parts of the brain,

that the brain has already been alert to these predictions, and they combine with the bottom-up sensory input in a statistical way to create this world model that we have.

And we can hack this in many ways, and one of the ways that I'm trying to hack this is through VR, because in VR, I can really control the incoming inputs for the brain

and decide whether these inputs fit the known predictions that people already have, or whether they don't.

So I'm looking at you right now, and yeah, you have a sort of avatar that is more or less predictable, but then you have this weird whisperer thing that creates novelty for my brain.

The novelty is the currency for the brain. You really can, and I like to think of it as that's the economy that the brain works on.

Novelty is passed on throughout the different parts of the brain, and the brain has to sort of use that and explain that the novelty is this world model that we have.

Right, so Hawkins' book really drives how our brains might formulate models of the world based on sensory inputs.

But then what was really great about Peter Say's book, The Neural Basis of Free Will, criteria causation, is he then says, well, in the brain, he says, well, maybe every neuron is a coincidence detector,

and maybe the basis of a bit of information in the brain is a coincidence detection.

And what that means is that if you have a hierarchy of cells, like this is, I believe this is a Jeff Hawkins' work, so let's say you have a hierarchy, let me make this bigger.

So if you have a hierarchy of cells, so let's say that you're the lower, the inputs, let's say your eyes, when the senses come to your brain, they're going through the bottom of the hierarchy first.

So let's say your eyes are over here, and do I have a better one?

So they go up through the hierarchy.

So what happens is at the lowest level of the hierarchy, your brain, just like an artificial neural network, could be recognizing lines and edges and curves and colors, is this blue light or red light.

And then at the second level of the hierarchy, or the next level of the hierarchy, it's going to combine these lines and edges go together.

This color goes with this edge.

And at another level, these lines and edges could be like an eye or something.

And at a higher level.

This slowly starts forming distractions and combining or sort of with the rest of our senses.

And the higher you go up in the hierarchy.

Yeah.

And the higher you go up in the hierarchy or the deeper you go into the brain, the larger the chunks get.

So then at one level you're looking at eyes and another level you're looking at, is this a dog or a cat or a car?

And this is how the self-driving cars are designed to work, where you see if I have the self-driving car one.

I don't know if I brought it.

So the idea is that the self-driving car is basically using deep learning neural networks to representations of the world.

And its concepts are how all the lines and edges connect at a higher level in the hierarchy of the network.

And so basically you can see the basis of the concept for an artificial neural network is the map of associations between all the lower level bits that come in through the base of the network or the sensors.

Exactly.

And so maybe some people don't think of their, you think of your eyes like one thing, but it's actually like lots of different sensors and lots of different, you know, edge detectors and color detectors.

Yeah, lots of different layers of computation.

And what's interesting, I think we said we tried to talk about the sense of self, right?

How does our brain abstract a sense of self that all, I guess, meanings have?

Yeah, so and I think that, you know, if you look at like, so look at, I was going to get the book for Douglas Hofstadter.

Douglas Hofstadter says that the self is a strange loop of, so it's like a learning feedback loop.

So you have all these concepts of the world.

This is a dog.

This is a cat.

This is a mouse.

And then if you had a self-driving car or a person observing the world long enough, they begin to notice features about like the self-driving car should begin to notice features of itself.

If it could see itself or its relationship to other things.

And if you had that in a feedback loop, it would begin to formulate a model of a self.

In some sense, it has like, it's not a self-aware self, but it's like a self like, can I move forward?

And to get a self-aware self, you'd have to have not only a model of the self, but you'd have to have it in a feedback loop with all the other data.

So I started this.

The way predictive coding looks at this, do you know the rubber hand experiment?

Maybe we can even try to show the rubber hand experiment here.

If you want to.

That could be fun on you.

So your brain is really, like we said, it just correlates different statistical input.

And sometimes the best explanation it can come up with for these statistical correlations is these statistical correlations are coming from one VA.

So if I take a rubber hand and put it on top of your hand, and all you're looking at is this rubber hand.

Now we're in VR.

So let me put my hand on top of yours.

And let's say I touch you at the same time, and then I'm touching this rubber hand.

I need another assistant to actually do this.

But you would start to actually view.

I'd start to think that this was my hand.

This is rubber hand with your hand.

They squash the rubber hand and they really solve heart rate go up.

So it's a very physiological, very real aspect of the brain.

And it points to the reality that what we're seeing as a model of ourselves is in fact data.

And if you substitute your real hand for another hand, your brain just takes the data.

Yeah, so your brain, so it's like your model of yourself is just data that your brain is putting together.

Exactly.

And it's very plastic.

And for instance, the clothes that we wear, the tools that we use, they all become a part of this self model of what we think of ourselves and the basic bodies that we have.

So people think there's this, you know, people call consciousness the hard problem, like how do you explain what that is?

And I think that, you know, there's several components to it.

There's audio component, there's visual component.

But just imagine like an animal in nature.

What I'm saying is that it's basically like, let's say this is a brain and this is the world that the animal is seeing.

And basically what you're seeing right now, like everything you're seeing is video that your brain is taking through the sensors of your eyes and through the hierarchy of the neural network assembling into a coherent picture.

And the reason it's, the reason it's like, you know, an active conscious experience, because it is for the brain a temporally active brain wave pattern.

And in the temporal reality of it is something you can observe when, you know, like every emotion, every modality has, you know, like a sign, is like a sign wave.

It has peaks, it has lows, it has like, when you, if you, I like to say, let's imagine that you take a second of consciousness and divide it up into milliseconds, even a thousand milliseconds of consciousness.

And you looked at it like a movie.

So it's like, you know, if you take a movie and you divide it up into 10 seconds or something and you're looking at each frame, you say, what is happening in each frame of this movie?

And you could say, well, in one frame of the movie, the animal or the person is looking at a laptop and another frame they're looking at the vibe next to the laptop.

And this is your laptop from your work.

And then they're looking at Sarah or I'm looking at Sarah.

And then they're looking at Sarah with a duck face.

Yeah, Sarah with the duck face.

And then I'm looking at this at this table and these pictures and I'm hearing my voice.

And so in 10 seconds of consciousness, I have this sort of like, so let me grab this one right here.

This is a picture.

Personally, I think that the consciousness and I mean, not just personally, I guess I'm in the Dennett school of thought when it comes to this word. It's an illusion.

I think it's a useless word that's not getting us anywhere. I think we should start looking at these new new words that we have, 86 billion of them, right?

Yeah.

And the moment we, enough of them get activated, it's what we call consciousness.

But that's just because, you know, enough of it gets to this area of our brain, which we can convert it into verbs and communicate it to others, right?

Yeah.

So a lot of what we call consciousness is just this reporting idea, the ability that I can report something to another social being or, you know, in writing.

But so this is where, you know, other parts of our brain are being activated and not the majority or this specific network doesn't mean that they themselves are not conscious.

A lot of athletes, people that dance, everybody that's the flow state, you know, we are extremely conscious when we do more.

So what exactly my body was doing? Absolutely not. I have to, in fact, let the go, the brain has to let go of this little, little verbal part to be able to be in the moment and move it to reality in that way.

So this is an illustrator.

So I know I hope the sound is better on the actual thing.

What is that?

Where's that?

I don't know.

You said I get this.

Well, I don't know.

Where's that coming from?

No idea. This is your old mic.

It's my access to the controls.

Oh, gosh, I'm so sorry.

But what I wanted to show was some settings.

Mute.

Wait, I hope that you're catching the air from my perspective.

Because we're all PC staff.

This is an interesting experience.

So how are we going to learn to interact with each other in these worlds where each of us has a totally different amount of control, right?

Yeah.

And maybe let's do another experiment that I really liked was just passing you, you staff.

What book can I pass you? Let's see, is there like a pen?

Oh, here's a dice.

Can I pass you the dice?

Maybe. I think you can roll the dice.

Wait, I don't want to give it to you.

Ah, it fell from your hand.

Here.

Oh, we can hand on shake.

I meant to be.

I guess it fell from my hand.

Interesting.

In this space before we cannot pass dice to each other.

But I can pass you this pen.

You can pass me a pen, which is very cool.

Because we can change our things.

But yeah, this is, again, why I'm so fascinated with VR and hacking our brains.

Because we can really control things and then control the environment in ways that we never had this capability before.

So can you see this?

It's not just about the visual sense.

It's about everything around us.

You can see this.

So this is an illustration of a neuron.

Yeah.

The density of neurons, which you were just talking about.

They're very densely packed in the brain.

And so an example neuron, let's see.

What I wanted to show you was we have this, which is a, I don't know if you can see that there.

This is the, well, actually, let's start with this one first.

So this is an example of different networks that are activated.

More or less specific conditions, I guess.

Yeah.

So the idea is that when you are neurons activate in a circuit, you can see, and there is a video that I got, but I'm not going to show it.

But gosh, I wish I could turn off the sound.

It's driving me nuts.

It is.

It's really driving me nuts.

Like, that's not fair.

It's like a meditation and hum.

Yeah.

But, okay, let's try media.

Save.

Save.

Explore.

Let's try to put this.

Okay.

Yeah.

Okay.

Okay, let's see where this happened.

Okay.

I don't know where that's coming from, but now we're in a different, like, location.

Wow.

Look at this.

It didn't stop.

Unfortunately, that didn't stop the background noise.

Yeah.

What is that coming from?

Anyway.

You guys just sit in your head.

So what I was going to say with this, when you have a neural circuit, you have a sequence of neurons.

It could be five or six.

You know, it could be a lot of neurons that are lighting up in a sequence, and they're not necessarily directly connected.

We're not necessarily aware of all the connections, but.

It does have a statistical correlation.

Yeah.

And so, but you could say that because of that, if you like watch it, watch videos of this activity as it's been mapped, it kind of suggests that our brain's neurons are lighting up.

And it kind of suggests the electromagnetism that our neurons are activated by electromagnetism.

And the point that I like bring up is that, you know, maybe because we have these, we have our dendrite.

Let's see.

This has dendrites.

Yeah.

We have, you know, dendritic.

The idea is that dendrites are computers.

They're, they're like mini computers, and they have all this amazing processing capability.

And the idea here.

Yeah.

So the idea is that they have all these, it's like, they all have these amazing, like it's like, it's like they branch like trees, right?

And they have all these different hairs and they can, they can actually.

Combine the data that they get in very complex ways.

Yeah.

And apparently.

Decide what to pass on and then what not, I guess.

And of course it's very chemical, giving them entermorpho, and entermorpho sizing them is a bit problematic.

Yeah.

They're extremely intelligent in their own way.

One of the ideas from the neuro basis are free.

Well, Peter C was in some, he's like, he's just trying to say that in some cases we make our decisions for what we're going to do in advance.

Like, and there are, our decisions are based on like all these little tiny bits of criteria.

So if we, if we encounter some situation or some person that matches all this criteria, then we have already planned act in a way.

And that sort of gets around the idea that your decisions are, like, you can see your, like, in a, if you're, if you're measuring your brain waves,

you can sometimes see your decision before you're personally aware of it.

Exactly.

But it could be possible that, that's because you sort of planned your, your decision of what you would do if certain criteria were met.

And so that's, he's trying to say that maybe it's, it's not that your decisions are being decided without your consciousness,

that your decisions are being decided at a previous time when you decided them.

And then they're sort of maybe perhaps stored in dendrites, which can activate their own action potentials and start causing neural activity.

And so the dendrite is perfectly positioned to, so normally when we see, like, see, let me just grab a picture of a neuron here.

Normally, no, actually not that one. Let's grab this one.

So normally in textbooks, they say, like a neuroscience textbook.

They say that the action potential travels between cells in a chemical synapse.

And it's, but they actually travel in a cell body.

And what travels between neurons in a chemical synapse is the neurotransmitter.

So you can see with medical imaging that are, it looks like there are electrical signals passing between neurons when you look at medical imaging.

But what, what's actually happening is the action potential is just passing along the body of a single cell.

And what travels between is, is our chemical activations that can trigger cells further down the line.

But what you don't see in all these textbooks, neuroscience, is the really complex mappings between all the neurons.

So this is like, this right here represents this right here.

And this is the dendrite. So the dendrite could activate something.

Then as we were noticing in the, in the previous call, there could be between 10,000 and 200,000 connections between all these different cells.

So the question is like, when you have a neural circuit, which I bring this one, if, if each point, like, let's say this point is a cycle of activity.

If each point is connected to 10,000 or 200,000 other possible neurons, how is it possible that this circuit lights up in a consistent way when that signal could be traveling down any possible branch?

And so the hypothesis that I have is that the, the dendrite itself, when you're, which you say you have a future decision you're going to make,

the dendrite itself is going to, when all the criteria are met, it's going to become the lowest possible negative.

And I mean, electromagnetic negative. So like a lightning bolt will strike the, the most negative point on the earth.

And so that would be a way for a dendritic computer to attract a signal to itself.

And thus it would allow for an organized neural circuit where you have a consistent pattern of activity,

and these would be the dendrites that have the lowest point of electro, electromagnetic negativity.

And the, and they are attracting the signals to themselves, creating a consistent neural circuit in the brain that resembles a highly coordinated pattern of action.

And so when you have a neural circuit, like imagine that each one of them, let's, let's say they imagine we have a neural circuit that's very general.

And one of them represents the, let's go back to the hierarchy of, of patterns in your brain.

So you have patterns that represent eyes and some of the dogs and cats.

And you have this guy, I don't know if I brought it this time, but you have this guy, his name is Jack Gallant.

And he mapped out in the neocortex all these semantic relationships between neural correlations and like in different things that people see in movies,

like dogs and cats and houses. And so he was saying, like in different parts of the brain, he says, you know, dogs will be here, cats will be here.

Let me get, let me get that pen.

And so he'd say, he would say, like, you know, somewhere over here is dogs, and some over here is cats, and some over here is, is motorbikes,

and some over here is cars, and cars and motorbikes will be closer together, and dogs and cats will be closer together,

and people would be closer to dogs and cats and far away from, from motorbikes and cars.

You build a contextual network of, again, all of this based on just these statistical correlations, right?

Yeah, so all these sensory patterns would be creating like, so this, so this, this would be one part of your neocortex,

and, and maybe from your eyes, and this would be creating like a, or eyes and ears, and this would be creating like visual patterns,

and you have audio patterns, and then your different parts of your brain would represent that.

And then when you have your neural activity, which is where I'm getting dizzy,

when you have your, when you have your neural activity, the idea is that in 10 seconds, in one second of neural activity,

maybe you're, maybe you're, you're seeing someone like I'm seeing Sarah right now, or you're, you're, I'm seeing this, this image,

and in the next millisecond, I'm hearing my voice, and the next millisecond, I'm thinking about myself wearing this headset,

and the next millisecond, I'm looking at the world behind Sarah, and in the next one, I'm looking at the pictures behind Sarah,

and each second is made up of, of all this, all these conceptual models which consist of coincidence patterns in the brain,

and they're lighting up in a sequence, in a neural circuit, and because they are, in anyway, because of the dendritic computation,

and so then-

And yes, you're talking about trying to hack this information directly from the Stalinist. It gets all distributed.

That's right.

And I thought it was a pretty interesting and novel idea.

Yeah.

Not to care how it, if it will be possible, but it's a very interesting idea to try to get this, at least at the bottom of the information

over where the feedback is happening, what's happening in this top-down feedback, and everything seems to, almost everything seems to be the Stalinist.

Yeah.

It's definitely an interesting thing to try to get type of reading from there, which apparently isn't really possible.

So if we're trying to, like, imagine how the brain actually, let me grab this one right here.

If we're trying to imagine how the information flows through the brain, you know, with regard to the visual cortex,

let's say the eye, let me grab that pen again.

The tracking is not so good. It was much better yesterday, but-

So imagine your eyes, like, right around here, right?

And so you have, you know, photons are bouncing off your ganglia, and your ganglia are, the proteins are flipping,

and it's causing an ionic displacement of calcium and potassium ions.

And the, so separating of charges, positive and negative charges, and that results in an action potential,

which is triggering chemical synapses.

And so you see signals in metal imaging traveling from the eye to, along the optic nerve to the thalamus,

which is here in the center, which I'm pointing out right now.

And so the signals travel, the optic is the optic nerve, this thalamus,

and then they travel back here to the visual cortex.

This is the obstetal lobes, and these are the parietal lobes right here.

And something similar happens with ears.

So with your, you know, you have your ears would be out here somewhere.

Let me get a different marker for ears.

And I don't know if they can see this.

So your ears would be out here somewhere.

And again, the signals are going to travel to the thalamus,

and then they're going to travel to the audio cortex.

And so the point being that like your, your eyes and your, in your ears,

and most of your senses, except for your nose, are going to travel through to the thalamus first.

Now I was talking to, I went to a meetup and I saw David Eagleman,

who's a, who's a kind of a well-known neuroscience author.

And he was saying that, you know, if you're trying to, you know, if you're trying to like use medical imaging to,

oh, what are you trying to get?

Me!

Yeah, you, oh.

To feedback on myself.

Let me send this to you.

So if you're trying to do medical imaging with EEG from the scalp,

if you consider the work, if you consider the work of Jack Alont,

all the semantic relationships are spread out in a sparsen distributed pattern all over the whole brain,

at the point within the neocortex.

But if you consider the incoming senses go to the thalamus first,

the thalamus could be the optimal place to look for to, to create a new brain-computer interface.

If you could, if you could have a sensor, and so I know of some new sensor technology that's coming,

but if you could have a sensor that, that you could either insert up the nose to target the thalamus reading

or point it from the nose up to the nasal gum to target the thalamus.

And you can, the sensor is like a light field sensor, but for electromagnetism,

and you can configure it for, so you can notice the origin point of the brain,

and you can predict it, like a light field camera for elect, for brain waves.

And then if you can map the signals coming from the, the thalamus,

then you have the signals before they get more broadly distributed across the neocortex,

and be aware, and so what's interesting is about the thalamus is that not only does everything go into your neocortex,

it goes through the thalamus, you're in a neocortex and gets, you know,

all the semantic map is all over the place, but, but that stuff, it's a big feedback loop,

so it all comes back into the thalamus, like the thalamus is like the, the, the bottom and the top of the pyramid,

and your brain is like a big feedback loop.

And so it all goes through the neocortex and back into the thalamus.

So that could be like the integration point for all the different sensors into modality.

So if you're, if you're seeing a dog and a cat and a house and a motorcycle and a car at the same time,

it could be that those things are, are, you know, being grouped or being like,

there's a neural circuit that's running through the neocortex

and passing that information back into the thalamus where it's being integrated.

And I think that, that could be true. I don't know.

So the idea here is that if we could take the self-driving car type of computer to put it in the backpack

and it can formulate a, you know, like, so there's this,

Matterport just releases really cool.

This is a 3D camera company and they just released a really cool, like,

AI that can recognize patterns from pixels and also draw, like, shapes around the recognized patterns.

And so, anyway, the idea is that if you can have an AI look for the patterns of something in the world

like what Sarah's drawing right now and say,

I'm drawing a car right now.

She's drawing a car right now.

I have a self-driving car.

So the idea is all these lines and edges that represent the self-driving car as a conceptual pattern in the computer

could be, oh, thanks. Thank you. Now we've got a car.

We have a car.

So, so the idea is that that concept can be recognized by the computer.

And at the same time, we, the idea is to apply some new brain-computer interfaces and deep learning programs

to studying the brain, looking for narration neural correlation,

neural correlation to the outside world.

So that we're creating, combining the conceptual models of the world that the computer recognizes

with the conceptual models that the computer is making of our brainwaves to say,

well, this car in the world, this pattern of a car matches this brainwave pattern in our head.

And then the next step is like, we look at the work of Olaf Spornes, right?

And that works for the brain.

So if the brain is like a network, what is the communication protocol?

Is it more like TCP or UDP?

Are we throwing packets or is there a firm handshake?

Is there a feedback loop?

That sort of thing.

So that's one of the things we talk about.

If we can figure out the communication protocol of the brain,

which I think could be alternating transmission protocol,

which means alternating TCP and UDP type of connections,

for conscious stuff, it would be, I don't know,

I feel like I'm taking up, I'm just going like a...

But anyway, so...

I'm being fascinated by avatar, at least by itself.

It's messing with my brain seriously.

At least your avatar is not talking when I'm talking.

That's what was happening yesterday.

Well, I hid my mouth partially because of that.

Yeah.

I don't think it's happening.

I think it's because we were in the same room.

I do feel that this is part because I am controlling

some of the movements.

Yeah.

But at the moment, there's like, I didn't come from my body

and my brain goes like, ah, what just happened here, right?

Yeah.

So the reason I have...

These are things that VR does not have to start thinking about.

So I'm part of this group that makes this program

called MicroDOS VR Vision Agency.

And what we're doing with MicroDOS VR,

what we're doing is we're combining VR with EEG.

And...

So it's creating...

Let me show you an image of the graphics of my...

If I have it.

Yeah.

So MicroDOS VR has these amazing graphics.

Okay.

And so you get in there with your controllers

and you're creating these amazing graphics.

Let me show you.

Let's realize this.

And then what we're going to do is we're going to have

your brain waves.

This is already working.

This was in the Neuralace podcast number five.

I talked to Android Jones.

We got this headset from Muse.

And let me pull this Muse down.

Okay.

Grab that.

What'd I do?

Grab my mind and move.

Sorry.

Which is...

So we got this headset from Muse.

Okay.

And the Muse headband.

And it integrates into the Facebook...

I don't know if you can see that.

It integrates into Facebook's...

No, no.

It integrates into the Vive headset.

Here's another one.

This one is Neurable.

This is another EEG for a Vive headset.

And so anyway, we're looking at the...

We're right now working with Muse.

We're looking to get a Neurable.

But the point is that your brain waves

can drive changes to light and sound effects

in microdose VR.

And that enable you to become aware

of how your thoughts and feelings are brain waves

and how those brain waves are causing

changes to light and sound effects.

In fact, it's seeing the changes in light and sound effects

that sort of like expand your awareness of your brain waves

and how your thoughts and feelings are brain waves

or are reflected in brain waves.

That's what I meant to say.

It's a really interesting and fun experience.

Another microdose VR picture.

What?

Go ahead.

Go ahead, your turn.

So I said it's a really fun experience with microdose.

And I love it.

It's one of the best VR VR experiences

that I've ever tried.

And have you tried my Uber?

That's my trippy experience.

Yeah.

Well, you know, it was...

I'll have to bring it.

I'll have to next meet it up.

Yeah.

Well, the thing is, when I'm organized...

You can do that, actually.

Okay.

I'll download it.

Because I was organizing it.

I really want to.

No, actually, I did try it.

Yeah.

I did try it.

But I don't...

So you get to come to the world.

That's right.

When you dance, the world comes alive and changes.

That's right.

About these experiences that they hack the brain.

Again, you're giving different suggestions

on what you're used to your creating.

It's an aesthetic of effects.

And that's a very, very powerful tool of letting go

of normal reality and the normal world model

and updating things and just getting your brain

into a calmer or more relaxed state

or more active state depending on what you know those times.

So I wanted to say...

So this here is the idea is that when you see...

This is a meme that I made.

When you see a part of your neocortex represents it

and you could call that an advanced data structure

in your biology, and that advanced data structure

may consist of a local microcircuit of neurons

that change direction or frequency of your brain.

And I'm referring to the...

Where's that image?

The neural circuits where I'm saying that maybe

the advanced data structure, like what you're seeing right now

could be patterns in that advanced data.

So it could be that microcircuit

that's activating that pattern in your brain.

And so anyway, that's the idea.

And so if that's true, then that makes the case for

why once we have that pattern,

once we know the transmission protocol of the brain,

if we have that pattern in the computer,

then we can create an artificial neurons

or just a device that can then receive electromagnetic signals

and send the pattern,

the difference between the pattern that you have

in your thalamus right now

and the pattern of what your thalamus would be

if you had something in your reality like these glasses,

like these glasses right here,

if you had that path,

the idea is that if you had this pattern in the computer,

you could send it back through the transmission protocol,

through electromagnetic signals,

cause your neuron and your brain to fire in a pattern

that is isometric to the pattern

that the computer recognized when you were looking at glasses,

and you would see glasses in your reality

that are not actually there,

so it's virtual reality sent directly to your brain.

Basically any pattern you could create for your brain

if you know that pattern is

and you know the transmission protocol of the brain.

And that's the sort of finding part,

but the really useful and not super fun part

is that if we understand how bunches of neurons

communicate to other bunches of neurons,

if we understand that network protocol,

and we understand how those communication signals

are packaged and what the data looks like,

then we could things like reconnect spines,

and we could connect artificial limbs

or transport your sense of self into an artificial body,

and you could have like a mechanical,

I mean a little bit of that now with VR,

and so what we've been talking about in a sense

is the convergence of all these great technologies

like artificial intelligence and virtual reality

and medical imaging like EEG,

and so we're beginning to see the convergence

of like biometric sensors, EEG,

EEG like heart rate sensors,

and eye tracking pupil dilation sensors

into VR headsets coming anyways.

And people are figuring out how to combine

of course blockchain and all these other great technologies

and drones together.

And that is the topic of our meetup in November

where we're going to be talking,

the San Francisco VRs.

We're going to be talking about

the conversion of virtual reality

with biosensors and how we pay management

and for our health, for our good things,

and maybe about the dangers and what we should be aware

of the companies that might be doing it without us.

I want to mention that we've been,

Microdose VR has been talking to a bunch of our scientists,

like one of them is Adam Gasly,

and Gasly in Gasly's lab is well known

for a lot of different things,

but what we're going to do now

is getting approval to prescribe VR for pain relief,

and he's actually going to the FDA to get approval,

and as soon as he does that,

Microdose VR is going to be following short on their tails

to get this prescribed for pain relief,

to get this passed for certification

so that doctors can prescribe VR apps like Microdose VR

for pain relief and for, you know,

you can use, you can get the app for, you know,

medicine, you can use it for fitness

because it's a very dancing app.

When I demo this, when I say Android,

demo this at events, people get in it

and we have these videos of people

and they just start dancing because it's so fun,

but that's, you know, it's,

there's enormous potential for healing from VR apps,

and there's also like enormous potential for,

VR developers have to be very responsible

because you can use, there's definitely a potential to,

to do the reverse, to not do the good things to mind.

And I think there's a lot more research

that it has to be this,

and from what we were discussing this yesterday,

why did we change our avatars

to sort of, and this value of yes,

they are all the brains to identify with,

but then I looked like a 10-year-old.

My brains is to start thinking that I'm a 10-year-old.

I don't think that will be healthy for them.

They're my brains start thinking that,

oh, I can be a weird duck creature with hop.

So much better.

That is going to a totally different paradigm

compared with real me and when I go back to VR.

And these are the things we have to start thinking about.

The more people will spend and what it is

that we're all outside of VR,

we have to combine it well

and not create very disturbing effects

that might happen if we don't take care of it.

So let's talk a little bit more about your company.

And who's this gentleman here

that's with you at your company, Matt?

So this is Matt Ho.

Can you grab his photo?

I need...

People can see it.

Okay.

Matt's a co-founder.

And he's a middleman.

He does WebVR because right now

there's all these different platforms.

More and more, there's just coming out.

And a good way to just go through WebVR

and you can just create web sites

that will alert all the platforms.

And that's sort of his expertise.

And the project we're doing is going to be based on that.

A memory palace experiment.

And CCIF, you can learn French better

and it will just be a few words

and then an experimental health condition.

But we're very curious

if being in an immersive place

or experts create memory palaces

using visual imagery to remember

each additional word and create a path.

What if I could literally take you to path

to learn the words?

What did you remember the better?

So that's our work.

So I wish I could grab this, but maybe I can.

Okay.

So one thing that I think might be useful

is explaining why VR can be more powerful

for learning and...

Our hands...

I'm like exploring.

Oh, we did it.

We were just in the same space

and then the hands just decided to shake

which is C.

One hypothesis for why VR

can have such a powerful effect

on your ability to learn

could come down to this concept

of criteria causation.

The idea is if you have...

If your neurons are coincidence detectors

and you're hierarchy of detecting high level coincidence patterns, and with your, you're creating this world around you, this world of information which is creating a lot of different control, right?

42:54:00

That's the design of what is the coincidence. So we're saying it was that you had this volume of spatial information. It takes up here, which space when you're in BR and that could definitely lead to causing your brain to trigger at a high level when it's trying to absorb and learn from all the information question, that that's a very big thing to like the fact that it's early, my individual says correlated to my microphone, it is, that's very important and we can get out to be.

43:25:00

So the more sensors you get into it, the most is reality. The vertical part is to just mean something to the forces of the picture. Okay. So okay we're gonna wrap it up and I'll say that what we call the guys we have for speakers. That's Sarah and Richard Vice must have decentraland.

43:42:00

You mentioned that Matt saw is really into web VR with the central line is gonna be talking about convergence of web with blockchain, which is what I think, maybe not the other city or yeah. All right. So, all right, so we're gonna go ahead and end up. Thank you much, Sarah.

44:01:00

Thanks for. Thanks for watching person as watching. We got some point but we'll see about the sound and yeah. Okay. Truth.
